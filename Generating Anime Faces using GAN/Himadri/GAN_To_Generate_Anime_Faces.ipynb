{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TeKfjIpoOnRb"
   },
   "source": [
    "# GAN\n",
    "\n",
    "Finally got hands on implementing \"the collest thing that has happened in deep learning in the last 20 years\". Yep, it's the Generative Adversarial Neural Networks (for the poor). This is just an implementation to understand the basic working of GAN (,a DCGAN actually) and apply it to generate images of new anime faces.\n",
    "\n",
    "The images are not gonna be highly detailed, since I am poor and my only savior is Google Colabs, so I am restricted to use small images of dimensions 64 X 64. Downloaded the dataset from Kaggle. The link to the dataset is:\n",
    "\n",
    "https://www.kaggle.com/aadilmalik94/animecharacterfaces\n",
    "\n",
    "### Importing and installing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ABlxlHd89m5j"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Input, Reshape, Dropout, Dense, Flatten, BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import sys\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3XzFyIwL9qE0"
   },
   "source": [
    "### Importing data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "HsMqph6FOkZh",
    "outputId": "558e2088-94c5-454e-9c29-0a46c449b47f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
      "Requirement already satisfied, skipping upgrade: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.0)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.38.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.21.0)\n",
      "Requirement already satisfied, skipping upgrade: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.11.28)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-e3fb3fe2-e0cd-4e0e-b2a0-e3ae543cd7f4\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-e3fb3fe2-e0cd-4e0e-b2a0-e3ae543cd7f4\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving kaggle.json to kaggle (1).json\n",
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
      "Downloading animecharacterfaces.zip to /content\n",
      " 98% 746M/760M [00:16<00:00, 33.5MB/s]\n",
      "100% 760M/760M [00:17<00:00, 46.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "#Installing Kaggle\n",
    "!pip install -U kaggle\n",
    "#Make a directory name dKaggle\n",
    "!mkdir -p ~/.kaggle\n",
    "#Upload the kaggle.json file\n",
    "from google.colab import files\n",
    "files.upload()\n",
    "#Copy the json file to the Kaggle directory\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "#Import the dataset. This usually takes a lot of time. The total dataset is >13GB\n",
    "!kaggle datasets download -d aadilmalik94/animecharacterfaces\n",
    "!ls\n",
    "#Unzipping the files\n",
    "!unzip \"animecharacterfaces.zip\"\n",
    "!ls\n",
    "%cd animeface-character-dataset/data/\n",
    "!ls\n",
    "%cd ../../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RfNQV4D50BDj"
   },
   "source": [
    "Here we define the function to load the images from the dataset and preprocess them. The images are rescaled between -1 and 1 (as follwed by all the implementations available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4DIYSxfOZnF"
   },
   "outputs": [],
   "source": [
    "def load_dataset(batch_size, image_shape, data_dir=None):\n",
    "    sample_dim = (batch_size,) + image_shape\n",
    "    sample = np.empty(sample_dim, dtype=np.float32)\n",
    "    all_data_dirlist = list(glob.glob(data_dir))\n",
    "    sample_imgs_paths = np.random.choice(all_data_dirlist,batch_size)\n",
    "    for index,img_filename in enumerate(sample_imgs_paths):\n",
    "        image = Image.open(img_filename)\n",
    "        image = image.resize(image_shape[:-1])\n",
    "        image = np.asarray(image)\n",
    "        image = (image/127.5) -1 \n",
    "        sample[index,...] = image\n",
    "    return sample\n",
    "\n",
    "def read_data():\n",
    "    image_shape=(64,64,3)\n",
    "    X_train = load_dataset(30000, (64,64,3), \"animeface-character-dataset/data/*.png\")\n",
    "    print('data loaded')\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eEyE5PBJ-f_h"
   },
   "source": [
    "Now that we have the data ready, time to code out GAN.\n",
    "\n",
    "# Deep Convolutional Generative Adversarial Networks (DCGAN) Theory\n",
    "\n",
    "So what exacly are GANs. Well, a Generative Adversarial Network (GAN) is a class of deep learning network  model invented by Ian Goodfellow in 2014. Two deep neural networks sort of compete with each other in a game. Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics.\n",
    "\n",
    "So the basic structue becomes two deep neural net architectures. On of the is the Generator Network and the other is the Discriminator Network.\n",
    "\n",
    "![GAN Structure](./img1.png)\n",
    "\n",
    "### Genertor Network\n",
    "\n",
    "The Generator Network takes random noise as input, propagates it through a neural network to transform the noise and reshape it into an image that is similar to the one in the dataset. The end goal of the Generator is to learn a distribution similar to the distribution of the training dataset to sample out realistic images.\n",
    "\n",
    "### Discriminator Network\n",
    "\n",
    "The Discriminator Network is more of a classifier that outputs the probability that the image fed to it is real. It is fed real images with final_value 1 denoting it as original images and los images from the Discriminator with final_value 0 denoting it as a fake image, during the whole training.\n",
    "\n",
    "Thus, it boils down to the fact that during training, the Generator has to generate images taht looked similar to the original images such the Discriminator cannot differentiate them. The Discriminator also has to train itself to better identify the fake images. This makes the Generator to converge more towrds generating more realistic images.\n",
    "\n",
    "DCGAN's are a type of GAN the uses Deep Convolutional Neural Networks (or CNN) as function approximators. CNN are best suited to find correlation within spatial data.\n",
    "\n",
    "![DCGAN Structure](./img2.png)\n",
    "\n",
    "That's almost the baisc theory required. We can now move on to code the architecutres and define the other necessary components. But before that lets keep all the possible parameters at one place.\n",
    "\n",
    "Note: I did not write the code to load pre-trained model, because I wanted to train it from scratch. However, I have saved the models after training whcih can be loaded back using the load_model() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UpwzC7_tcYEe"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GIgqufCuZh2m"
   },
   "outputs": [],
   "source": [
    "img_row = img_col = 64\n",
    "n_channels = 3\n",
    "img_shape = (img_row, img_col, n_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LLzl6fOiawQ-"
   },
   "source": [
    "## Defining the Generator and Discriminator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XhaCDoa0FUSS"
   },
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(64 * 4 * 4, activation=\"relu\", input_dim=100))\n",
    "    model.add(Reshape((4, 4, 64)))\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(256, kernel_size=3, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv2D(16, kernel_size=3, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Conv2D(n_channels, kernel_size=3, padding=\"same\"))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    noise = Input(shape=(100,))\n",
    "    img = model(noise)\n",
    "\n",
    "    return Model(noise, img)\n",
    "\n",
    "def build_discriminator():\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=img_shape, padding=\"same\"))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    img = Input(shape=img_shape)\n",
    "    validity = model(img)\n",
    "\n",
    "    return Model(img, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "BRY31ntHbH-A",
    "outputId": "49d66f7e-d774-4baa-c501-653b7b09c7ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_34 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPaddin (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 17, 17, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 17, 17, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 9, 9, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 9, 9, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 9, 9, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 9, 9, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 20736)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 20737     \n",
      "=================================================================\n",
      "Total params: 410,945\n",
      "Trainable params: 410,049\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 1024)              103424    \n",
      "_________________________________________________________________\n",
      "reshape_5 (Reshape)          (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_12 (UpSampling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_13 (UpSampling (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_14 (UpSampling (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 32, 32, 128)       295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_15 (UpSampling (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 64, 64, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 64, 64, 16)        9232      \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 64, 64, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 64, 64, 3)         435       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 64, 64, 3)         0         \n",
      "=================================================================\n",
      "Total params: 853,315\n",
      "Trainable params: 852,131\n",
      "Non-trainable params: 1,184\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator()\n",
    "\n",
    "# The generator takes noise as input and generates imgs\n",
    "z = Input(shape=(100,))\n",
    "img = generator(z)\n",
    "\n",
    "# For the combined model we will only train the generator\n",
    "discriminator.trainable = False\n",
    "valid = discriminator(img)\n",
    "\n",
    "# The combined GAN  (stacked generator and discriminator) trains the generator to fool the discriminator\n",
    "gan = Model(z, valid)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uF5fvxGs6P2q"
   },
   "source": [
    "## Defining the function to save the image output after 100 epochs of training\n",
    "\n",
    "The data have are 64 X 64 X 3 dimensional images.\n",
    "\n",
    "Source: https://github.com/jeffheaton/t81_558_deep_learning/blob/63bbb19f092736a7077fdebd39fba7c87db27014/t81_558_class_07_2_Keras_gan.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HV342euk6crW"
   },
   "outputs": [],
   "source": [
    "def save_images(epoch,noise):\n",
    "  image_array = np.full(( \n",
    "      16 + (4 * (64+16)), \n",
    "      16 + (7 * (64+16)), 3), \n",
    "      255, dtype=np.uint8)\n",
    "  \n",
    "  generated_images = generator.predict(noise)\n",
    "\n",
    "  generated_images = 0.5 * generated_images + 0.5\n",
    "\n",
    "  image_count = 0\n",
    "  for row in range(4):\n",
    "      for col in range(7):\n",
    "        r = row * (64+16) + 16\n",
    "        c = col * (64+16) + 16\n",
    "        image_array[r:r+64,c:c+64] = generated_images[image_count] * 255\n",
    "        image_count += 1\n",
    "\n",
    "          \n",
    "  output_path = './output_images'\n",
    "  if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "  \n",
    "  filename = os.path.join(output_path,f\"train_image_{epoch}.png\")\n",
    "  im = Image.fromarray(image_array)\n",
    "  im.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data into X_train variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "VkWwIGioaBGv",
    "outputId": "38f07d3e-9add-465b-fba2-9c5066855075"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n"
     ]
    }
   ],
   "source": [
    "X_train = read_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XoH6OLzMmcy8"
   },
   "outputs": [],
   "source": [
    "def train(epochs, batch_size, saveAt):\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        real_ids = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        real_images = X_train[real_ids]\n",
    "\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "        gen_images = generator.predict(noise)\n",
    "\n",
    "\n",
    "        real_loss = discriminator.train_on_batch(real_images, np.ones((batch_size, 1)))\n",
    "        fake_loss = discriminator.train_on_batch(gen_images, np.zeros((batch_size, 1)))\n",
    "        disc_loss = 0.5 * np.add(real_loss, fake_loss)\n",
    "\n",
    "\n",
    "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "        gen_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "\n",
    "        # Print details\n",
    "        print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, disc_loss[0], 100*disc_loss[1], gen_loss))\n",
    "\n",
    "        # Save images at saveAt points\n",
    "        if epoch % saveAt == 0:\n",
    "            save_images(epoch, noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "vPyuQcA9r2JN",
    "outputId": "1571ae0f-b6ca-446e-9a7d-739b6199a989"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:493: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.542992, acc.: 67.19%] [G loss: 1.729649]\n",
      "1 [D loss: 0.502218, acc.: 78.12%] [G loss: 1.886701]\n",
      "2 [D loss: 0.487596, acc.: 75.00%] [G loss: 1.964919]\n",
      "3 [D loss: 0.452026, acc.: 85.94%] [G loss: 1.876930]\n",
      "4 [D loss: 0.271426, acc.: 92.19%] [G loss: 2.012665]\n",
      "5 [D loss: 0.409888, acc.: 84.38%] [G loss: 2.651031]\n",
      "6 [D loss: 0.622145, acc.: 65.62%] [G loss: 2.703025]\n",
      "7 [D loss: 0.354155, acc.: 85.94%] [G loss: 3.286540]\n",
      "8 [D loss: 0.587899, acc.: 70.31%] [G loss: 3.545927]\n",
      "9 [D loss: 0.345541, acc.: 84.38%] [G loss: 3.706892]\n",
      "10 [D loss: 0.329320, acc.: 85.94%] [G loss: 2.677814]\n",
      "11 [D loss: 0.378747, acc.: 82.81%] [G loss: 2.536362]\n",
      "12 [D loss: 0.201384, acc.: 95.31%] [G loss: 2.666023]\n",
      "13 [D loss: 0.199269, acc.: 96.88%] [G loss: 2.527017]\n",
      "14 [D loss: 0.273919, acc.: 87.50%] [G loss: 2.225196]\n",
      "15 [D loss: 0.414209, acc.: 75.00%] [G loss: 3.822218]\n",
      "16 [D loss: 0.179617, acc.: 95.31%] [G loss: 4.159556]\n",
      "17 [D loss: 0.239672, acc.: 95.31%] [G loss: 3.226823]\n",
      "18 [D loss: 0.311909, acc.: 89.06%] [G loss: 1.433367]\n",
      "19 [D loss: 0.172855, acc.: 96.88%] [G loss: 0.223960]\n",
      "20 [D loss: 0.118529, acc.: 98.44%] [G loss: 0.372197]\n",
      "21 [D loss: 0.094189, acc.: 100.00%] [G loss: 0.693270]\n",
      "22 [D loss: 0.108006, acc.: 98.44%] [G loss: 0.698831]\n",
      "23 [D loss: 0.099902, acc.: 98.44%] [G loss: 1.548922]\n",
      "24 [D loss: 0.163373, acc.: 96.88%] [G loss: 1.347826]\n",
      "25 [D loss: 0.127955, acc.: 93.75%] [G loss: 2.026904]\n",
      "26 [D loss: 0.114983, acc.: 98.44%] [G loss: 2.907133]\n",
      "27 [D loss: 0.214065, acc.: 89.06%] [G loss: 2.326648]\n",
      "28 [D loss: 0.229771, acc.: 95.31%] [G loss: 3.542269]\n",
      "29 [D loss: 0.305450, acc.: 92.19%] [G loss: 2.585636]\n",
      "30 [D loss: 0.250563, acc.: 92.19%] [G loss: 2.665968]\n",
      "31 [D loss: 0.330928, acc.: 85.94%] [G loss: 2.943860]\n",
      "32 [D loss: 0.281673, acc.: 92.19%] [G loss: 2.217875]\n",
      "33 [D loss: 0.594180, acc.: 71.88%] [G loss: 2.380239]\n",
      "34 [D loss: 0.246303, acc.: 89.06%] [G loss: 2.885897]\n",
      "35 [D loss: 0.632925, acc.: 73.44%] [G loss: 1.558324]\n",
      "36 [D loss: 0.412832, acc.: 81.25%] [G loss: 1.612264]\n",
      "37 [D loss: 0.105467, acc.: 96.88%] [G loss: 1.124222]\n",
      "38 [D loss: 0.116818, acc.: 93.75%] [G loss: 0.864906]\n",
      "39 [D loss: 0.218528, acc.: 90.62%] [G loss: 1.432902]\n",
      "40 [D loss: 0.355530, acc.: 81.25%] [G loss: 1.484726]\n",
      "41 [D loss: 0.300635, acc.: 89.06%] [G loss: 2.158460]\n",
      "42 [D loss: 0.520033, acc.: 79.69%] [G loss: 1.547893]\n",
      "43 [D loss: 0.933196, acc.: 51.56%] [G loss: 3.508931]\n",
      "44 [D loss: 1.163221, acc.: 56.25%] [G loss: 2.684864]\n",
      "45 [D loss: 1.169012, acc.: 46.88%] [G loss: 3.623944]\n",
      "46 [D loss: 0.383381, acc.: 84.38%] [G loss: 3.862890]\n",
      "47 [D loss: 0.367562, acc.: 79.69%] [G loss: 3.231878]\n",
      "48 [D loss: 0.241697, acc.: 90.62%] [G loss: 2.996865]\n",
      "49 [D loss: 0.468477, acc.: 76.56%] [G loss: 4.357199]\n",
      "50 [D loss: 0.814998, acc.: 60.94%] [G loss: 3.236161]\n",
      "51 [D loss: 0.747530, acc.: 68.75%] [G loss: 3.017791]\n",
      "52 [D loss: 1.270872, acc.: 43.75%] [G loss: 3.616168]\n",
      "53 [D loss: 0.754121, acc.: 68.75%] [G loss: 2.944932]\n",
      "54 [D loss: 0.976642, acc.: 54.69%] [G loss: 3.994014]\n",
      "55 [D loss: 0.531501, acc.: 75.00%] [G loss: 3.652619]\n",
      "56 [D loss: 1.376522, acc.: 43.75%] [G loss: 4.519967]\n",
      "57 [D loss: 0.347044, acc.: 85.94%] [G loss: 6.191829]\n",
      "58 [D loss: 0.349813, acc.: 87.50%] [G loss: 1.322133]\n",
      "59 [D loss: 0.261122, acc.: 89.06%] [G loss: 0.483759]\n",
      "60 [D loss: 0.663699, acc.: 65.62%] [G loss: 3.581022]\n",
      "61 [D loss: 0.342783, acc.: 89.06%] [G loss: 3.853504]\n",
      "62 [D loss: 0.771562, acc.: 64.06%] [G loss: 4.072434]\n",
      "63 [D loss: 0.299903, acc.: 89.06%] [G loss: 5.292239]\n",
      "64 [D loss: 0.502746, acc.: 78.12%] [G loss: 5.125881]\n",
      "65 [D loss: 1.062493, acc.: 62.50%] [G loss: 4.808235]\n",
      "66 [D loss: 0.990687, acc.: 59.38%] [G loss: 1.954342]\n",
      "67 [D loss: 0.562696, acc.: 78.12%] [G loss: 2.069947]\n",
      "68 [D loss: 0.827309, acc.: 60.94%] [G loss: 2.064491]\n",
      "69 [D loss: 0.638905, acc.: 70.31%] [G loss: 2.128757]\n",
      "70 [D loss: 0.758561, acc.: 60.94%] [G loss: 2.580210]\n",
      "71 [D loss: 0.812382, acc.: 65.62%] [G loss: 1.411352]\n",
      "72 [D loss: 0.749163, acc.: 59.38%] [G loss: 1.383218]\n",
      "73 [D loss: 0.455894, acc.: 82.81%] [G loss: 1.521564]\n",
      "74 [D loss: 0.447736, acc.: 73.44%] [G loss: 1.329156]\n",
      "75 [D loss: 0.565627, acc.: 73.44%] [G loss: 2.313648]\n",
      "76 [D loss: 1.149472, acc.: 50.00%] [G loss: 3.695913]\n",
      "77 [D loss: 0.541704, acc.: 75.00%] [G loss: 4.009274]\n",
      "78 [D loss: 0.350498, acc.: 85.94%] [G loss: 2.862660]\n",
      "79 [D loss: 0.422317, acc.: 79.69%] [G loss: 2.435347]\n",
      "80 [D loss: 0.283760, acc.: 89.06%] [G loss: 2.086566]\n",
      "81 [D loss: 0.547524, acc.: 78.12%] [G loss: 2.272506]\n",
      "82 [D loss: 0.504662, acc.: 78.12%] [G loss: 3.111766]\n",
      "83 [D loss: 0.651968, acc.: 67.19%] [G loss: 3.296113]\n",
      "84 [D loss: 0.564497, acc.: 73.44%] [G loss: 4.094635]\n",
      "85 [D loss: 0.873419, acc.: 54.69%] [G loss: 4.128565]\n",
      "86 [D loss: 0.537960, acc.: 73.44%] [G loss: 3.309135]\n",
      "87 [D loss: 0.448587, acc.: 84.38%] [G loss: 2.534065]\n",
      "88 [D loss: 0.242706, acc.: 89.06%] [G loss: 2.367605]\n",
      "89 [D loss: 0.221215, acc.: 93.75%] [G loss: 1.142886]\n",
      "90 [D loss: 0.315312, acc.: 89.06%] [G loss: 1.131180]\n",
      "91 [D loss: 0.240775, acc.: 95.31%] [G loss: 0.949285]\n",
      "92 [D loss: 0.652040, acc.: 73.44%] [G loss: 0.958673]\n",
      "93 [D loss: 1.088484, acc.: 53.12%] [G loss: 2.664749]\n",
      "94 [D loss: 0.784389, acc.: 62.50%] [G loss: 3.171651]\n",
      "95 [D loss: 0.600836, acc.: 76.56%] [G loss: 2.450595]\n",
      "96 [D loss: 0.499966, acc.: 78.12%] [G loss: 3.530122]\n",
      "97 [D loss: 0.320151, acc.: 87.50%] [G loss: 3.839857]\n",
      "98 [D loss: 0.696436, acc.: 76.56%] [G loss: 2.927429]\n",
      "99 [D loss: 0.819471, acc.: 70.31%] [G loss: 3.369058]\n",
      "100 [D loss: 0.944634, acc.: 62.50%] [G loss: 3.938922]\n",
      "101 [D loss: 0.514544, acc.: 75.00%] [G loss: 2.802368]\n",
      "102 [D loss: 0.219463, acc.: 96.88%] [G loss: 1.480162]\n",
      "103 [D loss: 0.375861, acc.: 81.25%] [G loss: 1.322577]\n",
      "104 [D loss: 0.131150, acc.: 96.88%] [G loss: 1.151289]\n",
      "105 [D loss: 0.244702, acc.: 90.62%] [G loss: 1.279724]\n",
      "106 [D loss: 0.219337, acc.: 93.75%] [G loss: 0.649346]\n",
      "107 [D loss: 0.430204, acc.: 73.44%] [G loss: 1.904070]\n",
      "108 [D loss: 0.226942, acc.: 95.31%] [G loss: 1.853912]\n",
      "109 [D loss: 0.366398, acc.: 84.38%] [G loss: 1.596155]\n",
      "110 [D loss: 0.200605, acc.: 93.75%] [G loss: 2.716007]\n",
      "111 [D loss: 0.484122, acc.: 76.56%] [G loss: 3.116871]\n",
      "112 [D loss: 0.297216, acc.: 89.06%] [G loss: 3.657463]\n",
      "113 [D loss: 0.332360, acc.: 90.62%] [G loss: 2.713758]\n",
      "114 [D loss: 0.152956, acc.: 93.75%] [G loss: 2.058492]\n",
      "115 [D loss: 0.259046, acc.: 89.06%] [G loss: 1.298654]\n",
      "116 [D loss: 0.623900, acc.: 68.75%] [G loss: 0.848045]\n",
      "117 [D loss: 0.423720, acc.: 82.81%] [G loss: 1.208701]\n",
      "118 [D loss: 0.339833, acc.: 87.50%] [G loss: 1.191915]\n",
      "119 [D loss: 0.288115, acc.: 89.06%] [G loss: 0.608825]\n",
      "120 [D loss: 0.547265, acc.: 79.69%] [G loss: 0.724061]\n",
      "121 [D loss: 0.240962, acc.: 90.62%] [G loss: 1.382170]\n",
      "122 [D loss: 0.533571, acc.: 81.25%] [G loss: 1.002019]\n",
      "123 [D loss: 0.651373, acc.: 64.06%] [G loss: 2.041393]\n",
      "124 [D loss: 0.487773, acc.: 79.69%] [G loss: 2.170835]\n",
      "125 [D loss: 0.760692, acc.: 68.75%] [G loss: 2.756610]\n",
      "126 [D loss: 0.964301, acc.: 53.12%] [G loss: 3.138957]\n",
      "127 [D loss: 0.909409, acc.: 59.38%] [G loss: 3.630563]\n",
      "128 [D loss: 1.005555, acc.: 57.81%] [G loss: 2.323576]\n",
      "129 [D loss: 0.416927, acc.: 84.38%] [G loss: 0.719293]\n",
      "130 [D loss: 0.244637, acc.: 92.19%] [G loss: 0.570918]\n",
      "131 [D loss: 0.124498, acc.: 98.44%] [G loss: 0.555093]\n",
      "132 [D loss: 0.149258, acc.: 95.31%] [G loss: 0.396825]\n",
      "133 [D loss: 0.087380, acc.: 98.44%] [G loss: 0.436554]\n",
      "134 [D loss: 0.068519, acc.: 100.00%] [G loss: 0.404859]\n",
      "135 [D loss: 0.309166, acc.: 90.62%] [G loss: 0.558690]\n",
      "136 [D loss: 0.285101, acc.: 89.06%] [G loss: 0.674905]\n",
      "137 [D loss: 0.184790, acc.: 93.75%] [G loss: 1.761148]\n",
      "138 [D loss: 0.337838, acc.: 82.81%] [G loss: 1.829004]\n",
      "139 [D loss: 0.599892, acc.: 65.62%] [G loss: 3.287652]\n",
      "140 [D loss: 0.679578, acc.: 60.94%] [G loss: 3.671455]\n",
      "141 [D loss: 1.024489, acc.: 46.88%] [G loss: 2.586105]\n",
      "142 [D loss: 0.943641, acc.: 56.25%] [G loss: 2.863841]\n",
      "143 [D loss: 0.479643, acc.: 78.12%] [G loss: 2.540074]\n",
      "144 [D loss: 1.008261, acc.: 60.94%] [G loss: 2.461430]\n",
      "145 [D loss: 0.276142, acc.: 87.50%] [G loss: 2.570653]\n",
      "146 [D loss: 0.700304, acc.: 68.75%] [G loss: 2.567955]\n",
      "147 [D loss: 0.566128, acc.: 79.69%] [G loss: 2.954753]\n",
      "148 [D loss: 0.840422, acc.: 60.94%] [G loss: 2.156946]\n",
      "149 [D loss: 0.256460, acc.: 90.62%] [G loss: 1.066500]\n",
      "150 [D loss: 0.413695, acc.: 84.38%] [G loss: 0.270248]\n",
      "151 [D loss: 0.585625, acc.: 73.44%] [G loss: 0.519491]\n",
      "152 [D loss: 0.160141, acc.: 96.88%] [G loss: 0.841578]\n",
      "153 [D loss: 1.075580, acc.: 54.69%] [G loss: 3.576032]\n",
      "154 [D loss: 1.082578, acc.: 56.25%] [G loss: 4.689926]\n",
      "155 [D loss: 0.712727, acc.: 62.50%] [G loss: 3.068844]\n",
      "156 [D loss: 1.044554, acc.: 60.94%] [G loss: 1.507729]\n",
      "157 [D loss: 0.393926, acc.: 84.38%] [G loss: 2.032967]\n",
      "158 [D loss: 0.883226, acc.: 57.81%] [G loss: 2.488845]\n",
      "159 [D loss: 1.479775, acc.: 35.94%] [G loss: 3.662898]\n",
      "160 [D loss: 1.014357, acc.: 56.25%] [G loss: 3.166594]\n",
      "161 [D loss: 0.941792, acc.: 57.81%] [G loss: 1.191946]\n",
      "162 [D loss: 0.496836, acc.: 79.69%] [G loss: 1.640575]\n",
      "163 [D loss: 0.191889, acc.: 92.19%] [G loss: 1.367813]\n",
      "164 [D loss: 0.167474, acc.: 93.75%] [G loss: 0.795832]\n",
      "165 [D loss: 0.402715, acc.: 89.06%] [G loss: 2.658085]\n",
      "166 [D loss: 0.630680, acc.: 70.31%] [G loss: 2.552431]\n",
      "167 [D loss: 0.553745, acc.: 73.44%] [G loss: 3.404484]\n",
      "168 [D loss: 0.585401, acc.: 76.56%] [G loss: 1.797659]\n",
      "169 [D loss: 1.541523, acc.: 32.81%] [G loss: 1.745222]\n",
      "170 [D loss: 0.262036, acc.: 90.62%] [G loss: 1.429133]\n",
      "171 [D loss: 0.371675, acc.: 85.94%] [G loss: 0.753983]\n",
      "172 [D loss: 0.363750, acc.: 79.69%] [G loss: 1.169896]\n",
      "173 [D loss: 0.160595, acc.: 95.31%] [G loss: 1.644978]\n",
      "174 [D loss: 0.591533, acc.: 71.88%] [G loss: 1.864374]\n",
      "175 [D loss: 0.347821, acc.: 81.25%] [G loss: 3.586989]\n",
      "176 [D loss: 1.103546, acc.: 45.31%] [G loss: 1.828293]\n",
      "177 [D loss: 0.596828, acc.: 73.44%] [G loss: 0.326573]\n",
      "178 [D loss: 0.404118, acc.: 81.25%] [G loss: 0.382272]\n",
      "179 [D loss: 0.198930, acc.: 92.19%] [G loss: 0.239491]\n",
      "180 [D loss: 0.185137, acc.: 96.88%] [G loss: 0.334823]\n",
      "181 [D loss: 0.277075, acc.: 90.62%] [G loss: 1.530778]\n",
      "182 [D loss: 0.363907, acc.: 82.81%] [G loss: 1.989998]\n",
      "183 [D loss: 1.191721, acc.: 53.12%] [G loss: 2.179928]\n",
      "184 [D loss: 0.573312, acc.: 75.00%] [G loss: 3.093525]\n",
      "185 [D loss: 1.027277, acc.: 54.69%] [G loss: 1.415700]\n",
      "186 [D loss: 0.922691, acc.: 57.81%] [G loss: 2.508744]\n",
      "187 [D loss: 0.831012, acc.: 57.81%] [G loss: 1.807353]\n",
      "188 [D loss: 0.734909, acc.: 65.62%] [G loss: 2.383217]\n",
      "189 [D loss: 0.466953, acc.: 81.25%] [G loss: 2.508930]\n",
      "190 [D loss: 0.486602, acc.: 75.00%] [G loss: 1.597142]\n",
      "191 [D loss: 0.563389, acc.: 78.12%] [G loss: 1.708429]\n",
      "192 [D loss: 0.516743, acc.: 76.56%] [G loss: 2.255413]\n",
      "193 [D loss: 1.505102, acc.: 37.50%] [G loss: 0.988300]\n",
      "194 [D loss: 0.919154, acc.: 51.56%] [G loss: 2.089173]\n",
      "195 [D loss: 1.248376, acc.: 45.31%] [G loss: 2.447315]\n",
      "196 [D loss: 1.564094, acc.: 29.69%] [G loss: 1.983961]\n",
      "197 [D loss: 0.629454, acc.: 68.75%] [G loss: 2.304328]\n",
      "198 [D loss: 0.707920, acc.: 68.75%] [G loss: 0.827251]\n",
      "199 [D loss: 0.646088, acc.: 70.31%] [G loss: 1.347460]\n",
      "200 [D loss: 0.518975, acc.: 75.00%] [G loss: 0.760356]\n",
      "201 [D loss: 0.381169, acc.: 81.25%] [G loss: 0.604259]\n",
      "202 [D loss: 0.444670, acc.: 76.56%] [G loss: 0.970217]\n",
      "203 [D loss: 0.595429, acc.: 76.56%] [G loss: 1.063261]\n",
      "204 [D loss: 0.450557, acc.: 78.12%] [G loss: 1.918528]\n",
      "205 [D loss: 1.133689, acc.: 40.62%] [G loss: 2.192201]\n",
      "206 [D loss: 1.068328, acc.: 35.94%] [G loss: 2.404163]\n",
      "207 [D loss: 1.544957, acc.: 31.25%] [G loss: 1.818217]\n",
      "208 [D loss: 0.868081, acc.: 62.50%] [G loss: 1.571481]\n",
      "209 [D loss: 0.511482, acc.: 71.88%] [G loss: 1.432271]\n",
      "210 [D loss: 0.324252, acc.: 87.50%] [G loss: 1.594087]\n",
      "211 [D loss: 0.590631, acc.: 73.44%] [G loss: 1.031156]\n",
      "212 [D loss: 0.261792, acc.: 87.50%] [G loss: 0.642024]\n",
      "213 [D loss: 0.224506, acc.: 92.19%] [G loss: 0.395547]\n",
      "214 [D loss: 0.199706, acc.: 93.75%] [G loss: 0.451780]\n",
      "215 [D loss: 0.194698, acc.: 95.31%] [G loss: 0.519599]\n",
      "216 [D loss: 0.172044, acc.: 92.19%] [G loss: 0.619029]\n",
      "217 [D loss: 1.102412, acc.: 48.44%] [G loss: 0.786619]\n",
      "218 [D loss: 0.444713, acc.: 82.81%] [G loss: 1.276023]\n",
      "219 [D loss: 0.736511, acc.: 64.06%] [G loss: 1.711825]\n",
      "220 [D loss: 1.062388, acc.: 50.00%] [G loss: 1.195009]\n",
      "221 [D loss: 1.119603, acc.: 50.00%] [G loss: 2.027978]\n",
      "222 [D loss: 0.594616, acc.: 79.69%] [G loss: 2.080766]\n",
      "223 [D loss: 0.932005, acc.: 51.56%] [G loss: 2.368645]\n",
      "224 [D loss: 0.773368, acc.: 64.06%] [G loss: 1.897355]\n",
      "225 [D loss: 0.791471, acc.: 54.69%] [G loss: 1.594584]\n",
      "226 [D loss: 0.897192, acc.: 54.69%] [G loss: 2.009158]\n",
      "227 [D loss: 1.050738, acc.: 42.19%] [G loss: 2.191494]\n",
      "228 [D loss: 1.004189, acc.: 48.44%] [G loss: 2.173613]\n",
      "229 [D loss: 0.748975, acc.: 65.62%] [G loss: 2.955811]\n",
      "230 [D loss: 1.370700, acc.: 48.44%] [G loss: 1.768973]\n",
      "231 [D loss: 0.753365, acc.: 56.25%] [G loss: 2.264428]\n",
      "232 [D loss: 0.648633, acc.: 71.88%] [G loss: 1.545569]\n",
      "233 [D loss: 0.599242, acc.: 71.88%] [G loss: 0.631154]\n",
      "234 [D loss: 0.330245, acc.: 82.81%] [G loss: 0.591607]\n",
      "235 [D loss: 0.414308, acc.: 81.25%] [G loss: 0.817149]\n",
      "236 [D loss: 0.292729, acc.: 87.50%] [G loss: 0.835717]\n",
      "237 [D loss: 0.331624, acc.: 87.50%] [G loss: 0.760311]\n",
      "238 [D loss: 0.842801, acc.: 54.69%] [G loss: 2.116244]\n",
      "239 [D loss: 0.566262, acc.: 73.44%] [G loss: 1.855297]\n",
      "240 [D loss: 0.597022, acc.: 65.62%] [G loss: 0.787536]\n",
      "241 [D loss: 0.335260, acc.: 84.38%] [G loss: 1.245840]\n",
      "242 [D loss: 1.195901, acc.: 39.06%] [G loss: 2.254824]\n",
      "243 [D loss: 0.719345, acc.: 57.81%] [G loss: 2.519038]\n",
      "244 [D loss: 0.988255, acc.: 60.94%] [G loss: 1.147401]\n",
      "245 [D loss: 0.683148, acc.: 65.62%] [G loss: 2.218117]\n",
      "246 [D loss: 0.513259, acc.: 76.56%] [G loss: 0.995818]\n",
      "247 [D loss: 0.515110, acc.: 70.31%] [G loss: 0.214803]\n",
      "248 [D loss: 0.293923, acc.: 85.94%] [G loss: 0.216906]\n",
      "249 [D loss: 0.096706, acc.: 98.44%] [G loss: 0.240422]\n",
      "250 [D loss: 0.154129, acc.: 95.31%] [G loss: 0.156898]\n",
      "251 [D loss: 0.690389, acc.: 68.75%] [G loss: 0.359888]\n",
      "252 [D loss: 0.181730, acc.: 92.19%] [G loss: 0.714309]\n",
      "253 [D loss: 0.983757, acc.: 51.56%] [G loss: 0.639544]\n",
      "254 [D loss: 0.620815, acc.: 62.50%] [G loss: 1.071721]\n",
      "255 [D loss: 0.373495, acc.: 81.25%] [G loss: 0.747916]\n",
      "256 [D loss: 1.753421, acc.: 35.94%] [G loss: 2.321774]\n",
      "257 [D loss: 1.303032, acc.: 40.62%] [G loss: 2.038568]\n",
      "258 [D loss: 0.558386, acc.: 73.44%] [G loss: 2.480385]\n",
      "259 [D loss: 0.759948, acc.: 56.25%] [G loss: 1.859979]\n",
      "260 [D loss: 0.405595, acc.: 81.25%] [G loss: 1.183507]\n",
      "261 [D loss: 0.508712, acc.: 73.44%] [G loss: 1.016284]\n",
      "262 [D loss: 0.455640, acc.: 78.12%] [G loss: 2.058055]\n",
      "263 [D loss: 0.245247, acc.: 89.06%] [G loss: 2.317038]\n",
      "264 [D loss: 0.511736, acc.: 73.44%] [G loss: 1.523953]\n",
      "265 [D loss: 0.152849, acc.: 96.88%] [G loss: 1.474744]\n",
      "266 [D loss: 0.334602, acc.: 87.50%] [G loss: 0.786138]\n",
      "267 [D loss: 0.241977, acc.: 92.19%] [G loss: 0.870342]\n",
      "268 [D loss: 0.379809, acc.: 84.38%] [G loss: 0.825813]\n",
      "269 [D loss: 0.290456, acc.: 89.06%] [G loss: 1.100682]\n",
      "270 [D loss: 0.281376, acc.: 90.62%] [G loss: 0.592423]\n",
      "271 [D loss: 1.362860, acc.: 29.69%] [G loss: 1.067006]\n",
      "272 [D loss: 0.718050, acc.: 65.62%] [G loss: 1.008067]\n",
      "273 [D loss: 0.453874, acc.: 73.44%] [G loss: 1.498766]\n",
      "274 [D loss: 0.455665, acc.: 79.69%] [G loss: 1.411289]\n",
      "275 [D loss: 0.711557, acc.: 65.62%] [G loss: 1.101382]\n",
      "276 [D loss: 1.094658, acc.: 43.75%] [G loss: 2.422628]\n",
      "277 [D loss: 1.361110, acc.: 46.88%] [G loss: 1.382747]\n",
      "278 [D loss: 0.461203, acc.: 78.12%] [G loss: 0.994328]\n",
      "279 [D loss: 0.845813, acc.: 54.69%] [G loss: 2.101712]\n",
      "280 [D loss: 0.373471, acc.: 84.38%] [G loss: 1.281499]\n",
      "281 [D loss: 0.277115, acc.: 92.19%] [G loss: 0.610838]\n",
      "282 [D loss: 0.593991, acc.: 71.88%] [G loss: 2.213251]\n",
      "283 [D loss: 0.480933, acc.: 82.81%] [G loss: 1.686560]\n",
      "284 [D loss: 0.877811, acc.: 48.44%] [G loss: 1.153949]\n",
      "285 [D loss: 0.339438, acc.: 85.94%] [G loss: 2.116284]\n",
      "286 [D loss: 0.279184, acc.: 90.62%] [G loss: 0.618950]\n",
      "287 [D loss: 1.356978, acc.: 39.06%] [G loss: 1.738777]\n",
      "288 [D loss: 0.402511, acc.: 82.81%] [G loss: 2.144952]\n",
      "289 [D loss: 0.629681, acc.: 62.50%] [G loss: 2.150252]\n",
      "290 [D loss: 1.201741, acc.: 40.62%] [G loss: 1.928888]\n",
      "291 [D loss: 0.635917, acc.: 71.88%] [G loss: 1.113197]\n",
      "292 [D loss: 0.258929, acc.: 87.50%] [G loss: 0.372212]\n",
      "293 [D loss: 0.179207, acc.: 95.31%] [G loss: 0.148233]\n",
      "294 [D loss: 0.282550, acc.: 82.81%] [G loss: 0.173964]\n",
      "295 [D loss: 0.180438, acc.: 92.19%] [G loss: 0.193752]\n",
      "296 [D loss: 0.110339, acc.: 93.75%] [G loss: 0.099215]\n",
      "297 [D loss: 0.219505, acc.: 90.62%] [G loss: 0.264765]\n",
      "298 [D loss: 0.657167, acc.: 70.31%] [G loss: 1.088604]\n",
      "299 [D loss: 0.327724, acc.: 84.38%] [G loss: 0.668401]\n",
      "300 [D loss: 0.660627, acc.: 57.81%] [G loss: 1.266129]\n",
      "301 [D loss: 0.599826, acc.: 68.75%] [G loss: 0.736148]\n",
      "302 [D loss: 0.531136, acc.: 79.69%] [G loss: 1.297293]\n",
      "303 [D loss: 0.297161, acc.: 85.94%] [G loss: 1.331031]\n",
      "304 [D loss: 0.261544, acc.: 90.62%] [G loss: 0.758264]\n",
      "305 [D loss: 0.450753, acc.: 78.12%] [G loss: 0.728916]\n",
      "306 [D loss: 0.345753, acc.: 87.50%] [G loss: 0.862363]\n",
      "307 [D loss: 0.247784, acc.: 90.62%] [G loss: 0.710859]\n",
      "308 [D loss: 2.313310, acc.: 14.06%] [G loss: 0.337072]\n",
      "309 [D loss: 0.181848, acc.: 95.31%] [G loss: 0.743628]\n",
      "310 [D loss: 0.262891, acc.: 89.06%] [G loss: 0.826408]\n",
      "311 [D loss: 1.055611, acc.: 48.44%] [G loss: 1.098001]\n",
      "312 [D loss: 0.478470, acc.: 76.56%] [G loss: 1.717675]\n",
      "313 [D loss: 0.858525, acc.: 50.00%] [G loss: 0.788205]\n",
      "314 [D loss: 0.214312, acc.: 92.19%] [G loss: 0.733013]\n",
      "315 [D loss: 0.450979, acc.: 82.81%] [G loss: 0.587045]\n",
      "316 [D loss: 0.716603, acc.: 64.06%] [G loss: 0.576296]\n",
      "317 [D loss: 0.527204, acc.: 70.31%] [G loss: 1.124274]\n",
      "318 [D loss: 0.416254, acc.: 75.00%] [G loss: 0.445629]\n",
      "319 [D loss: 1.958121, acc.: 40.62%] [G loss: 1.269802]\n",
      "320 [D loss: 0.737473, acc.: 59.38%] [G loss: 2.555962]\n",
      "321 [D loss: 2.343171, acc.: 17.19%] [G loss: 1.651903]\n",
      "322 [D loss: 1.431777, acc.: 35.94%] [G loss: 1.658566]\n",
      "323 [D loss: 0.766524, acc.: 59.38%] [G loss: 2.859400]\n",
      "324 [D loss: 0.842397, acc.: 51.56%] [G loss: 2.069658]\n",
      "325 [D loss: 0.978832, acc.: 57.81%] [G loss: 0.977163]\n",
      "326 [D loss: 0.837664, acc.: 57.81%] [G loss: 1.653533]\n",
      "327 [D loss: 0.704856, acc.: 67.19%] [G loss: 1.078032]\n",
      "328 [D loss: 0.894251, acc.: 54.69%] [G loss: 0.787537]\n",
      "329 [D loss: 0.846390, acc.: 57.81%] [G loss: 1.292505]\n",
      "330 [D loss: 0.621929, acc.: 73.44%] [G loss: 0.542455]\n",
      "331 [D loss: 0.934245, acc.: 54.69%] [G loss: 1.126141]\n",
      "332 [D loss: 0.462686, acc.: 78.12%] [G loss: 1.814836]\n",
      "333 [D loss: 1.244836, acc.: 37.50%] [G loss: 1.869457]\n",
      "334 [D loss: 0.849020, acc.: 57.81%] [G loss: 0.927489]\n",
      "335 [D loss: 0.852532, acc.: 56.25%] [G loss: 1.648461]\n",
      "336 [D loss: 0.468592, acc.: 79.69%] [G loss: 1.095875]\n",
      "337 [D loss: 0.736262, acc.: 65.62%] [G loss: 1.657284]\n",
      "338 [D loss: 1.012476, acc.: 45.31%] [G loss: 1.832969]\n",
      "339 [D loss: 1.361694, acc.: 34.38%] [G loss: 1.835984]\n",
      "340 [D loss: 0.705536, acc.: 70.31%] [G loss: 2.182549]\n",
      "341 [D loss: 0.551132, acc.: 76.56%] [G loss: 2.074428]\n",
      "342 [D loss: 0.722066, acc.: 59.38%] [G loss: 1.673293]\n",
      "343 [D loss: 0.837469, acc.: 59.38%] [G loss: 1.641613]\n",
      "344 [D loss: 0.605156, acc.: 68.75%] [G loss: 1.615289]\n",
      "345 [D loss: 0.883659, acc.: 57.81%] [G loss: 1.175566]\n",
      "346 [D loss: 0.729726, acc.: 65.62%] [G loss: 1.194379]\n",
      "347 [D loss: 0.694688, acc.: 62.50%] [G loss: 1.231607]\n",
      "348 [D loss: 0.543019, acc.: 70.31%] [G loss: 1.475969]\n",
      "349 [D loss: 0.938789, acc.: 54.69%] [G loss: 0.889134]\n",
      "350 [D loss: 0.422535, acc.: 85.94%] [G loss: 0.817770]\n",
      "351 [D loss: 0.824491, acc.: 57.81%] [G loss: 1.571062]\n",
      "352 [D loss: 0.728574, acc.: 64.06%] [G loss: 1.188323]\n",
      "353 [D loss: 0.855272, acc.: 50.00%] [G loss: 1.339278]\n",
      "354 [D loss: 1.046184, acc.: 48.44%] [G loss: 1.603960]\n",
      "355 [D loss: 0.909900, acc.: 56.25%] [G loss: 1.241305]\n",
      "356 [D loss: 0.943358, acc.: 54.69%] [G loss: 1.473774]\n",
      "357 [D loss: 0.805896, acc.: 59.38%] [G loss: 1.921993]\n",
      "358 [D loss: 0.440585, acc.: 82.81%] [G loss: 1.102031]\n",
      "359 [D loss: 0.497351, acc.: 70.31%] [G loss: 0.872272]\n",
      "360 [D loss: 0.813069, acc.: 57.81%] [G loss: 1.184997]\n",
      "361 [D loss: 0.985767, acc.: 35.94%] [G loss: 1.496999]\n",
      "362 [D loss: 0.335142, acc.: 87.50%] [G loss: 1.136606]\n",
      "363 [D loss: 0.667326, acc.: 60.94%] [G loss: 1.024973]\n",
      "364 [D loss: 0.465046, acc.: 73.44%] [G loss: 1.603820]\n",
      "365 [D loss: 0.631630, acc.: 67.19%] [G loss: 2.270109]\n",
      "366 [D loss: 0.764641, acc.: 51.56%] [G loss: 1.912939]\n",
      "367 [D loss: 0.987092, acc.: 43.75%] [G loss: 2.111624]\n",
      "368 [D loss: 0.904059, acc.: 43.75%] [G loss: 2.119098]\n",
      "369 [D loss: 0.676968, acc.: 65.62%] [G loss: 0.957449]\n",
      "370 [D loss: 0.672185, acc.: 70.31%] [G loss: 1.668135]\n",
      "371 [D loss: 0.322881, acc.: 89.06%] [G loss: 2.170270]\n",
      "372 [D loss: 0.792984, acc.: 62.50%] [G loss: 2.089844]\n",
      "373 [D loss: 0.579429, acc.: 75.00%] [G loss: 1.533633]\n",
      "374 [D loss: 0.543858, acc.: 78.12%] [G loss: 1.461045]\n",
      "375 [D loss: 0.827345, acc.: 51.56%] [G loss: 1.387152]\n",
      "376 [D loss: 0.594235, acc.: 67.19%] [G loss: 1.367176]\n",
      "377 [D loss: 0.727227, acc.: 62.50%] [G loss: 1.575370]\n",
      "378 [D loss: 0.756196, acc.: 59.38%] [G loss: 1.368756]\n",
      "379 [D loss: 0.568136, acc.: 71.88%] [G loss: 1.933634]\n",
      "380 [D loss: 0.559458, acc.: 82.81%] [G loss: 1.819481]\n",
      "381 [D loss: 0.535135, acc.: 76.56%] [G loss: 1.960025]\n",
      "382 [D loss: 0.563156, acc.: 75.00%] [G loss: 1.368828]\n",
      "383 [D loss: 0.535160, acc.: 71.88%] [G loss: 0.514575]\n",
      "384 [D loss: 0.390544, acc.: 81.25%] [G loss: 0.922379]\n",
      "385 [D loss: 0.339989, acc.: 89.06%] [G loss: 1.566802]\n",
      "386 [D loss: 0.752789, acc.: 53.12%] [G loss: 1.101197]\n",
      "387 [D loss: 0.679687, acc.: 67.19%] [G loss: 1.449783]\n",
      "388 [D loss: 1.135696, acc.: 46.88%] [G loss: 1.782776]\n",
      "389 [D loss: 0.750957, acc.: 62.50%] [G loss: 1.921217]\n",
      "390 [D loss: 0.697852, acc.: 65.62%] [G loss: 1.934151]\n",
      "391 [D loss: 1.186621, acc.: 40.62%] [G loss: 1.668431]\n",
      "392 [D loss: 0.732168, acc.: 68.75%] [G loss: 1.664024]\n",
      "393 [D loss: 1.036480, acc.: 48.44%] [G loss: 1.270312]\n",
      "394 [D loss: 0.934772, acc.: 54.69%] [G loss: 0.922641]\n",
      "395 [D loss: 0.900684, acc.: 53.12%] [G loss: 1.236945]\n",
      "396 [D loss: 0.670184, acc.: 73.44%] [G loss: 1.203377]\n",
      "397 [D loss: 0.760399, acc.: 62.50%] [G loss: 0.701818]\n",
      "398 [D loss: 0.535952, acc.: 73.44%] [G loss: 0.549465]\n",
      "399 [D loss: 0.325696, acc.: 89.06%] [G loss: 0.646982]\n",
      "400 [D loss: 0.276062, acc.: 87.50%] [G loss: 0.448160]\n",
      "401 [D loss: 0.495663, acc.: 71.88%] [G loss: 0.790395]\n",
      "402 [D loss: 0.431742, acc.: 81.25%] [G loss: 0.879985]\n",
      "403 [D loss: 0.486172, acc.: 75.00%] [G loss: 1.375749]\n",
      "404 [D loss: 0.489016, acc.: 79.69%] [G loss: 1.403936]\n",
      "405 [D loss: 0.698595, acc.: 57.81%] [G loss: 1.138057]\n",
      "406 [D loss: 0.581740, acc.: 64.06%] [G loss: 0.828038]\n",
      "407 [D loss: 0.423602, acc.: 82.81%] [G loss: 1.280684]\n",
      "408 [D loss: 0.884187, acc.: 56.25%] [G loss: 2.153341]\n",
      "409 [D loss: 0.722444, acc.: 62.50%] [G loss: 1.532824]\n",
      "410 [D loss: 0.645873, acc.: 68.75%] [G loss: 1.842291]\n",
      "411 [D loss: 0.972621, acc.: 46.88%] [G loss: 2.135031]\n",
      "412 [D loss: 1.014662, acc.: 54.69%] [G loss: 2.062562]\n",
      "413 [D loss: 0.727911, acc.: 54.69%] [G loss: 2.049237]\n",
      "414 [D loss: 0.398837, acc.: 84.38%] [G loss: 1.675850]\n",
      "415 [D loss: 0.898959, acc.: 50.00%] [G loss: 1.337276]\n",
      "416 [D loss: 0.662526, acc.: 70.31%] [G loss: 1.568024]\n",
      "417 [D loss: 0.780558, acc.: 53.12%] [G loss: 2.104527]\n",
      "418 [D loss: 0.759347, acc.: 57.81%] [G loss: 1.361750]\n",
      "419 [D loss: 0.537629, acc.: 70.31%] [G loss: 1.305067]\n",
      "420 [D loss: 0.639038, acc.: 70.31%] [G loss: 0.382300]\n",
      "421 [D loss: 0.687458, acc.: 64.06%] [G loss: 0.721296]\n",
      "422 [D loss: 0.690361, acc.: 60.94%] [G loss: 1.280065]\n",
      "423 [D loss: 0.387545, acc.: 82.81%] [G loss: 1.159815]\n",
      "424 [D loss: 1.192502, acc.: 31.25%] [G loss: 1.648071]\n",
      "425 [D loss: 0.575585, acc.: 67.19%] [G loss: 2.497391]\n",
      "426 [D loss: 0.532637, acc.: 68.75%] [G loss: 1.744902]\n",
      "427 [D loss: 1.315390, acc.: 34.38%] [G loss: 0.896183]\n",
      "428 [D loss: 0.385513, acc.: 85.94%] [G loss: 1.516034]\n",
      "429 [D loss: 0.705588, acc.: 67.19%] [G loss: 1.110824]\n",
      "430 [D loss: 1.807206, acc.: 18.75%] [G loss: 0.520101]\n",
      "431 [D loss: 0.388870, acc.: 84.38%] [G loss: 1.073716]\n",
      "432 [D loss: 0.253549, acc.: 90.62%] [G loss: 1.432651]\n",
      "433 [D loss: 0.493349, acc.: 79.69%] [G loss: 1.273157]\n",
      "434 [D loss: 0.897437, acc.: 45.31%] [G loss: 1.932815]\n",
      "435 [D loss: 0.600425, acc.: 70.31%] [G loss: 1.595119]\n",
      "436 [D loss: 1.097393, acc.: 43.75%] [G loss: 0.901572]\n",
      "437 [D loss: 0.735464, acc.: 59.38%] [G loss: 0.902312]\n",
      "438 [D loss: 0.746217, acc.: 59.38%] [G loss: 0.674119]\n",
      "439 [D loss: 1.477164, acc.: 32.81%] [G loss: 1.031903]\n",
      "440 [D loss: 0.504154, acc.: 75.00%] [G loss: 1.313584]\n",
      "441 [D loss: 0.738430, acc.: 68.75%] [G loss: 1.135945]\n",
      "442 [D loss: 0.889919, acc.: 53.12%] [G loss: 0.944218]\n",
      "443 [D loss: 0.679413, acc.: 60.94%] [G loss: 1.590867]\n",
      "444 [D loss: 1.052118, acc.: 46.88%] [G loss: 1.385126]\n",
      "445 [D loss: 1.104285, acc.: 46.88%] [G loss: 1.584658]\n",
      "446 [D loss: 0.507556, acc.: 76.56%] [G loss: 1.344097]\n",
      "447 [D loss: 1.112793, acc.: 39.06%] [G loss: 1.639465]\n",
      "448 [D loss: 1.019329, acc.: 43.75%] [G loss: 1.605938]\n",
      "449 [D loss: 0.370210, acc.: 84.38%] [G loss: 0.861142]\n",
      "450 [D loss: 0.366466, acc.: 82.81%] [G loss: 0.428091]\n",
      "451 [D loss: 0.460606, acc.: 78.12%] [G loss: 0.302346]\n",
      "452 [D loss: 0.548349, acc.: 76.56%] [G loss: 0.850998]\n",
      "453 [D loss: 0.355861, acc.: 85.94%] [G loss: 0.798138]\n",
      "454 [D loss: 0.918662, acc.: 50.00%] [G loss: 1.487990]\n",
      "455 [D loss: 0.543674, acc.: 67.19%] [G loss: 1.105184]\n",
      "456 [D loss: 0.687588, acc.: 67.19%] [G loss: 1.287113]\n",
      "457 [D loss: 0.986877, acc.: 45.31%] [G loss: 0.983985]\n",
      "458 [D loss: 0.819652, acc.: 64.06%] [G loss: 2.149379]\n",
      "459 [D loss: 0.444699, acc.: 81.25%] [G loss: 2.088516]\n",
      "460 [D loss: 1.052105, acc.: 48.44%] [G loss: 1.424286]\n",
      "461 [D loss: 0.701149, acc.: 65.62%] [G loss: 1.582497]\n",
      "462 [D loss: 0.790900, acc.: 62.50%] [G loss: 2.372655]\n",
      "463 [D loss: 0.854846, acc.: 62.50%] [G loss: 2.001277]\n",
      "464 [D loss: 0.658926, acc.: 65.62%] [G loss: 1.529958]\n",
      "465 [D loss: 0.751710, acc.: 62.50%] [G loss: 1.603551]\n",
      "466 [D loss: 0.610143, acc.: 75.00%] [G loss: 1.614522]\n",
      "467 [D loss: 0.439215, acc.: 75.00%] [G loss: 1.142114]\n",
      "468 [D loss: 0.399059, acc.: 81.25%] [G loss: 1.039366]\n",
      "469 [D loss: 0.780992, acc.: 60.94%] [G loss: 1.084213]\n",
      "470 [D loss: 0.715088, acc.: 62.50%] [G loss: 0.936267]\n",
      "471 [D loss: 0.926108, acc.: 51.56%] [G loss: 1.035924]\n",
      "472 [D loss: 0.811278, acc.: 53.12%] [G loss: 0.838935]\n",
      "473 [D loss: 0.649010, acc.: 62.50%] [G loss: 1.358527]\n",
      "474 [D loss: 0.592116, acc.: 60.94%] [G loss: 1.475147]\n",
      "475 [D loss: 1.228838, acc.: 42.19%] [G loss: 0.683003]\n",
      "476 [D loss: 0.987852, acc.: 48.44%] [G loss: 1.492281]\n",
      "477 [D loss: 0.471628, acc.: 79.69%] [G loss: 1.359549]\n",
      "478 [D loss: 1.179324, acc.: 28.12%] [G loss: 1.420895]\n",
      "479 [D loss: 0.678372, acc.: 75.00%] [G loss: 0.994637]\n",
      "480 [D loss: 0.975969, acc.: 40.62%] [G loss: 1.883029]\n",
      "481 [D loss: 0.714644, acc.: 67.19%] [G loss: 1.170796]\n",
      "482 [D loss: 0.727528, acc.: 59.38%] [G loss: 1.271474]\n",
      "483 [D loss: 0.438688, acc.: 76.56%] [G loss: 2.007890]\n",
      "484 [D loss: 1.085715, acc.: 45.31%] [G loss: 2.103224]\n",
      "485 [D loss: 1.094342, acc.: 50.00%] [G loss: 1.373243]\n",
      "486 [D loss: 0.877090, acc.: 51.56%] [G loss: 2.240564]\n",
      "487 [D loss: 0.562223, acc.: 71.88%] [G loss: 1.637605]\n",
      "488 [D loss: 0.706010, acc.: 62.50%] [G loss: 1.896542]\n",
      "489 [D loss: 0.845789, acc.: 46.88%] [G loss: 1.254539]\n",
      "490 [D loss: 1.048753, acc.: 50.00%] [G loss: 1.680993]\n",
      "491 [D loss: 0.826457, acc.: 59.38%] [G loss: 1.740581]\n",
      "492 [D loss: 0.626885, acc.: 62.50%] [G loss: 1.504702]\n",
      "493 [D loss: 0.785208, acc.: 54.69%] [G loss: 1.561235]\n",
      "494 [D loss: 0.936300, acc.: 45.31%] [G loss: 1.940410]\n",
      "495 [D loss: 0.974368, acc.: 45.31%] [G loss: 1.529541]\n",
      "496 [D loss: 0.784820, acc.: 57.81%] [G loss: 1.704560]\n",
      "497 [D loss: 0.641226, acc.: 70.31%] [G loss: 2.010033]\n",
      "498 [D loss: 0.635808, acc.: 65.62%] [G loss: 1.288006]\n",
      "499 [D loss: 0.355761, acc.: 81.25%] [G loss: 0.884263]\n",
      "500 [D loss: 0.431008, acc.: 76.56%] [G loss: 0.525318]\n",
      "501 [D loss: 0.436766, acc.: 79.69%] [G loss: 0.933574]\n",
      "502 [D loss: 0.302115, acc.: 90.62%] [G loss: 1.744179]\n",
      "503 [D loss: 0.278213, acc.: 84.38%] [G loss: 1.256178]\n",
      "504 [D loss: 0.746389, acc.: 57.81%] [G loss: 0.744031]\n",
      "505 [D loss: 0.471886, acc.: 81.25%] [G loss: 1.519349]\n",
      "506 [D loss: 0.594159, acc.: 75.00%] [G loss: 1.310861]\n",
      "507 [D loss: 0.577353, acc.: 68.75%] [G loss: 1.170379]\n",
      "508 [D loss: 0.627703, acc.: 68.75%] [G loss: 1.686610]\n",
      "509 [D loss: 0.445893, acc.: 79.69%] [G loss: 1.493902]\n",
      "510 [D loss: 0.739689, acc.: 57.81%] [G loss: 1.563072]\n",
      "511 [D loss: 0.791091, acc.: 60.94%] [G loss: 1.045693]\n",
      "512 [D loss: 0.616197, acc.: 70.31%] [G loss: 1.763986]\n",
      "513 [D loss: 0.574148, acc.: 68.75%] [G loss: 2.069778]\n",
      "514 [D loss: 0.870630, acc.: 54.69%] [G loss: 1.549264]\n",
      "515 [D loss: 1.027828, acc.: 50.00%] [G loss: 1.244939]\n",
      "516 [D loss: 1.050656, acc.: 32.81%] [G loss: 1.446509]\n",
      "517 [D loss: 0.884266, acc.: 53.12%] [G loss: 1.638294]\n",
      "518 [D loss: 0.571194, acc.: 73.44%] [G loss: 1.191309]\n",
      "519 [D loss: 0.544424, acc.: 71.88%] [G loss: 0.885791]\n",
      "520 [D loss: 0.408526, acc.: 84.38%] [G loss: 0.835497]\n",
      "521 [D loss: 0.331080, acc.: 84.38%] [G loss: 0.688617]\n",
      "522 [D loss: 0.185259, acc.: 92.19%] [G loss: 0.607651]\n",
      "523 [D loss: 0.368180, acc.: 87.50%] [G loss: 0.551508]\n",
      "524 [D loss: 0.396478, acc.: 79.69%] [G loss: 1.057261]\n",
      "525 [D loss: 0.505868, acc.: 81.25%] [G loss: 1.122037]\n",
      "526 [D loss: 0.489486, acc.: 79.69%] [G loss: 1.509408]\n",
      "527 [D loss: 0.730205, acc.: 62.50%] [G loss: 1.232786]\n",
      "528 [D loss: 0.264152, acc.: 93.75%] [G loss: 1.115139]\n",
      "529 [D loss: 0.661926, acc.: 62.50%] [G loss: 1.714855]\n",
      "530 [D loss: 0.845668, acc.: 53.12%] [G loss: 1.410246]\n",
      "531 [D loss: 0.655507, acc.: 67.19%] [G loss: 1.697136]\n",
      "532 [D loss: 0.681198, acc.: 65.62%] [G loss: 0.977326]\n",
      "533 [D loss: 0.416948, acc.: 82.81%] [G loss: 1.323879]\n",
      "534 [D loss: 0.448992, acc.: 79.69%] [G loss: 2.014259]\n",
      "535 [D loss: 0.634166, acc.: 62.50%] [G loss: 1.357970]\n",
      "536 [D loss: 1.078363, acc.: 37.50%] [G loss: 1.512271]\n",
      "537 [D loss: 0.507428, acc.: 71.88%] [G loss: 1.382913]\n",
      "538 [D loss: 0.578274, acc.: 67.19%] [G loss: 1.763129]\n",
      "539 [D loss: 0.678476, acc.: 67.19%] [G loss: 1.870146]\n",
      "540 [D loss: 1.259799, acc.: 37.50%] [G loss: 1.363688]\n",
      "541 [D loss: 1.048684, acc.: 45.31%] [G loss: 1.468491]\n",
      "542 [D loss: 0.554017, acc.: 75.00%] [G loss: 1.455151]\n",
      "543 [D loss: 0.714405, acc.: 64.06%] [G loss: 2.271340]\n",
      "544 [D loss: 0.369250, acc.: 87.50%] [G loss: 1.622738]\n",
      "545 [D loss: 0.738518, acc.: 62.50%] [G loss: 1.853194]\n",
      "546 [D loss: 0.693390, acc.: 64.06%] [G loss: 1.849028]\n",
      "547 [D loss: 0.464501, acc.: 75.00%] [G loss: 2.185099]\n",
      "548 [D loss: 0.761716, acc.: 54.69%] [G loss: 2.466781]\n",
      "549 [D loss: 1.212096, acc.: 39.06%] [G loss: 1.436083]\n",
      "550 [D loss: 0.650781, acc.: 68.75%] [G loss: 0.881983]\n",
      "551 [D loss: 0.477792, acc.: 71.88%] [G loss: 0.516885]\n",
      "552 [D loss: 0.476679, acc.: 81.25%] [G loss: 1.525238]\n",
      "553 [D loss: 0.523225, acc.: 73.44%] [G loss: 2.156708]\n",
      "554 [D loss: 0.413151, acc.: 81.25%] [G loss: 1.795797]\n",
      "555 [D loss: 0.853458, acc.: 51.56%] [G loss: 1.961667]\n",
      "556 [D loss: 0.366765, acc.: 84.38%] [G loss: 1.196365]\n",
      "557 [D loss: 0.433091, acc.: 79.69%] [G loss: 0.845653]\n",
      "558 [D loss: 0.563258, acc.: 75.00%] [G loss: 1.603912]\n",
      "559 [D loss: 0.294264, acc.: 84.38%] [G loss: 1.788842]\n",
      "560 [D loss: 0.600563, acc.: 68.75%] [G loss: 2.342943]\n",
      "561 [D loss: 0.497780, acc.: 75.00%] [G loss: 3.102671]\n",
      "562 [D loss: 1.047090, acc.: 48.44%] [G loss: 2.905844]\n",
      "563 [D loss: 0.392994, acc.: 84.38%] [G loss: 2.843150]\n",
      "564 [D loss: 0.611208, acc.: 70.31%] [G loss: 1.954674]\n",
      "565 [D loss: 0.740168, acc.: 62.50%] [G loss: 2.206108]\n",
      "566 [D loss: 0.549854, acc.: 78.12%] [G loss: 1.445715]\n",
      "567 [D loss: 0.980780, acc.: 46.88%] [G loss: 1.784173]\n",
      "568 [D loss: 0.917647, acc.: 56.25%] [G loss: 2.191250]\n",
      "569 [D loss: 1.447344, acc.: 40.62%] [G loss: 1.694629]\n",
      "570 [D loss: 0.370060, acc.: 81.25%] [G loss: 1.737366]\n",
      "571 [D loss: 0.331894, acc.: 85.94%] [G loss: 1.384026]\n",
      "572 [D loss: 0.386840, acc.: 79.69%] [G loss: 0.929846]\n",
      "573 [D loss: 0.248284, acc.: 90.62%] [G loss: 1.163684]\n",
      "574 [D loss: 0.214315, acc.: 90.62%] [G loss: 0.613156]\n",
      "575 [D loss: 0.400671, acc.: 81.25%] [G loss: 0.659020]\n",
      "576 [D loss: 0.864407, acc.: 54.69%] [G loss: 1.594599]\n",
      "577 [D loss: 0.186259, acc.: 95.31%] [G loss: 1.525740]\n",
      "578 [D loss: 0.584045, acc.: 71.88%] [G loss: 0.948894]\n",
      "579 [D loss: 0.455216, acc.: 79.69%] [G loss: 2.431407]\n",
      "580 [D loss: 0.360013, acc.: 85.94%] [G loss: 2.467776]\n",
      "581 [D loss: 1.302070, acc.: 39.06%] [G loss: 2.396281]\n",
      "582 [D loss: 0.420379, acc.: 81.25%] [G loss: 2.845411]\n",
      "583 [D loss: 0.579069, acc.: 68.75%] [G loss: 2.198009]\n",
      "584 [D loss: 0.768466, acc.: 64.06%] [G loss: 2.316148]\n",
      "585 [D loss: 0.586409, acc.: 73.44%] [G loss: 2.052526]\n",
      "586 [D loss: 0.483967, acc.: 82.81%] [G loss: 0.960298]\n",
      "587 [D loss: 0.514089, acc.: 75.00%] [G loss: 0.686019]\n",
      "588 [D loss: 0.341628, acc.: 84.38%] [G loss: 1.377422]\n",
      "589 [D loss: 0.400219, acc.: 79.69%] [G loss: 0.586123]\n",
      "590 [D loss: 1.259156, acc.: 43.75%] [G loss: 2.034324]\n",
      "591 [D loss: 0.272133, acc.: 89.06%] [G loss: 3.701289]\n",
      "592 [D loss: 0.765249, acc.: 67.19%] [G loss: 1.157108]\n",
      "593 [D loss: 1.189643, acc.: 43.75%] [G loss: 1.553349]\n",
      "594 [D loss: 0.493590, acc.: 75.00%] [G loss: 2.192530]\n",
      "595 [D loss: 0.611959, acc.: 67.19%] [G loss: 1.309815]\n",
      "596 [D loss: 0.446980, acc.: 82.81%] [G loss: 1.552436]\n",
      "597 [D loss: 0.373318, acc.: 84.38%] [G loss: 0.794439]\n",
      "598 [D loss: 0.589981, acc.: 65.62%] [G loss: 0.398985]\n",
      "599 [D loss: 0.365747, acc.: 82.81%] [G loss: 0.415255]\n",
      "600 [D loss: 0.341023, acc.: 89.06%] [G loss: 0.827825]\n",
      "601 [D loss: 0.273426, acc.: 87.50%] [G loss: 0.650108]\n",
      "602 [D loss: 0.270479, acc.: 92.19%] [G loss: 0.252623]\n",
      "603 [D loss: 0.226425, acc.: 90.62%] [G loss: 0.198397]\n",
      "604 [D loss: 0.534932, acc.: 71.88%] [G loss: 0.925277]\n",
      "605 [D loss: 0.195529, acc.: 92.19%] [G loss: 0.891786]\n",
      "606 [D loss: 0.638898, acc.: 64.06%] [G loss: 0.686627]\n",
      "607 [D loss: 0.291016, acc.: 87.50%] [G loss: 1.182750]\n",
      "608 [D loss: 0.740854, acc.: 64.06%] [G loss: 1.613192]\n",
      "609 [D loss: 0.289800, acc.: 87.50%] [G loss: 1.201264]\n",
      "610 [D loss: 0.597758, acc.: 76.56%] [G loss: 0.433344]\n",
      "611 [D loss: 0.719107, acc.: 57.81%] [G loss: 0.872570]\n",
      "612 [D loss: 0.135452, acc.: 93.75%] [G loss: 0.597235]\n",
      "613 [D loss: 0.797030, acc.: 62.50%] [G loss: 2.200331]\n",
      "614 [D loss: 0.557910, acc.: 75.00%] [G loss: 2.196880]\n",
      "615 [D loss: 0.749006, acc.: 67.19%] [G loss: 2.111324]\n",
      "616 [D loss: 1.764997, acc.: 20.31%] [G loss: 1.790774]\n",
      "617 [D loss: 0.453010, acc.: 79.69%] [G loss: 3.194245]\n",
      "618 [D loss: 0.682880, acc.: 71.88%] [G loss: 2.396916]\n",
      "619 [D loss: 0.844150, acc.: 51.56%] [G loss: 1.213254]\n",
      "620 [D loss: 0.603212, acc.: 68.75%] [G loss: 1.584737]\n",
      "621 [D loss: 0.693858, acc.: 62.50%] [G loss: 2.107031]\n",
      "622 [D loss: 0.705090, acc.: 60.94%] [G loss: 1.812857]\n",
      "623 [D loss: 1.572192, acc.: 21.88%] [G loss: 1.149606]\n",
      "624 [D loss: 0.511868, acc.: 75.00%] [G loss: 1.875723]\n",
      "625 [D loss: 0.607374, acc.: 68.75%] [G loss: 2.046670]\n",
      "626 [D loss: 0.460710, acc.: 81.25%] [G loss: 1.621548]\n",
      "627 [D loss: 0.921696, acc.: 57.81%] [G loss: 1.969602]\n",
      "628 [D loss: 0.403791, acc.: 82.81%] [G loss: 1.643323]\n",
      "629 [D loss: 0.563344, acc.: 76.56%] [G loss: 1.718042]\n",
      "630 [D loss: 0.306572, acc.: 89.06%] [G loss: 1.998746]\n",
      "631 [D loss: 0.464089, acc.: 75.00%] [G loss: 1.344486]\n",
      "632 [D loss: 0.471018, acc.: 76.56%] [G loss: 1.890042]\n",
      "633 [D loss: 0.272902, acc.: 90.62%] [G loss: 2.176252]\n",
      "634 [D loss: 0.475683, acc.: 79.69%] [G loss: 1.023751]\n",
      "635 [D loss: 0.288000, acc.: 85.94%] [G loss: 0.913440]\n",
      "636 [D loss: 0.481042, acc.: 75.00%] [G loss: 0.986316]\n",
      "637 [D loss: 0.193005, acc.: 90.62%] [G loss: 0.835113]\n",
      "638 [D loss: 0.457395, acc.: 82.81%] [G loss: 1.141207]\n",
      "639 [D loss: 0.350284, acc.: 85.94%] [G loss: 0.893737]\n",
      "640 [D loss: 1.508516, acc.: 23.44%] [G loss: 0.465377]\n",
      "641 [D loss: 0.690914, acc.: 62.50%] [G loss: 1.199110]\n",
      "642 [D loss: 0.422477, acc.: 75.00%] [G loss: 0.801372]\n",
      "643 [D loss: 0.265747, acc.: 90.62%] [G loss: 0.410647]\n",
      "644 [D loss: 0.671549, acc.: 60.94%] [G loss: 1.335690]\n",
      "645 [D loss: 0.333268, acc.: 82.81%] [G loss: 1.387486]\n",
      "646 [D loss: 1.418245, acc.: 28.12%] [G loss: 1.617682]\n",
      "647 [D loss: 0.523627, acc.: 73.44%] [G loss: 3.242575]\n",
      "648 [D loss: 0.533482, acc.: 70.31%] [G loss: 1.637452]\n",
      "649 [D loss: 0.621216, acc.: 62.50%] [G loss: 1.586582]\n",
      "650 [D loss: 0.845653, acc.: 54.69%] [G loss: 1.721521]\n",
      "651 [D loss: 0.797490, acc.: 54.69%] [G loss: 1.959801]\n",
      "652 [D loss: 0.746489, acc.: 56.25%] [G loss: 2.017408]\n",
      "653 [D loss: 0.807631, acc.: 60.94%] [G loss: 1.857641]\n",
      "654 [D loss: 0.855070, acc.: 59.38%] [G loss: 2.020628]\n",
      "655 [D loss: 1.281009, acc.: 40.62%] [G loss: 1.352236]\n",
      "656 [D loss: 0.598199, acc.: 70.31%] [G loss: 1.495240]\n",
      "657 [D loss: 0.537657, acc.: 71.88%] [G loss: 2.081183]\n",
      "658 [D loss: 0.577309, acc.: 78.12%] [G loss: 1.782104]\n",
      "659 [D loss: 0.663789, acc.: 64.06%] [G loss: 1.251363]\n",
      "660 [D loss: 0.801768, acc.: 60.94%] [G loss: 1.822527]\n",
      "661 [D loss: 0.692589, acc.: 64.06%] [G loss: 1.315522]\n",
      "662 [D loss: 0.378911, acc.: 87.50%] [G loss: 0.579149]\n",
      "663 [D loss: 0.347890, acc.: 82.81%] [G loss: 0.609065]\n",
      "664 [D loss: 0.267227, acc.: 90.62%] [G loss: 1.338491]\n",
      "665 [D loss: 0.346396, acc.: 85.94%] [G loss: 2.038258]\n",
      "666 [D loss: 0.381919, acc.: 82.81%] [G loss: 2.247351]\n",
      "667 [D loss: 0.605956, acc.: 68.75%] [G loss: 1.589198]\n",
      "668 [D loss: 0.412557, acc.: 78.12%] [G loss: 1.017458]\n",
      "669 [D loss: 0.659009, acc.: 64.06%] [G loss: 1.163024]\n",
      "670 [D loss: 0.260118, acc.: 90.62%] [G loss: 0.963414]\n",
      "671 [D loss: 0.388715, acc.: 84.38%] [G loss: 1.208837]\n",
      "672 [D loss: 0.434643, acc.: 82.81%] [G loss: 1.337209]\n",
      "673 [D loss: 0.738954, acc.: 59.38%] [G loss: 0.882415]\n",
      "674 [D loss: 0.601721, acc.: 73.44%] [G loss: 1.624151]\n",
      "675 [D loss: 0.394573, acc.: 87.50%] [G loss: 1.249979]\n",
      "676 [D loss: 0.573106, acc.: 67.19%] [G loss: 0.802770]\n",
      "677 [D loss: 0.590516, acc.: 67.19%] [G loss: 1.201175]\n",
      "678 [D loss: 0.312133, acc.: 92.19%] [G loss: 1.571162]\n",
      "679 [D loss: 0.439054, acc.: 79.69%] [G loss: 1.493662]\n",
      "680 [D loss: 0.610024, acc.: 70.31%] [G loss: 2.157352]\n",
      "681 [D loss: 0.613844, acc.: 62.50%] [G loss: 1.835803]\n",
      "682 [D loss: 0.764126, acc.: 65.62%] [G loss: 0.700071]\n",
      "683 [D loss: 0.480539, acc.: 73.44%] [G loss: 1.355474]\n",
      "684 [D loss: 0.804138, acc.: 54.69%] [G loss: 1.646675]\n",
      "685 [D loss: 0.628290, acc.: 62.50%] [G loss: 1.285657]\n",
      "686 [D loss: 0.826350, acc.: 62.50%] [G loss: 1.459271]\n",
      "687 [D loss: 0.299551, acc.: 84.38%] [G loss: 1.065693]\n",
      "688 [D loss: 0.936941, acc.: 51.56%] [G loss: 1.947687]\n",
      "689 [D loss: 0.400233, acc.: 79.69%] [G loss: 2.307199]\n",
      "690 [D loss: 0.771169, acc.: 67.19%] [G loss: 0.408151]\n",
      "691 [D loss: 0.919518, acc.: 60.94%] [G loss: 1.767244]\n",
      "692 [D loss: 0.220405, acc.: 95.31%] [G loss: 2.694101]\n",
      "693 [D loss: 1.061767, acc.: 48.44%] [G loss: 1.236968]\n",
      "694 [D loss: 0.803415, acc.: 54.69%] [G loss: 2.148395]\n",
      "695 [D loss: 0.797060, acc.: 64.06%] [G loss: 1.588988]\n",
      "696 [D loss: 1.116021, acc.: 42.19%] [G loss: 1.100263]\n",
      "697 [D loss: 0.722822, acc.: 65.62%] [G loss: 2.009351]\n",
      "698 [D loss: 0.280524, acc.: 92.19%] [G loss: 2.999764]\n",
      "699 [D loss: 1.024269, acc.: 45.31%] [G loss: 0.996156]\n",
      "700 [D loss: 0.650834, acc.: 62.50%] [G loss: 1.839088]\n",
      "701 [D loss: 0.673916, acc.: 65.62%] [G loss: 2.139403]\n",
      "702 [D loss: 0.522689, acc.: 78.12%] [G loss: 1.132896]\n",
      "703 [D loss: 0.437195, acc.: 81.25%] [G loss: 1.313031]\n",
      "704 [D loss: 0.707033, acc.: 64.06%] [G loss: 1.250181]\n",
      "705 [D loss: 0.326795, acc.: 82.81%] [G loss: 1.745826]\n",
      "706 [D loss: 0.372731, acc.: 87.50%] [G loss: 1.828196]\n",
      "707 [D loss: 0.565766, acc.: 82.81%] [G loss: 2.171824]\n",
      "708 [D loss: 1.068821, acc.: 43.75%] [G loss: 1.236863]\n",
      "709 [D loss: 0.638839, acc.: 62.50%] [G loss: 1.570087]\n",
      "710 [D loss: 0.515110, acc.: 76.56%] [G loss: 2.217795]\n",
      "711 [D loss: 0.798492, acc.: 62.50%] [G loss: 2.015749]\n",
      "712 [D loss: 0.745936, acc.: 57.81%] [G loss: 1.501123]\n",
      "713 [D loss: 0.634310, acc.: 70.31%] [G loss: 1.973375]\n",
      "714 [D loss: 1.210690, acc.: 40.62%] [G loss: 1.715373]\n",
      "715 [D loss: 0.373722, acc.: 84.38%] [G loss: 1.992552]\n",
      "716 [D loss: 0.666984, acc.: 70.31%] [G loss: 1.663597]\n",
      "717 [D loss: 0.527225, acc.: 68.75%] [G loss: 1.667829]\n",
      "718 [D loss: 1.601172, acc.: 32.81%] [G loss: 1.872879]\n",
      "719 [D loss: 0.414593, acc.: 82.81%] [G loss: 2.538301]\n",
      "720 [D loss: 0.764014, acc.: 62.50%] [G loss: 1.055654]\n",
      "721 [D loss: 0.333712, acc.: 84.38%] [G loss: 1.087668]\n",
      "722 [D loss: 0.272892, acc.: 84.38%] [G loss: 1.548557]\n",
      "723 [D loss: 0.468329, acc.: 84.38%] [G loss: 0.842824]\n",
      "724 [D loss: 0.516132, acc.: 70.31%] [G loss: 0.966213]\n",
      "725 [D loss: 0.280025, acc.: 90.62%] [G loss: 2.244454]\n",
      "726 [D loss: 0.586721, acc.: 70.31%] [G loss: 2.124011]\n",
      "727 [D loss: 1.114223, acc.: 48.44%] [G loss: 1.567248]\n",
      "728 [D loss: 0.936847, acc.: 53.12%] [G loss: 1.730179]\n",
      "729 [D loss: 0.420553, acc.: 84.38%] [G loss: 1.230428]\n",
      "730 [D loss: 0.628219, acc.: 70.31%] [G loss: 1.527154]\n",
      "731 [D loss: 0.588139, acc.: 70.31%] [G loss: 1.495285]\n",
      "732 [D loss: 0.427296, acc.: 79.69%] [G loss: 1.472859]\n",
      "733 [D loss: 0.404693, acc.: 79.69%] [G loss: 1.493563]\n",
      "734 [D loss: 0.376326, acc.: 82.81%] [G loss: 1.537985]\n",
      "735 [D loss: 0.734560, acc.: 56.25%] [G loss: 0.790365]\n",
      "736 [D loss: 0.534611, acc.: 71.88%] [G loss: 1.969624]\n",
      "737 [D loss: 1.022549, acc.: 50.00%] [G loss: 2.195107]\n",
      "738 [D loss: 1.783095, acc.: 21.88%] [G loss: 1.176100]\n",
      "739 [D loss: 0.415186, acc.: 79.69%] [G loss: 1.915995]\n",
      "740 [D loss: 0.507978, acc.: 78.12%] [G loss: 1.031710]\n",
      "741 [D loss: 0.503202, acc.: 76.56%] [G loss: 0.843289]\n",
      "742 [D loss: 0.446711, acc.: 79.69%] [G loss: 1.062284]\n",
      "743 [D loss: 0.555986, acc.: 71.88%] [G loss: 1.644275]\n",
      "744 [D loss: 1.823200, acc.: 17.19%] [G loss: 1.529151]\n",
      "745 [D loss: 0.944184, acc.: 43.75%] [G loss: 1.906826]\n",
      "746 [D loss: 0.406304, acc.: 89.06%] [G loss: 2.202098]\n",
      "747 [D loss: 0.794362, acc.: 57.81%] [G loss: 1.377593]\n",
      "748 [D loss: 0.534233, acc.: 75.00%] [G loss: 1.232939]\n",
      "749 [D loss: 1.125035, acc.: 50.00%] [G loss: 0.770653]\n",
      "750 [D loss: 0.309644, acc.: 87.50%] [G loss: 0.889219]\n",
      "751 [D loss: 1.100939, acc.: 42.19%] [G loss: 1.233979]\n",
      "752 [D loss: 0.168892, acc.: 98.44%] [G loss: 1.206106]\n",
      "753 [D loss: 0.206793, acc.: 90.62%] [G loss: 0.476462]\n",
      "754 [D loss: 0.225229, acc.: 93.75%] [G loss: 0.237157]\n",
      "755 [D loss: 0.664359, acc.: 67.19%] [G loss: 0.441311]\n",
      "756 [D loss: 0.303343, acc.: 85.94%] [G loss: 0.522642]\n",
      "757 [D loss: 0.668043, acc.: 65.62%] [G loss: 0.775349]\n",
      "758 [D loss: 0.357046, acc.: 79.69%] [G loss: 0.847474]\n",
      "759 [D loss: 0.560022, acc.: 78.12%] [G loss: 0.514916]\n",
      "760 [D loss: 0.363474, acc.: 84.38%] [G loss: 0.435481]\n",
      "761 [D loss: 0.502499, acc.: 70.31%] [G loss: 1.243353]\n",
      "762 [D loss: 0.490954, acc.: 73.44%] [G loss: 0.890446]\n",
      "763 [D loss: 1.399771, acc.: 31.25%] [G loss: 0.220340]\n",
      "764 [D loss: 0.514061, acc.: 78.12%] [G loss: 1.170341]\n",
      "765 [D loss: 1.459330, acc.: 23.44%] [G loss: 1.447814]\n",
      "766 [D loss: 0.755489, acc.: 68.75%] [G loss: 1.982328]\n",
      "767 [D loss: 1.089560, acc.: 45.31%] [G loss: 1.978370]\n",
      "768 [D loss: 0.516892, acc.: 73.44%] [G loss: 2.717265]\n",
      "769 [D loss: 0.492530, acc.: 81.25%] [G loss: 2.097903]\n",
      "770 [D loss: 0.642857, acc.: 67.19%] [G loss: 2.089969]\n",
      "771 [D loss: 1.338787, acc.: 28.12%] [G loss: 1.885936]\n",
      "772 [D loss: 0.938951, acc.: 60.94%] [G loss: 0.944363]\n",
      "773 [D loss: 0.477496, acc.: 76.56%] [G loss: 0.900482]\n",
      "774 [D loss: 0.496474, acc.: 73.44%] [G loss: 1.012360]\n",
      "775 [D loss: 0.497331, acc.: 76.56%] [G loss: 1.011236]\n",
      "776 [D loss: 1.167917, acc.: 34.38%] [G loss: 0.886422]\n",
      "777 [D loss: 0.484832, acc.: 71.88%] [G loss: 2.195720]\n",
      "778 [D loss: 0.975959, acc.: 46.88%] [G loss: 1.834873]\n",
      "779 [D loss: 0.323928, acc.: 82.81%] [G loss: 1.290275]\n",
      "780 [D loss: 0.742906, acc.: 62.50%] [G loss: 1.888167]\n",
      "781 [D loss: 0.709023, acc.: 67.19%] [G loss: 1.347650]\n",
      "782 [D loss: 0.772209, acc.: 54.69%] [G loss: 1.142229]\n",
      "783 [D loss: 1.123382, acc.: 40.62%] [G loss: 1.846936]\n",
      "784 [D loss: 1.663726, acc.: 28.12%] [G loss: 1.779830]\n",
      "785 [D loss: 0.645702, acc.: 70.31%] [G loss: 1.330520]\n",
      "786 [D loss: 0.839059, acc.: 57.81%] [G loss: 1.414022]\n",
      "787 [D loss: 0.686586, acc.: 62.50%] [G loss: 1.611570]\n",
      "788 [D loss: 0.808925, acc.: 53.12%] [G loss: 1.477702]\n",
      "789 [D loss: 0.830676, acc.: 57.81%] [G loss: 1.862333]\n",
      "790 [D loss: 0.812790, acc.: 50.00%] [G loss: 1.804784]\n",
      "791 [D loss: 0.463201, acc.: 78.12%] [G loss: 1.367028]\n",
      "792 [D loss: 0.852313, acc.: 53.12%] [G loss: 1.752932]\n",
      "793 [D loss: 0.561070, acc.: 73.44%] [G loss: 1.731721]\n",
      "794 [D loss: 0.898520, acc.: 53.12%] [G loss: 1.565078]\n",
      "795 [D loss: 0.455199, acc.: 76.56%] [G loss: 1.472842]\n",
      "796 [D loss: 0.565442, acc.: 71.88%] [G loss: 0.766894]\n",
      "797 [D loss: 0.528344, acc.: 68.75%] [G loss: 1.013599]\n",
      "798 [D loss: 0.656634, acc.: 68.75%] [G loss: 1.852502]\n",
      "799 [D loss: 0.480606, acc.: 78.12%] [G loss: 1.807383]\n",
      "800 [D loss: 0.828527, acc.: 54.69%] [G loss: 1.889390]\n",
      "801 [D loss: 0.753597, acc.: 67.19%] [G loss: 1.596990]\n",
      "802 [D loss: 0.901583, acc.: 48.44%] [G loss: 1.164501]\n",
      "803 [D loss: 1.013153, acc.: 46.88%] [G loss: 1.189333]\n",
      "804 [D loss: 0.557861, acc.: 78.12%] [G loss: 0.974516]\n",
      "805 [D loss: 0.626846, acc.: 68.75%] [G loss: 1.234546]\n",
      "806 [D loss: 0.693068, acc.: 65.62%] [G loss: 1.679034]\n",
      "807 [D loss: 0.808678, acc.: 53.12%] [G loss: 2.377121]\n",
      "808 [D loss: 0.534010, acc.: 68.75%] [G loss: 1.602301]\n",
      "809 [D loss: 0.399053, acc.: 84.38%] [G loss: 1.028882]\n",
      "810 [D loss: 0.933471, acc.: 48.44%] [G loss: 1.090028]\n",
      "811 [D loss: 0.627840, acc.: 65.62%] [G loss: 2.562935]\n",
      "812 [D loss: 0.700084, acc.: 65.62%] [G loss: 1.196669]\n",
      "813 [D loss: 1.222876, acc.: 39.06%] [G loss: 0.779118]\n",
      "814 [D loss: 0.874497, acc.: 46.88%] [G loss: 2.094363]\n",
      "815 [D loss: 0.772231, acc.: 65.62%] [G loss: 1.697407]\n",
      "816 [D loss: 0.761338, acc.: 56.25%] [G loss: 1.287052]\n",
      "817 [D loss: 0.552712, acc.: 70.31%] [G loss: 1.569890]\n",
      "818 [D loss: 0.796005, acc.: 54.69%] [G loss: 1.566135]\n",
      "819 [D loss: 0.806621, acc.: 54.69%] [G loss: 1.578010]\n",
      "820 [D loss: 0.625273, acc.: 65.62%] [G loss: 1.532025]\n",
      "821 [D loss: 0.748954, acc.: 59.38%] [G loss: 1.572037]\n",
      "822 [D loss: 1.048429, acc.: 37.50%] [G loss: 1.391195]\n",
      "823 [D loss: 0.805874, acc.: 60.94%] [G loss: 0.906293]\n",
      "824 [D loss: 0.412882, acc.: 84.38%] [G loss: 0.749030]\n",
      "825 [D loss: 0.518577, acc.: 75.00%] [G loss: 0.471773]\n",
      "826 [D loss: 0.773080, acc.: 60.94%] [G loss: 1.730286]\n",
      "827 [D loss: 0.552217, acc.: 70.31%] [G loss: 1.689162]\n",
      "828 [D loss: 0.385948, acc.: 81.25%] [G loss: 1.080580]\n",
      "829 [D loss: 0.507595, acc.: 76.56%] [G loss: 0.779336]\n",
      "830 [D loss: 0.481670, acc.: 75.00%] [G loss: 1.507223]\n",
      "831 [D loss: 0.528197, acc.: 73.44%] [G loss: 1.485233]\n",
      "832 [D loss: 0.646210, acc.: 70.31%] [G loss: 1.916989]\n",
      "833 [D loss: 0.694975, acc.: 60.94%] [G loss: 1.414013]\n",
      "834 [D loss: 0.565055, acc.: 73.44%] [G loss: 1.354169]\n",
      "835 [D loss: 1.168860, acc.: 43.75%] [G loss: 1.444701]\n",
      "836 [D loss: 1.036000, acc.: 54.69%] [G loss: 1.459514]\n",
      "837 [D loss: 0.607941, acc.: 70.31%] [G loss: 1.644295]\n",
      "838 [D loss: 1.027970, acc.: 51.56%] [G loss: 1.584203]\n",
      "839 [D loss: 0.773532, acc.: 56.25%] [G loss: 1.852228]\n",
      "840 [D loss: 0.421626, acc.: 81.25%] [G loss: 1.954206]\n",
      "841 [D loss: 0.830497, acc.: 51.56%] [G loss: 1.157673]\n",
      "842 [D loss: 0.995196, acc.: 40.62%] [G loss: 1.710450]\n",
      "843 [D loss: 0.945725, acc.: 50.00%] [G loss: 2.137512]\n",
      "844 [D loss: 0.532433, acc.: 75.00%] [G loss: 1.672719]\n",
      "845 [D loss: 0.856013, acc.: 45.31%] [G loss: 1.187506]\n",
      "846 [D loss: 1.072312, acc.: 40.62%] [G loss: 1.408181]\n",
      "847 [D loss: 0.717397, acc.: 62.50%] [G loss: 2.238207]\n",
      "848 [D loss: 0.994223, acc.: 53.12%] [G loss: 1.217435]\n",
      "849 [D loss: 0.636648, acc.: 68.75%] [G loss: 0.993066]\n",
      "850 [D loss: 0.612268, acc.: 62.50%] [G loss: 1.389821]\n",
      "851 [D loss: 0.421068, acc.: 79.69%] [G loss: 1.114207]\n",
      "852 [D loss: 0.641578, acc.: 59.38%] [G loss: 1.100701]\n",
      "853 [D loss: 0.790381, acc.: 57.81%] [G loss: 0.843060]\n",
      "854 [D loss: 1.174240, acc.: 43.75%] [G loss: 0.967253]\n",
      "855 [D loss: 0.453850, acc.: 78.12%] [G loss: 1.918214]\n",
      "856 [D loss: 0.984069, acc.: 48.44%] [G loss: 0.957364]\n",
      "857 [D loss: 0.573969, acc.: 71.88%] [G loss: 0.939963]\n",
      "858 [D loss: 1.150719, acc.: 34.38%] [G loss: 1.549580]\n",
      "859 [D loss: 1.475587, acc.: 26.56%] [G loss: 1.071141]\n",
      "860 [D loss: 0.859148, acc.: 45.31%] [G loss: 1.180676]\n",
      "861 [D loss: 0.548795, acc.: 73.44%] [G loss: 1.434148]\n",
      "862 [D loss: 0.799826, acc.: 62.50%] [G loss: 1.180836]\n",
      "863 [D loss: 0.430165, acc.: 84.38%] [G loss: 0.447872]\n",
      "864 [D loss: 0.407134, acc.: 81.25%] [G loss: 0.693692]\n",
      "865 [D loss: 0.387469, acc.: 79.69%] [G loss: 0.503273]\n",
      "866 [D loss: 0.806315, acc.: 56.25%] [G loss: 1.090749]\n",
      "867 [D loss: 0.351308, acc.: 82.81%] [G loss: 2.122833]\n",
      "868 [D loss: 0.769398, acc.: 54.69%] [G loss: 0.960299]\n",
      "869 [D loss: 1.081178, acc.: 39.06%] [G loss: 1.370194]\n",
      "870 [D loss: 0.732683, acc.: 60.94%] [G loss: 1.877207]\n",
      "871 [D loss: 1.386527, acc.: 25.00%] [G loss: 1.029964]\n",
      "872 [D loss: 0.665156, acc.: 68.75%] [G loss: 1.838267]\n",
      "873 [D loss: 0.624067, acc.: 73.44%] [G loss: 1.794238]\n",
      "874 [D loss: 0.723884, acc.: 56.25%] [G loss: 1.030540]\n",
      "875 [D loss: 0.627822, acc.: 67.19%] [G loss: 0.902783]\n",
      "876 [D loss: 0.476921, acc.: 78.12%] [G loss: 1.282952]\n",
      "877 [D loss: 0.334416, acc.: 87.50%] [G loss: 0.658644]\n",
      "878 [D loss: 0.479082, acc.: 70.31%] [G loss: 0.521419]\n",
      "879 [D loss: 0.365028, acc.: 82.81%] [G loss: 1.060785]\n",
      "880 [D loss: 0.592150, acc.: 64.06%] [G loss: 1.634372]\n",
      "881 [D loss: 0.801162, acc.: 51.56%] [G loss: 1.561163]\n",
      "882 [D loss: 0.845204, acc.: 50.00%] [G loss: 1.451381]\n",
      "883 [D loss: 0.741228, acc.: 54.69%] [G loss: 1.722824]\n",
      "884 [D loss: 0.584063, acc.: 70.31%] [G loss: 1.171351]\n",
      "885 [D loss: 1.489345, acc.: 25.00%] [G loss: 1.073390]\n",
      "886 [D loss: 0.414638, acc.: 78.12%] [G loss: 1.411087]\n",
      "887 [D loss: 1.020667, acc.: 39.06%] [G loss: 1.383296]\n",
      "888 [D loss: 0.966252, acc.: 40.62%] [G loss: 1.886802]\n",
      "889 [D loss: 0.549216, acc.: 68.75%] [G loss: 1.340341]\n",
      "890 [D loss: 1.190389, acc.: 35.94%] [G loss: 1.666145]\n",
      "891 [D loss: 0.741778, acc.: 60.94%] [G loss: 2.250807]\n",
      "892 [D loss: 1.120961, acc.: 43.75%] [G loss: 1.261958]\n",
      "893 [D loss: 0.721740, acc.: 59.38%] [G loss: 1.080604]\n",
      "894 [D loss: 1.581381, acc.: 23.44%] [G loss: 1.323930]\n",
      "895 [D loss: 0.521900, acc.: 73.44%] [G loss: 1.688163]\n",
      "896 [D loss: 0.696324, acc.: 54.69%] [G loss: 1.798642]\n",
      "897 [D loss: 0.686387, acc.: 67.19%] [G loss: 0.937558]\n",
      "898 [D loss: 0.615674, acc.: 70.31%] [G loss: 1.032936]\n",
      "899 [D loss: 0.649349, acc.: 60.94%] [G loss: 1.550601]\n",
      "900 [D loss: 0.653116, acc.: 73.44%] [G loss: 1.398799]\n",
      "901 [D loss: 0.924418, acc.: 45.31%] [G loss: 1.453152]\n",
      "902 [D loss: 0.666772, acc.: 62.50%] [G loss: 1.457557]\n",
      "903 [D loss: 0.610417, acc.: 65.62%] [G loss: 1.218610]\n",
      "904 [D loss: 0.242526, acc.: 93.75%] [G loss: 0.859036]\n",
      "905 [D loss: 0.937584, acc.: 50.00%] [G loss: 1.256985]\n",
      "906 [D loss: 0.876554, acc.: 50.00%] [G loss: 1.408265]\n",
      "907 [D loss: 0.907701, acc.: 43.75%] [G loss: 1.240339]\n",
      "908 [D loss: 0.477782, acc.: 76.56%] [G loss: 0.998832]\n",
      "909 [D loss: 0.659235, acc.: 64.06%] [G loss: 1.143848]\n",
      "910 [D loss: 0.438130, acc.: 73.44%] [G loss: 1.419195]\n",
      "911 [D loss: 1.161385, acc.: 32.81%] [G loss: 1.143324]\n",
      "912 [D loss: 0.613177, acc.: 71.88%] [G loss: 1.494681]\n",
      "913 [D loss: 0.390545, acc.: 75.00%] [G loss: 1.948752]\n",
      "914 [D loss: 0.855978, acc.: 53.12%] [G loss: 1.216821]\n",
      "915 [D loss: 1.001320, acc.: 45.31%] [G loss: 0.996696]\n",
      "916 [D loss: 0.614715, acc.: 67.19%] [G loss: 1.832471]\n",
      "917 [D loss: 0.801228, acc.: 62.50%] [G loss: 1.572521]\n",
      "918 [D loss: 0.917992, acc.: 45.31%] [G loss: 1.554020]\n",
      "919 [D loss: 0.639465, acc.: 65.62%] [G loss: 1.647063]\n",
      "920 [D loss: 0.619498, acc.: 64.06%] [G loss: 1.379935]\n",
      "921 [D loss: 0.484623, acc.: 81.25%] [G loss: 1.593205]\n",
      "922 [D loss: 0.551151, acc.: 70.31%] [G loss: 1.268664]\n",
      "923 [D loss: 0.615735, acc.: 59.38%] [G loss: 1.267487]\n",
      "924 [D loss: 0.736160, acc.: 67.19%] [G loss: 1.545640]\n",
      "925 [D loss: 0.525444, acc.: 79.69%] [G loss: 1.531840]\n",
      "926 [D loss: 0.673389, acc.: 62.50%] [G loss: 1.129700]\n",
      "927 [D loss: 0.838705, acc.: 54.69%] [G loss: 1.180484]\n",
      "928 [D loss: 0.593989, acc.: 65.62%] [G loss: 1.622610]\n",
      "929 [D loss: 0.679492, acc.: 57.81%] [G loss: 1.594842]\n",
      "930 [D loss: 0.550644, acc.: 76.56%] [G loss: 1.285739]\n",
      "931 [D loss: 0.746505, acc.: 57.81%] [G loss: 1.778502]\n",
      "932 [D loss: 0.607659, acc.: 70.31%] [G loss: 1.512468]\n",
      "933 [D loss: 0.586756, acc.: 67.19%] [G loss: 1.637468]\n",
      "934 [D loss: 0.628241, acc.: 68.75%] [G loss: 1.476062]\n",
      "935 [D loss: 0.857331, acc.: 50.00%] [G loss: 1.752671]\n",
      "936 [D loss: 0.748510, acc.: 59.38%] [G loss: 1.864173]\n",
      "937 [D loss: 0.591358, acc.: 78.12%] [G loss: 0.875433]\n",
      "938 [D loss: 0.596275, acc.: 71.88%] [G loss: 0.383456]\n",
      "939 [D loss: 0.727231, acc.: 57.81%] [G loss: 0.869982]\n",
      "940 [D loss: 0.514796, acc.: 79.69%] [G loss: 1.221673]\n",
      "941 [D loss: 0.823996, acc.: 48.44%] [G loss: 1.221825]\n",
      "942 [D loss: 0.367445, acc.: 82.81%] [G loss: 1.447493]\n",
      "943 [D loss: 0.503973, acc.: 67.19%] [G loss: 0.553978]\n",
      "944 [D loss: 0.672034, acc.: 65.62%] [G loss: 1.132364]\n",
      "945 [D loss: 0.373019, acc.: 89.06%] [G loss: 1.292127]\n",
      "946 [D loss: 0.414223, acc.: 82.81%] [G loss: 1.113602]\n",
      "947 [D loss: 0.632234, acc.: 67.19%] [G loss: 1.317422]\n",
      "948 [D loss: 0.391940, acc.: 79.69%] [G loss: 0.838380]\n",
      "949 [D loss: 0.557862, acc.: 71.88%] [G loss: 0.567665]\n",
      "950 [D loss: 0.332352, acc.: 84.38%] [G loss: 1.049895]\n",
      "951 [D loss: 0.905512, acc.: 54.69%] [G loss: 1.887078]\n",
      "952 [D loss: 0.329738, acc.: 87.50%] [G loss: 1.555224]\n",
      "953 [D loss: 0.371417, acc.: 82.81%] [G loss: 1.275305]\n",
      "954 [D loss: 0.230934, acc.: 95.31%] [G loss: 1.151329]\n",
      "955 [D loss: 0.370966, acc.: 84.38%] [G loss: 0.888712]\n",
      "956 [D loss: 0.725649, acc.: 64.06%] [G loss: 1.944496]\n",
      "957 [D loss: 0.904267, acc.: 56.25%] [G loss: 1.773817]\n",
      "958 [D loss: 1.214271, acc.: 50.00%] [G loss: 0.853058]\n",
      "959 [D loss: 0.546448, acc.: 68.75%] [G loss: 1.163710]\n",
      "960 [D loss: 0.553340, acc.: 70.31%] [G loss: 2.055452]\n",
      "961 [D loss: 0.584568, acc.: 68.75%] [G loss: 1.851173]\n",
      "962 [D loss: 0.506325, acc.: 76.56%] [G loss: 0.882863]\n",
      "963 [D loss: 0.481926, acc.: 71.88%] [G loss: 0.855325]\n",
      "964 [D loss: 0.591819, acc.: 75.00%] [G loss: 1.644245]\n",
      "965 [D loss: 0.369442, acc.: 85.94%] [G loss: 1.102479]\n",
      "966 [D loss: 0.638485, acc.: 68.75%] [G loss: 2.171912]\n",
      "967 [D loss: 1.007568, acc.: 48.44%] [G loss: 1.495141]\n",
      "968 [D loss: 0.646668, acc.: 65.62%] [G loss: 1.396530]\n",
      "969 [D loss: 1.648623, acc.: 29.69%] [G loss: 1.410205]\n",
      "970 [D loss: 0.238869, acc.: 92.19%] [G loss: 2.525291]\n",
      "971 [D loss: 0.619599, acc.: 75.00%] [G loss: 1.419559]\n",
      "972 [D loss: 0.416399, acc.: 81.25%] [G loss: 0.953081]\n",
      "973 [D loss: 0.304784, acc.: 90.62%] [G loss: 1.036145]\n",
      "974 [D loss: 0.392235, acc.: 84.38%] [G loss: 0.858972]\n",
      "975 [D loss: 0.546196, acc.: 76.56%] [G loss: 0.723994]\n",
      "976 [D loss: 0.472414, acc.: 84.38%] [G loss: 0.655238]\n",
      "977 [D loss: 0.325984, acc.: 82.81%] [G loss: 1.163539]\n",
      "978 [D loss: 0.357236, acc.: 85.94%] [G loss: 1.185791]\n",
      "979 [D loss: 0.646055, acc.: 60.94%] [G loss: 1.996821]\n",
      "980 [D loss: 0.402164, acc.: 79.69%] [G loss: 2.491167]\n",
      "981 [D loss: 0.914218, acc.: 46.88%] [G loss: 1.221888]\n",
      "982 [D loss: 1.058542, acc.: 46.88%] [G loss: 1.543190]\n",
      "983 [D loss: 0.539451, acc.: 73.44%] [G loss: 1.825246]\n",
      "984 [D loss: 0.915935, acc.: 54.69%] [G loss: 1.865725]\n",
      "985 [D loss: 1.045453, acc.: 40.62%] [G loss: 1.189741]\n",
      "986 [D loss: 0.952257, acc.: 46.88%] [G loss: 1.634645]\n",
      "987 [D loss: 0.975667, acc.: 54.69%] [G loss: 1.211619]\n",
      "988 [D loss: 0.585005, acc.: 71.88%] [G loss: 1.313434]\n",
      "989 [D loss: 0.493553, acc.: 79.69%] [G loss: 0.925959]\n",
      "990 [D loss: 0.880129, acc.: 53.12%] [G loss: 1.021350]\n",
      "991 [D loss: 0.919624, acc.: 62.50%] [G loss: 1.578124]\n",
      "992 [D loss: 0.736332, acc.: 60.94%] [G loss: 1.499968]\n",
      "993 [D loss: 1.148273, acc.: 35.94%] [G loss: 0.973474]\n",
      "994 [D loss: 0.789896, acc.: 56.25%] [G loss: 1.898164]\n",
      "995 [D loss: 0.556638, acc.: 65.62%] [G loss: 0.866834]\n",
      "996 [D loss: 0.841895, acc.: 57.81%] [G loss: 0.259410]\n",
      "997 [D loss: 0.428004, acc.: 84.38%] [G loss: 0.546637]\n",
      "998 [D loss: 0.822490, acc.: 54.69%] [G loss: 1.083081]\n",
      "999 [D loss: 1.340716, acc.: 35.94%] [G loss: 0.997090]\n",
      "1000 [D loss: 0.627591, acc.: 56.25%] [G loss: 1.905060]\n",
      "1001 [D loss: 0.422225, acc.: 79.69%] [G loss: 1.226454]\n",
      "1002 [D loss: 1.110806, acc.: 37.50%] [G loss: 1.099998]\n",
      "1003 [D loss: 0.692173, acc.: 65.62%] [G loss: 1.577961]\n",
      "1004 [D loss: 0.606018, acc.: 71.88%] [G loss: 1.235901]\n",
      "1005 [D loss: 0.538746, acc.: 75.00%] [G loss: 1.299211]\n",
      "1006 [D loss: 0.938716, acc.: 48.44%] [G loss: 1.673347]\n",
      "1007 [D loss: 0.236083, acc.: 89.06%] [G loss: 1.605691]\n",
      "1008 [D loss: 1.005618, acc.: 43.75%] [G loss: 0.959901]\n",
      "1009 [D loss: 0.648521, acc.: 60.94%] [G loss: 1.681493]\n",
      "1010 [D loss: 0.954280, acc.: 43.75%] [G loss: 1.454684]\n",
      "1011 [D loss: 0.893559, acc.: 50.00%] [G loss: 1.394639]\n",
      "1012 [D loss: 0.788849, acc.: 53.12%] [G loss: 1.351939]\n",
      "1013 [D loss: 0.597947, acc.: 76.56%] [G loss: 1.510825]\n",
      "1014 [D loss: 1.171853, acc.: 35.94%] [G loss: 1.461940]\n",
      "1015 [D loss: 0.543748, acc.: 68.75%] [G loss: 1.313548]\n",
      "1016 [D loss: 0.523982, acc.: 73.44%] [G loss: 1.097056]\n",
      "1017 [D loss: 0.728346, acc.: 54.69%] [G loss: 1.069291]\n",
      "1018 [D loss: 0.551430, acc.: 71.88%] [G loss: 1.298012]\n",
      "1019 [D loss: 0.340251, acc.: 90.62%] [G loss: 1.010605]\n",
      "1020 [D loss: 0.803921, acc.: 50.00%] [G loss: 0.998082]\n",
      "1021 [D loss: 0.657215, acc.: 62.50%] [G loss: 1.608018]\n",
      "1022 [D loss: 0.440392, acc.: 81.25%] [G loss: 1.133322]\n",
      "1023 [D loss: 0.704794, acc.: 65.62%] [G loss: 0.790033]\n",
      "1024 [D loss: 0.716524, acc.: 65.62%] [G loss: 1.354293]\n",
      "1025 [D loss: 0.534744, acc.: 71.88%] [G loss: 1.979258]\n",
      "1026 [D loss: 1.405247, acc.: 35.94%] [G loss: 0.753739]\n",
      "1027 [D loss: 0.584446, acc.: 76.56%] [G loss: 0.709849]\n",
      "1028 [D loss: 0.510977, acc.: 73.44%] [G loss: 1.049914]\n",
      "1029 [D loss: 0.349857, acc.: 82.81%] [G loss: 0.464824]\n",
      "1030 [D loss: 0.399668, acc.: 84.38%] [G loss: 0.761364]\n",
      "1031 [D loss: 0.647817, acc.: 67.19%] [G loss: 1.844198]\n",
      "1032 [D loss: 0.597153, acc.: 70.31%] [G loss: 1.096304]\n",
      "1033 [D loss: 0.409876, acc.: 84.38%] [G loss: 1.176245]\n",
      "1034 [D loss: 0.392352, acc.: 79.69%] [G loss: 1.346283]\n",
      "1035 [D loss: 1.480667, acc.: 21.88%] [G loss: 1.395375]\n",
      "1036 [D loss: 0.812765, acc.: 57.81%] [G loss: 2.135071]\n",
      "1037 [D loss: 0.348212, acc.: 82.81%] [G loss: 2.210366]\n",
      "1038 [D loss: 0.620460, acc.: 71.88%] [G loss: 1.411695]\n",
      "1039 [D loss: 0.561098, acc.: 71.88%] [G loss: 1.251985]\n",
      "1040 [D loss: 0.536358, acc.: 67.19%] [G loss: 0.902117]\n",
      "1041 [D loss: 0.426667, acc.: 82.81%] [G loss: 0.363561]\n",
      "1042 [D loss: 0.474003, acc.: 81.25%] [G loss: 0.761223]\n",
      "1043 [D loss: 0.614536, acc.: 68.75%] [G loss: 0.883137]\n",
      "1044 [D loss: 0.815045, acc.: 64.06%] [G loss: 1.369940]\n",
      "1045 [D loss: 0.808403, acc.: 60.94%] [G loss: 1.187322]\n",
      "1046 [D loss: 0.254146, acc.: 96.88%] [G loss: 0.574126]\n",
      "1047 [D loss: 0.131572, acc.: 98.44%] [G loss: 0.451658]\n",
      "1048 [D loss: 1.065312, acc.: 45.31%] [G loss: 0.755420]\n",
      "1049 [D loss: 0.491131, acc.: 82.81%] [G loss: 0.582050]\n",
      "1050 [D loss: 0.385448, acc.: 85.94%] [G loss: 0.623407]\n",
      "1051 [D loss: 0.356472, acc.: 89.06%] [G loss: 0.515754]\n",
      "1052 [D loss: 0.586197, acc.: 73.44%] [G loss: 0.316726]\n",
      "1053 [D loss: 0.129701, acc.: 95.31%] [G loss: 0.462887]\n",
      "1054 [D loss: 1.265022, acc.: 46.88%] [G loss: 1.622089]\n",
      "1055 [D loss: 0.869180, acc.: 57.81%] [G loss: 2.101071]\n",
      "1056 [D loss: 0.810539, acc.: 59.38%] [G loss: 1.251855]\n",
      "1057 [D loss: 0.530024, acc.: 76.56%] [G loss: 1.742279]\n",
      "1058 [D loss: 0.369511, acc.: 82.81%] [G loss: 1.118361]\n",
      "1059 [D loss: 0.753035, acc.: 60.94%] [G loss: 1.330793]\n",
      "1060 [D loss: 0.797708, acc.: 56.25%] [G loss: 0.957344]\n",
      "1061 [D loss: 0.387253, acc.: 84.38%] [G loss: 1.336514]\n",
      "1062 [D loss: 1.123001, acc.: 34.38%] [G loss: 1.236724]\n",
      "1063 [D loss: 0.484933, acc.: 76.56%] [G loss: 2.087904]\n",
      "1064 [D loss: 0.572776, acc.: 71.88%] [G loss: 1.455493]\n",
      "1065 [D loss: 0.631699, acc.: 65.62%] [G loss: 0.534234]\n",
      "1066 [D loss: 1.263326, acc.: 51.56%] [G loss: 1.526481]\n",
      "1067 [D loss: 1.537284, acc.: 32.81%] [G loss: 1.344535]\n",
      "1068 [D loss: 0.728642, acc.: 60.94%] [G loss: 1.428694]\n",
      "1069 [D loss: 0.704367, acc.: 57.81%] [G loss: 1.546494]\n",
      "1070 [D loss: 0.478504, acc.: 79.69%] [G loss: 1.199434]\n",
      "1071 [D loss: 0.843484, acc.: 51.56%] [G loss: 1.562991]\n",
      "1072 [D loss: 0.860283, acc.: 51.56%] [G loss: 2.058049]\n",
      "1073 [D loss: 0.654376, acc.: 65.62%] [G loss: 2.271522]\n",
      "1074 [D loss: 0.556994, acc.: 73.44%] [G loss: 0.634641]\n",
      "1075 [D loss: 0.644158, acc.: 57.81%] [G loss: 0.967838]\n",
      "1076 [D loss: 0.573419, acc.: 67.19%] [G loss: 1.197948]\n",
      "1077 [D loss: 0.548411, acc.: 73.44%] [G loss: 1.586123]\n",
      "1078 [D loss: 1.287929, acc.: 34.38%] [G loss: 1.507165]\n",
      "1079 [D loss: 0.537283, acc.: 70.31%] [G loss: 1.661650]\n",
      "1080 [D loss: 0.447913, acc.: 78.12%] [G loss: 1.734864]\n",
      "1081 [D loss: 1.396976, acc.: 21.88%] [G loss: 0.816385]\n",
      "1082 [D loss: 0.870756, acc.: 50.00%] [G loss: 1.180318]\n",
      "1083 [D loss: 0.565762, acc.: 67.19%] [G loss: 1.900606]\n",
      "1084 [D loss: 0.792434, acc.: 54.69%] [G loss: 1.332460]\n",
      "1085 [D loss: 0.588948, acc.: 71.88%] [G loss: 1.263151]\n",
      "1086 [D loss: 0.622445, acc.: 65.62%] [G loss: 1.252564]\n",
      "1087 [D loss: 1.056582, acc.: 40.62%] [G loss: 1.224678]\n",
      "1088 [D loss: 1.168374, acc.: 39.06%] [G loss: 1.343298]\n",
      "1089 [D loss: 0.690260, acc.: 65.62%] [G loss: 1.538021]\n",
      "1090 [D loss: 0.886040, acc.: 57.81%] [G loss: 2.694870]\n",
      "1091 [D loss: 1.033085, acc.: 42.19%] [G loss: 1.671020]\n",
      "1092 [D loss: 0.757965, acc.: 60.94%] [G loss: 1.051367]\n",
      "1093 [D loss: 1.033205, acc.: 53.12%] [G loss: 1.353884]\n",
      "1094 [D loss: 0.543138, acc.: 73.44%] [G loss: 1.862046]\n",
      "1095 [D loss: 0.786993, acc.: 56.25%] [G loss: 1.637443]\n",
      "1096 [D loss: 0.467293, acc.: 73.44%] [G loss: 1.060135]\n",
      "1097 [D loss: 1.071586, acc.: 50.00%] [G loss: 1.027212]\n",
      "1098 [D loss: 0.870748, acc.: 56.25%] [G loss: 1.801464]\n",
      "1099 [D loss: 0.629178, acc.: 62.50%] [G loss: 1.050531]\n",
      "1100 [D loss: 0.917794, acc.: 48.44%] [G loss: 1.336415]\n",
      "1101 [D loss: 0.512953, acc.: 71.88%] [G loss: 1.644464]\n",
      "1102 [D loss: 0.575402, acc.: 73.44%] [G loss: 0.727007]\n",
      "1103 [D loss: 0.570207, acc.: 75.00%] [G loss: 0.931011]\n",
      "1104 [D loss: 0.588021, acc.: 64.06%] [G loss: 0.854910]\n",
      "1105 [D loss: 0.397893, acc.: 81.25%] [G loss: 1.227902]\n",
      "1106 [D loss: 0.805371, acc.: 51.56%] [G loss: 1.058050]\n",
      "1107 [D loss: 0.581554, acc.: 71.88%] [G loss: 1.138819]\n",
      "1108 [D loss: 0.896215, acc.: 46.88%] [G loss: 1.022641]\n",
      "1109 [D loss: 0.572686, acc.: 70.31%] [G loss: 1.047253]\n",
      "1110 [D loss: 0.590007, acc.: 71.88%] [G loss: 1.799394]\n",
      "1111 [D loss: 0.587047, acc.: 70.31%] [G loss: 1.477453]\n",
      "1112 [D loss: 0.466457, acc.: 76.56%] [G loss: 1.212890]\n",
      "1113 [D loss: 0.728156, acc.: 64.06%] [G loss: 0.615738]\n",
      "1114 [D loss: 0.721770, acc.: 65.62%] [G loss: 0.998046]\n",
      "1115 [D loss: 1.038741, acc.: 35.94%] [G loss: 1.287049]\n",
      "1116 [D loss: 1.499744, acc.: 14.06%] [G loss: 1.823290]\n",
      "1117 [D loss: 1.107863, acc.: 42.19%] [G loss: 1.786499]\n",
      "1118 [D loss: 1.336812, acc.: 31.25%] [G loss: 1.207937]\n",
      "1119 [D loss: 0.752015, acc.: 59.38%] [G loss: 1.317319]\n",
      "1120 [D loss: 0.592027, acc.: 62.50%] [G loss: 1.460938]\n",
      "1121 [D loss: 0.416434, acc.: 82.81%] [G loss: 1.325413]\n",
      "1122 [D loss: 0.850757, acc.: 53.12%] [G loss: 1.196078]\n",
      "1123 [D loss: 0.446734, acc.: 78.12%] [G loss: 0.981607]\n",
      "1124 [D loss: 0.812674, acc.: 53.12%] [G loss: 1.389784]\n",
      "1125 [D loss: 1.193397, acc.: 35.94%] [G loss: 1.201875]\n",
      "1126 [D loss: 0.435199, acc.: 81.25%] [G loss: 0.860286]\n",
      "1127 [D loss: 0.991504, acc.: 43.75%] [G loss: 0.987605]\n",
      "1128 [D loss: 0.520016, acc.: 75.00%] [G loss: 1.486727]\n",
      "1129 [D loss: 0.669929, acc.: 70.31%] [G loss: 1.066954]\n",
      "1130 [D loss: 0.703367, acc.: 59.38%] [G loss: 0.978985]\n",
      "1131 [D loss: 0.716920, acc.: 56.25%] [G loss: 1.610960]\n",
      "1132 [D loss: 1.302986, acc.: 31.25%] [G loss: 1.461512]\n",
      "1133 [D loss: 0.421791, acc.: 79.69%] [G loss: 0.905120]\n",
      "1134 [D loss: 0.477865, acc.: 82.81%] [G loss: 1.336064]\n",
      "1135 [D loss: 0.819681, acc.: 64.06%] [G loss: 0.947318]\n",
      "1136 [D loss: 0.512037, acc.: 76.56%] [G loss: 1.004955]\n",
      "1137 [D loss: 0.588305, acc.: 64.06%] [G loss: 1.129914]\n",
      "1138 [D loss: 0.854402, acc.: 46.88%] [G loss: 1.358203]\n",
      "1139 [D loss: 0.721328, acc.: 62.50%] [G loss: 1.060045]\n",
      "1140 [D loss: 0.883714, acc.: 56.25%] [G loss: 1.006069]\n",
      "1141 [D loss: 0.657611, acc.: 64.06%] [G loss: 1.458066]\n",
      "1142 [D loss: 0.682986, acc.: 67.19%] [G loss: 2.114707]\n",
      "1143 [D loss: 0.643183, acc.: 64.06%] [G loss: 1.561320]\n",
      "1144 [D loss: 0.896381, acc.: 43.75%] [G loss: 1.359503]\n",
      "1145 [D loss: 0.518739, acc.: 71.88%] [G loss: 1.100205]\n",
      "1146 [D loss: 0.363477, acc.: 79.69%] [G loss: 0.870832]\n",
      "1147 [D loss: 0.861676, acc.: 56.25%] [G loss: 1.621644]\n",
      "1148 [D loss: 0.543019, acc.: 67.19%] [G loss: 1.871580]\n",
      "1149 [D loss: 0.797418, acc.: 54.69%] [G loss: 1.892294]\n",
      "1150 [D loss: 0.522678, acc.: 76.56%] [G loss: 0.713380]\n",
      "1151 [D loss: 0.765573, acc.: 59.38%] [G loss: 0.697151]\n",
      "1152 [D loss: 0.416796, acc.: 84.38%] [G loss: 0.592211]\n",
      "1153 [D loss: 0.704097, acc.: 62.50%] [G loss: 1.509608]\n",
      "1154 [D loss: 0.524396, acc.: 78.12%] [G loss: 1.547812]\n",
      "1155 [D loss: 0.842959, acc.: 54.69%] [G loss: 1.473094]\n",
      "1156 [D loss: 0.576244, acc.: 67.19%] [G loss: 1.866939]\n",
      "1157 [D loss: 0.715135, acc.: 67.19%] [G loss: 1.850802]\n",
      "1158 [D loss: 1.248637, acc.: 28.12%] [G loss: 1.351937]\n",
      "1159 [D loss: 0.757770, acc.: 53.12%] [G loss: 1.338921]\n",
      "1160 [D loss: 0.785140, acc.: 54.69%] [G loss: 1.486421]\n",
      "1161 [D loss: 0.724361, acc.: 56.25%] [G loss: 1.947477]\n",
      "1162 [D loss: 0.755906, acc.: 56.25%] [G loss: 1.679863]\n",
      "1163 [D loss: 0.958374, acc.: 43.75%] [G loss: 1.152198]\n",
      "1164 [D loss: 0.624192, acc.: 71.88%] [G loss: 1.170191]\n",
      "1165 [D loss: 0.614196, acc.: 67.19%] [G loss: 1.304725]\n",
      "1166 [D loss: 0.438001, acc.: 81.25%] [G loss: 1.444484]\n",
      "1167 [D loss: 0.571481, acc.: 75.00%] [G loss: 1.032658]\n",
      "1168 [D loss: 0.516442, acc.: 76.56%] [G loss: 1.453014]\n",
      "1169 [D loss: 0.566622, acc.: 64.06%] [G loss: 1.013022]\n",
      "1170 [D loss: 0.404811, acc.: 85.94%] [G loss: 1.014669]\n",
      "1171 [D loss: 0.597106, acc.: 67.19%] [G loss: 1.333549]\n",
      "1172 [D loss: 1.049655, acc.: 46.88%] [G loss: 1.358704]\n",
      "1173 [D loss: 0.998713, acc.: 45.31%] [G loss: 1.311159]\n",
      "1174 [D loss: 0.449061, acc.: 81.25%] [G loss: 1.474324]\n",
      "1175 [D loss: 0.552222, acc.: 76.56%] [G loss: 0.641869]\n",
      "1176 [D loss: 0.722893, acc.: 67.19%] [G loss: 0.837917]\n",
      "1177 [D loss: 0.740095, acc.: 75.00%] [G loss: 1.418476]\n",
      "1178 [D loss: 0.795670, acc.: 60.94%] [G loss: 1.233417]\n",
      "1179 [D loss: 1.161847, acc.: 37.50%] [G loss: 1.683042]\n",
      "1180 [D loss: 0.627846, acc.: 68.75%] [G loss: 1.539096]\n",
      "1181 [D loss: 1.143668, acc.: 40.62%] [G loss: 1.103462]\n",
      "1182 [D loss: 1.003136, acc.: 40.62%] [G loss: 1.461961]\n",
      "1183 [D loss: 0.644904, acc.: 64.06%] [G loss: 2.077132]\n",
      "1184 [D loss: 0.585692, acc.: 70.31%] [G loss: 1.908394]\n",
      "1185 [D loss: 0.779841, acc.: 62.50%] [G loss: 1.450870]\n",
      "1186 [D loss: 0.864575, acc.: 54.69%] [G loss: 1.324228]\n",
      "1187 [D loss: 0.477248, acc.: 78.12%] [G loss: 1.378179]\n",
      "1188 [D loss: 0.573678, acc.: 64.06%] [G loss: 0.726838]\n",
      "1189 [D loss: 0.776469, acc.: 59.38%] [G loss: 0.752720]\n",
      "1190 [D loss: 0.872253, acc.: 59.38%] [G loss: 1.602694]\n",
      "1191 [D loss: 1.097141, acc.: 29.69%] [G loss: 1.396768]\n",
      "1192 [D loss: 1.396186, acc.: 37.50%] [G loss: 0.891391]\n",
      "1193 [D loss: 0.540361, acc.: 79.69%] [G loss: 1.478464]\n",
      "1194 [D loss: 0.586023, acc.: 71.88%] [G loss: 1.592469]\n",
      "1195 [D loss: 0.662571, acc.: 60.94%] [G loss: 1.863567]\n",
      "1196 [D loss: 0.975990, acc.: 54.69%] [G loss: 1.236441]\n",
      "1197 [D loss: 0.576065, acc.: 67.19%] [G loss: 0.780406]\n",
      "1198 [D loss: 0.714333, acc.: 65.62%] [G loss: 0.989171]\n",
      "1199 [D loss: 0.458096, acc.: 84.38%] [G loss: 0.940116]\n",
      "1200 [D loss: 0.871947, acc.: 45.31%] [G loss: 0.963570]\n",
      "1201 [D loss: 0.204544, acc.: 93.75%] [G loss: 1.070085]\n",
      "1202 [D loss: 0.555388, acc.: 68.75%] [G loss: 0.922156]\n",
      "1203 [D loss: 0.552605, acc.: 67.19%] [G loss: 1.252159]\n",
      "1204 [D loss: 0.944255, acc.: 46.88%] [G loss: 1.226330]\n",
      "1205 [D loss: 0.737688, acc.: 59.38%] [G loss: 1.522836]\n",
      "1206 [D loss: 0.949131, acc.: 48.44%] [G loss: 1.588285]\n",
      "1207 [D loss: 0.782721, acc.: 48.44%] [G loss: 1.750907]\n",
      "1208 [D loss: 0.952818, acc.: 48.44%] [G loss: 1.275091]\n",
      "1209 [D loss: 0.626561, acc.: 65.62%] [G loss: 1.494069]\n",
      "1210 [D loss: 1.078641, acc.: 40.62%] [G loss: 1.517744]\n",
      "1211 [D loss: 1.004898, acc.: 51.56%] [G loss: 1.645725]\n",
      "1212 [D loss: 0.650564, acc.: 67.19%] [G loss: 1.474235]\n",
      "1213 [D loss: 0.551284, acc.: 73.44%] [G loss: 1.107524]\n",
      "1214 [D loss: 0.682659, acc.: 62.50%] [G loss: 0.898732]\n",
      "1215 [D loss: 0.851031, acc.: 46.88%] [G loss: 1.197346]\n",
      "1216 [D loss: 0.596698, acc.: 73.44%] [G loss: 1.037004]\n",
      "1217 [D loss: 0.612070, acc.: 67.19%] [G loss: 0.995103]\n",
      "1218 [D loss: 0.663623, acc.: 73.44%] [G loss: 0.799521]\n",
      "1219 [D loss: 0.801307, acc.: 51.56%] [G loss: 1.375268]\n",
      "1220 [D loss: 1.102374, acc.: 46.88%] [G loss: 1.804686]\n",
      "1221 [D loss: 0.471766, acc.: 78.12%] [G loss: 1.118774]\n",
      "1222 [D loss: 0.693173, acc.: 67.19%] [G loss: 1.026159]\n",
      "1223 [D loss: 0.627912, acc.: 62.50%] [G loss: 1.495786]\n",
      "1224 [D loss: 0.682256, acc.: 65.62%] [G loss: 1.716461]\n",
      "1225 [D loss: 0.733979, acc.: 62.50%] [G loss: 0.642841]\n",
      "1226 [D loss: 0.619880, acc.: 71.88%] [G loss: 0.992027]\n",
      "1227 [D loss: 1.066698, acc.: 45.31%] [G loss: 1.126261]\n",
      "1228 [D loss: 0.679384, acc.: 57.81%] [G loss: 1.507777]\n",
      "1229 [D loss: 0.743951, acc.: 60.94%] [G loss: 1.816792]\n",
      "1230 [D loss: 0.571549, acc.: 76.56%] [G loss: 1.631006]\n",
      "1231 [D loss: 0.676552, acc.: 60.94%] [G loss: 1.057657]\n",
      "1232 [D loss: 0.406727, acc.: 82.81%] [G loss: 0.886461]\n",
      "1233 [D loss: 0.627357, acc.: 68.75%] [G loss: 0.981110]\n",
      "1234 [D loss: 0.629908, acc.: 64.06%] [G loss: 0.832245]\n",
      "1235 [D loss: 0.731943, acc.: 62.50%] [G loss: 0.784351]\n",
      "1236 [D loss: 0.622996, acc.: 62.50%] [G loss: 1.316527]\n",
      "1237 [D loss: 0.903915, acc.: 48.44%] [G loss: 1.667114]\n",
      "1238 [D loss: 0.769406, acc.: 62.50%] [G loss: 2.040480]\n",
      "1239 [D loss: 0.629469, acc.: 70.31%] [G loss: 1.018710]\n",
      "1240 [D loss: 0.729680, acc.: 57.81%] [G loss: 0.967370]\n",
      "1241 [D loss: 0.798378, acc.: 54.69%] [G loss: 1.555089]\n",
      "1242 [D loss: 0.761629, acc.: 46.88%] [G loss: 1.694354]\n",
      "1243 [D loss: 0.628371, acc.: 73.44%] [G loss: 1.147993]\n",
      "1244 [D loss: 0.801338, acc.: 65.62%] [G loss: 1.279204]\n",
      "1245 [D loss: 0.633314, acc.: 62.50%] [G loss: 1.676973]\n",
      "1246 [D loss: 1.126700, acc.: 40.62%] [G loss: 0.644737]\n",
      "1247 [D loss: 0.677127, acc.: 56.25%] [G loss: 0.863601]\n",
      "1248 [D loss: 0.753106, acc.: 56.25%] [G loss: 1.403970]\n",
      "1249 [D loss: 0.756196, acc.: 54.69%] [G loss: 1.640805]\n",
      "1250 [D loss: 0.838880, acc.: 54.69%] [G loss: 1.544140]\n",
      "1251 [D loss: 0.452457, acc.: 79.69%] [G loss: 0.882580]\n",
      "1252 [D loss: 0.922966, acc.: 54.69%] [G loss: 1.113295]\n",
      "1253 [D loss: 0.941315, acc.: 54.69%] [G loss: 1.372393]\n",
      "1254 [D loss: 0.737159, acc.: 62.50%] [G loss: 1.118631]\n",
      "1255 [D loss: 0.722436, acc.: 59.38%] [G loss: 1.551290]\n",
      "1256 [D loss: 0.800253, acc.: 57.81%] [G loss: 0.845309]\n",
      "1257 [D loss: 1.165645, acc.: 37.50%] [G loss: 1.555799]\n",
      "1258 [D loss: 0.830215, acc.: 60.94%] [G loss: 1.300194]\n",
      "1259 [D loss: 1.075292, acc.: 39.06%] [G loss: 0.992780]\n",
      "1260 [D loss: 0.648279, acc.: 64.06%] [G loss: 1.777686]\n",
      "1261 [D loss: 0.774196, acc.: 56.25%] [G loss: 0.920774]\n",
      "1262 [D loss: 1.004604, acc.: 39.06%] [G loss: 0.739966]\n",
      "1263 [D loss: 0.833468, acc.: 51.56%] [G loss: 1.161314]\n",
      "1264 [D loss: 0.416744, acc.: 82.81%] [G loss: 1.306294]\n",
      "1265 [D loss: 0.656712, acc.: 62.50%] [G loss: 1.524541]\n",
      "1266 [D loss: 1.010691, acc.: 48.44%] [G loss: 0.956040]\n",
      "1267 [D loss: 0.675830, acc.: 64.06%] [G loss: 1.015139]\n",
      "1268 [D loss: 1.043738, acc.: 42.19%] [G loss: 1.861868]\n",
      "1269 [D loss: 1.103697, acc.: 46.88%] [G loss: 1.501944]\n",
      "1270 [D loss: 1.010382, acc.: 43.75%] [G loss: 1.157675]\n",
      "1271 [D loss: 0.651336, acc.: 65.62%] [G loss: 1.432402]\n",
      "1272 [D loss: 1.038995, acc.: 40.62%] [G loss: 1.585942]\n",
      "1273 [D loss: 0.719164, acc.: 62.50%] [G loss: 1.318940]\n",
      "1274 [D loss: 0.560659, acc.: 68.75%] [G loss: 1.135728]\n",
      "1275 [D loss: 0.746773, acc.: 53.12%] [G loss: 1.807341]\n",
      "1276 [D loss: 0.662655, acc.: 65.62%] [G loss: 1.164345]\n",
      "1277 [D loss: 0.592688, acc.: 73.44%] [G loss: 0.984438]\n",
      "1278 [D loss: 0.734191, acc.: 56.25%] [G loss: 0.852746]\n",
      "1279 [D loss: 0.427409, acc.: 79.69%] [G loss: 1.616940]\n",
      "1280 [D loss: 0.802429, acc.: 59.38%] [G loss: 1.282471]\n",
      "1281 [D loss: 0.590159, acc.: 68.75%] [G loss: 1.363029]\n",
      "1282 [D loss: 0.722617, acc.: 64.06%] [G loss: 0.999781]\n",
      "1283 [D loss: 0.815333, acc.: 48.44%] [G loss: 0.771718]\n",
      "1284 [D loss: 0.847673, acc.: 53.12%] [G loss: 1.462186]\n",
      "1285 [D loss: 0.804756, acc.: 54.69%] [G loss: 1.485790]\n",
      "1286 [D loss: 0.631162, acc.: 75.00%] [G loss: 1.470527]\n",
      "1287 [D loss: 0.833130, acc.: 50.00%] [G loss: 1.008406]\n",
      "1288 [D loss: 0.837864, acc.: 53.12%] [G loss: 0.966958]\n",
      "1289 [D loss: 0.418244, acc.: 82.81%] [G loss: 1.239771]\n",
      "1290 [D loss: 0.480745, acc.: 81.25%] [G loss: 0.448309]\n",
      "1291 [D loss: 0.696291, acc.: 59.38%] [G loss: 1.117719]\n",
      "1292 [D loss: 0.679502, acc.: 56.25%] [G loss: 1.176854]\n",
      "1293 [D loss: 0.894580, acc.: 59.38%] [G loss: 1.373542]\n",
      "1294 [D loss: 0.628996, acc.: 64.06%] [G loss: 0.639834]\n",
      "1295 [D loss: 0.725262, acc.: 67.19%] [G loss: 1.017066]\n",
      "1296 [D loss: 0.898724, acc.: 48.44%] [G loss: 1.194959]\n",
      "1297 [D loss: 0.890362, acc.: 54.69%] [G loss: 1.218793]\n",
      "1298 [D loss: 0.760675, acc.: 59.38%] [G loss: 1.239222]\n",
      "1299 [D loss: 0.743636, acc.: 57.81%] [G loss: 1.161078]\n",
      "1300 [D loss: 0.698719, acc.: 68.75%] [G loss: 1.090240]\n",
      "1301 [D loss: 0.762029, acc.: 57.81%] [G loss: 1.324239]\n",
      "1302 [D loss: 0.597371, acc.: 71.88%] [G loss: 0.965099]\n",
      "1303 [D loss: 0.701754, acc.: 59.38%] [G loss: 1.483376]\n",
      "1304 [D loss: 0.744622, acc.: 62.50%] [G loss: 1.529666]\n",
      "1305 [D loss: 0.667072, acc.: 62.50%] [G loss: 1.267914]\n",
      "1306 [D loss: 0.833621, acc.: 54.69%] [G loss: 1.023382]\n",
      "1307 [D loss: 0.617021, acc.: 70.31%] [G loss: 1.346385]\n",
      "1308 [D loss: 0.928844, acc.: 48.44%] [G loss: 0.904873]\n",
      "1309 [D loss: 0.470235, acc.: 79.69%] [G loss: 1.084521]\n",
      "1310 [D loss: 0.840172, acc.: 48.44%] [G loss: 1.172731]\n",
      "1311 [D loss: 0.785221, acc.: 56.25%] [G loss: 1.347986]\n",
      "1312 [D loss: 0.481437, acc.: 76.56%] [G loss: 1.318574]\n",
      "1313 [D loss: 0.676427, acc.: 67.19%] [G loss: 1.223920]\n",
      "1314 [D loss: 0.526748, acc.: 68.75%] [G loss: 0.994997]\n",
      "1315 [D loss: 0.705813, acc.: 64.06%] [G loss: 1.554196]\n",
      "1316 [D loss: 1.352638, acc.: 29.69%] [G loss: 1.084410]\n",
      "1317 [D loss: 0.782613, acc.: 59.38%] [G loss: 1.269643]\n",
      "1318 [D loss: 0.784870, acc.: 59.38%] [G loss: 1.824853]\n",
      "1319 [D loss: 0.810194, acc.: 57.81%] [G loss: 1.307224]\n",
      "1320 [D loss: 1.011334, acc.: 48.44%] [G loss: 0.907266]\n",
      "1321 [D loss: 0.971126, acc.: 40.62%] [G loss: 1.074602]\n",
      "1322 [D loss: 1.129382, acc.: 28.12%] [G loss: 1.170803]\n",
      "1323 [D loss: 0.840926, acc.: 46.88%] [G loss: 1.567246]\n",
      "1324 [D loss: 0.657958, acc.: 68.75%] [G loss: 1.726864]\n",
      "1325 [D loss: 0.838601, acc.: 54.69%] [G loss: 1.349460]\n",
      "1326 [D loss: 0.528534, acc.: 79.69%] [G loss: 0.975591]\n",
      "1327 [D loss: 0.885924, acc.: 50.00%] [G loss: 1.680316]\n",
      "1328 [D loss: 0.650860, acc.: 62.50%] [G loss: 1.537741]\n",
      "1329 [D loss: 0.611345, acc.: 68.75%] [G loss: 1.163193]\n",
      "1330 [D loss: 0.535338, acc.: 76.56%] [G loss: 0.955508]\n",
      "1331 [D loss: 0.770316, acc.: 62.50%] [G loss: 0.743916]\n",
      "1332 [D loss: 0.775535, acc.: 51.56%] [G loss: 1.093401]\n",
      "1333 [D loss: 1.039081, acc.: 39.06%] [G loss: 1.742445]\n",
      "1334 [D loss: 0.567985, acc.: 75.00%] [G loss: 1.415663]\n",
      "1335 [D loss: 1.039645, acc.: 43.75%] [G loss: 1.385274]\n",
      "1336 [D loss: 0.682120, acc.: 67.19%] [G loss: 1.229203]\n",
      "1337 [D loss: 0.736031, acc.: 56.25%] [G loss: 0.973234]\n",
      "1338 [D loss: 0.844959, acc.: 51.56%] [G loss: 1.385627]\n",
      "1339 [D loss: 0.773140, acc.: 59.38%] [G loss: 2.016302]\n",
      "1340 [D loss: 0.770677, acc.: 56.25%] [G loss: 1.904212]\n",
      "1341 [D loss: 0.916199, acc.: 48.44%] [G loss: 0.868721]\n",
      "1342 [D loss: 0.677321, acc.: 59.38%] [G loss: 1.462676]\n",
      "1343 [D loss: 1.044344, acc.: 43.75%] [G loss: 1.198312]\n",
      "1344 [D loss: 0.732534, acc.: 56.25%] [G loss: 1.653358]\n",
      "1345 [D loss: 0.497734, acc.: 75.00%] [G loss: 1.643408]\n",
      "1346 [D loss: 0.844715, acc.: 50.00%] [G loss: 1.481634]\n",
      "1347 [D loss: 0.958120, acc.: 35.94%] [G loss: 1.137824]\n",
      "1348 [D loss: 0.803885, acc.: 57.81%] [G loss: 0.940904]\n",
      "1349 [D loss: 0.730867, acc.: 51.56%] [G loss: 0.935318]\n",
      "1350 [D loss: 0.566922, acc.: 62.50%] [G loss: 1.630787]\n",
      "1351 [D loss: 0.755073, acc.: 59.38%] [G loss: 1.083849]\n",
      "1352 [D loss: 0.889348, acc.: 46.88%] [G loss: 1.572150]\n",
      "1353 [D loss: 0.992160, acc.: 32.81%] [G loss: 1.336058]\n",
      "1354 [D loss: 0.536509, acc.: 67.19%] [G loss: 1.143001]\n",
      "1355 [D loss: 1.298427, acc.: 26.56%] [G loss: 1.195935]\n",
      "1356 [D loss: 0.721597, acc.: 48.44%] [G loss: 1.797715]\n",
      "1357 [D loss: 0.522437, acc.: 75.00%] [G loss: 1.594163]\n",
      "1358 [D loss: 0.670890, acc.: 65.62%] [G loss: 1.623544]\n",
      "1359 [D loss: 0.642653, acc.: 64.06%] [G loss: 0.598403]\n",
      "1360 [D loss: 0.676584, acc.: 68.75%] [G loss: 0.788775]\n",
      "1361 [D loss: 0.461920, acc.: 85.94%] [G loss: 1.029361]\n",
      "1362 [D loss: 0.702666, acc.: 59.38%] [G loss: 1.165288]\n",
      "1363 [D loss: 0.737692, acc.: 59.38%] [G loss: 1.450596]\n",
      "1364 [D loss: 0.518022, acc.: 70.31%] [G loss: 0.624457]\n",
      "1365 [D loss: 0.770565, acc.: 57.81%] [G loss: 0.854497]\n",
      "1366 [D loss: 0.636686, acc.: 67.19%] [G loss: 1.525130]\n",
      "1367 [D loss: 0.714750, acc.: 62.50%] [G loss: 1.237650]\n",
      "1368 [D loss: 0.692043, acc.: 64.06%] [G loss: 0.890988]\n",
      "1369 [D loss: 0.872482, acc.: 48.44%] [G loss: 1.190227]\n",
      "1370 [D loss: 0.420614, acc.: 82.81%] [G loss: 1.464918]\n",
      "1371 [D loss: 0.589516, acc.: 67.19%] [G loss: 0.964017]\n",
      "1372 [D loss: 0.374362, acc.: 84.38%] [G loss: 0.824754]\n",
      "1373 [D loss: 0.367061, acc.: 85.94%] [G loss: 0.668521]\n",
      "1374 [D loss: 0.617512, acc.: 62.50%] [G loss: 0.905288]\n",
      "1375 [D loss: 0.268317, acc.: 92.19%] [G loss: 0.942041]\n",
      "1376 [D loss: 0.545170, acc.: 73.44%] [G loss: 0.984524]\n",
      "1377 [D loss: 0.667663, acc.: 60.94%] [G loss: 1.490032]\n",
      "1378 [D loss: 0.612877, acc.: 62.50%] [G loss: 1.419835]\n",
      "1379 [D loss: 0.860454, acc.: 62.50%] [G loss: 0.899657]\n",
      "1380 [D loss: 0.543403, acc.: 73.44%] [G loss: 0.784387]\n",
      "1381 [D loss: 0.857869, acc.: 51.56%] [G loss: 0.958662]\n",
      "1382 [D loss: 0.409822, acc.: 82.81%] [G loss: 1.244120]\n",
      "1383 [D loss: 0.785028, acc.: 59.38%] [G loss: 1.246815]\n",
      "1384 [D loss: 0.501354, acc.: 75.00%] [G loss: 0.725670]\n",
      "1385 [D loss: 0.508632, acc.: 75.00%] [G loss: 0.848534]\n",
      "1386 [D loss: 0.784519, acc.: 54.69%] [G loss: 1.188512]\n",
      "1387 [D loss: 0.488136, acc.: 71.88%] [G loss: 1.614722]\n",
      "1388 [D loss: 1.063363, acc.: 48.44%] [G loss: 0.773056]\n",
      "1389 [D loss: 1.314956, acc.: 34.38%] [G loss: 0.450938]\n",
      "1390 [D loss: 0.572244, acc.: 70.31%] [G loss: 0.979696]\n",
      "1391 [D loss: 0.664950, acc.: 67.19%] [G loss: 0.785644]\n",
      "1392 [D loss: 0.634004, acc.: 65.62%] [G loss: 0.785045]\n",
      "1393 [D loss: 0.629863, acc.: 70.31%] [G loss: 0.871414]\n",
      "1394 [D loss: 0.754712, acc.: 53.12%] [G loss: 0.762037]\n",
      "1395 [D loss: 1.061614, acc.: 43.75%] [G loss: 1.735848]\n",
      "1396 [D loss: 1.180506, acc.: 42.19%] [G loss: 1.340670]\n",
      "1397 [D loss: 0.693049, acc.: 65.62%] [G loss: 1.392135]\n",
      "1398 [D loss: 0.510502, acc.: 75.00%] [G loss: 1.673418]\n",
      "1399 [D loss: 0.953075, acc.: 46.88%] [G loss: 1.431750]\n",
      "1400 [D loss: 0.484566, acc.: 73.44%] [G loss: 1.462552]\n",
      "1401 [D loss: 0.879258, acc.: 53.12%] [G loss: 1.148375]\n",
      "1402 [D loss: 0.700658, acc.: 64.06%] [G loss: 0.851266]\n",
      "1403 [D loss: 0.915627, acc.: 53.12%] [G loss: 1.257648]\n",
      "1404 [D loss: 0.858225, acc.: 51.56%] [G loss: 1.429890]\n",
      "1405 [D loss: 0.779201, acc.: 64.06%] [G loss: 1.483172]\n",
      "1406 [D loss: 1.001609, acc.: 46.88%] [G loss: 1.423575]\n",
      "1407 [D loss: 0.635163, acc.: 70.31%] [G loss: 1.078540]\n",
      "1408 [D loss: 0.875533, acc.: 48.44%] [G loss: 0.873948]\n",
      "1409 [D loss: 0.467599, acc.: 75.00%] [G loss: 1.122683]\n",
      "1410 [D loss: 0.505445, acc.: 71.88%] [G loss: 0.944895]\n",
      "1411 [D loss: 0.585211, acc.: 68.75%] [G loss: 1.400259]\n",
      "1412 [D loss: 0.642859, acc.: 68.75%] [G loss: 0.911309]\n",
      "1413 [D loss: 0.825192, acc.: 60.94%] [G loss: 1.216316]\n",
      "1414 [D loss: 0.387028, acc.: 78.12%] [G loss: 0.628538]\n",
      "1415 [D loss: 0.832633, acc.: 53.12%] [G loss: 0.675358]\n",
      "1416 [D loss: 0.629580, acc.: 67.19%] [G loss: 1.229290]\n",
      "1417 [D loss: 0.949688, acc.: 46.88%] [G loss: 1.072138]\n",
      "1418 [D loss: 0.809381, acc.: 59.38%] [G loss: 0.914132]\n",
      "1419 [D loss: 0.869684, acc.: 48.44%] [G loss: 0.906783]\n",
      "1420 [D loss: 0.845256, acc.: 46.88%] [G loss: 1.408185]\n",
      "1421 [D loss: 0.690831, acc.: 67.19%] [G loss: 1.039740]\n",
      "1422 [D loss: 1.398188, acc.: 32.81%] [G loss: 0.977066]\n",
      "1423 [D loss: 0.634126, acc.: 67.19%] [G loss: 1.525517]\n",
      "1424 [D loss: 0.930654, acc.: 43.75%] [G loss: 0.915078]\n",
      "1425 [D loss: 0.658115, acc.: 67.19%] [G loss: 0.985963]\n",
      "1426 [D loss: 0.654707, acc.: 62.50%] [G loss: 0.997075]\n",
      "1427 [D loss: 0.629139, acc.: 73.44%] [G loss: 1.160188]\n",
      "1428 [D loss: 0.716747, acc.: 64.06%] [G loss: 1.257194]\n",
      "1429 [D loss: 0.848449, acc.: 46.88%] [G loss: 1.167205]\n",
      "1430 [D loss: 0.664003, acc.: 60.94%] [G loss: 1.504861]\n",
      "1431 [D loss: 1.206136, acc.: 28.12%] [G loss: 1.066929]\n",
      "1432 [D loss: 0.902857, acc.: 48.44%] [G loss: 1.202590]\n",
      "1433 [D loss: 0.722181, acc.: 60.94%] [G loss: 1.337672]\n",
      "1434 [D loss: 0.670977, acc.: 73.44%] [G loss: 0.945314]\n",
      "1435 [D loss: 0.588851, acc.: 67.19%] [G loss: 0.940726]\n",
      "1436 [D loss: 1.099925, acc.: 31.25%] [G loss: 1.103548]\n",
      "1437 [D loss: 0.963973, acc.: 46.88%] [G loss: 1.620706]\n",
      "1438 [D loss: 0.512346, acc.: 71.88%] [G loss: 2.117732]\n",
      "1439 [D loss: 0.917731, acc.: 53.12%] [G loss: 0.890118]\n",
      "1440 [D loss: 0.854702, acc.: 50.00%] [G loss: 0.784686]\n",
      "1441 [D loss: 1.200563, acc.: 31.25%] [G loss: 0.912822]\n",
      "1442 [D loss: 0.600435, acc.: 67.19%] [G loss: 1.080431]\n",
      "1443 [D loss: 0.816997, acc.: 57.81%] [G loss: 1.153807]\n",
      "1444 [D loss: 0.606502, acc.: 73.44%] [G loss: 1.079680]\n",
      "1445 [D loss: 0.974748, acc.: 48.44%] [G loss: 1.076405]\n",
      "1446 [D loss: 0.577491, acc.: 68.75%] [G loss: 1.277642]\n",
      "1447 [D loss: 0.497695, acc.: 71.88%] [G loss: 1.216299]\n",
      "1448 [D loss: 0.620238, acc.: 68.75%] [G loss: 0.954346]\n",
      "1449 [D loss: 1.000488, acc.: 46.88%] [G loss: 1.192981]\n",
      "1450 [D loss: 0.491746, acc.: 68.75%] [G loss: 1.713100]\n",
      "1451 [D loss: 0.979630, acc.: 37.50%] [G loss: 1.142054]\n",
      "1452 [D loss: 0.621459, acc.: 64.06%] [G loss: 0.893864]\n",
      "1453 [D loss: 0.634209, acc.: 64.06%] [G loss: 0.991318]\n",
      "1454 [D loss: 0.794204, acc.: 67.19%] [G loss: 0.950233]\n",
      "1455 [D loss: 0.620417, acc.: 59.38%] [G loss: 1.195566]\n",
      "1456 [D loss: 0.610276, acc.: 70.31%] [G loss: 1.710310]\n",
      "1457 [D loss: 0.620740, acc.: 73.44%] [G loss: 1.279720]\n",
      "1458 [D loss: 0.834565, acc.: 59.38%] [G loss: 1.347257]\n",
      "1459 [D loss: 0.969970, acc.: 45.31%] [G loss: 1.114796]\n",
      "1460 [D loss: 0.760136, acc.: 59.38%] [G loss: 0.969033]\n",
      "1461 [D loss: 0.677088, acc.: 62.50%] [G loss: 1.441128]\n",
      "1462 [D loss: 0.879602, acc.: 56.25%] [G loss: 1.080341]\n",
      "1463 [D loss: 0.425516, acc.: 76.56%] [G loss: 0.665001]\n",
      "1464 [D loss: 1.003535, acc.: 50.00%] [G loss: 1.012486]\n",
      "1465 [D loss: 0.463187, acc.: 78.12%] [G loss: 1.476985]\n",
      "1466 [D loss: 0.677041, acc.: 65.62%] [G loss: 1.057547]\n",
      "1467 [D loss: 0.892073, acc.: 51.56%] [G loss: 1.079587]\n",
      "1468 [D loss: 0.542889, acc.: 81.25%] [G loss: 1.692341]\n",
      "1469 [D loss: 0.882618, acc.: 57.81%] [G loss: 0.949073]\n",
      "1470 [D loss: 0.791011, acc.: 54.69%] [G loss: 0.893096]\n",
      "1471 [D loss: 0.665755, acc.: 60.94%] [G loss: 1.197245]\n",
      "1472 [D loss: 0.600705, acc.: 65.62%] [G loss: 1.140297]\n",
      "1473 [D loss: 0.729398, acc.: 53.12%] [G loss: 1.210274]\n",
      "1474 [D loss: 0.886878, acc.: 54.69%] [G loss: 1.876050]\n",
      "1475 [D loss: 0.679754, acc.: 67.19%] [G loss: 0.855342]\n",
      "1476 [D loss: 0.373116, acc.: 82.81%] [G loss: 0.502794]\n",
      "1477 [D loss: 0.686162, acc.: 59.38%] [G loss: 0.862192]\n",
      "1478 [D loss: 0.519382, acc.: 68.75%] [G loss: 1.099213]\n",
      "1479 [D loss: 0.763506, acc.: 62.50%] [G loss: 1.325143]\n",
      "1480 [D loss: 0.650884, acc.: 59.38%] [G loss: 1.449917]\n",
      "1481 [D loss: 0.730035, acc.: 59.38%] [G loss: 1.136483]\n",
      "1482 [D loss: 0.748727, acc.: 64.06%] [G loss: 0.659686]\n",
      "1483 [D loss: 1.127509, acc.: 46.88%] [G loss: 0.827421]\n",
      "1484 [D loss: 0.388433, acc.: 79.69%] [G loss: 1.079886]\n",
      "1485 [D loss: 0.750458, acc.: 62.50%] [G loss: 1.149056]\n",
      "1486 [D loss: 0.424380, acc.: 82.81%] [G loss: 0.788727]\n",
      "1487 [D loss: 0.584460, acc.: 73.44%] [G loss: 0.884669]\n",
      "1488 [D loss: 1.010513, acc.: 40.62%] [G loss: 1.164430]\n",
      "1489 [D loss: 0.991572, acc.: 48.44%] [G loss: 1.818062]\n",
      "1490 [D loss: 1.310806, acc.: 23.44%] [G loss: 1.676084]\n",
      "1491 [D loss: 0.623428, acc.: 67.19%] [G loss: 1.662524]\n",
      "1492 [D loss: 0.594188, acc.: 73.44%] [G loss: 1.098344]\n",
      "1493 [D loss: 0.549122, acc.: 65.62%] [G loss: 0.642569]\n",
      "1494 [D loss: 0.426287, acc.: 75.00%] [G loss: 0.786267]\n",
      "1495 [D loss: 0.859442, acc.: 48.44%] [G loss: 0.942659]\n",
      "1496 [D loss: 0.384639, acc.: 82.81%] [G loss: 1.239436]\n",
      "1497 [D loss: 0.463510, acc.: 75.00%] [G loss: 1.320461]\n",
      "1498 [D loss: 0.566274, acc.: 71.88%] [G loss: 0.784624]\n",
      "1499 [D loss: 0.450624, acc.: 78.12%] [G loss: 1.115627]\n",
      "1500 [D loss: 0.759331, acc.: 64.06%] [G loss: 1.521468]\n",
      "1501 [D loss: 1.122757, acc.: 42.19%] [G loss: 1.074662]\n",
      "1502 [D loss: 1.184941, acc.: 40.62%] [G loss: 0.917220]\n",
      "1503 [D loss: 0.423257, acc.: 84.38%] [G loss: 1.413006]\n",
      "1504 [D loss: 0.708870, acc.: 59.38%] [G loss: 1.356078]\n",
      "1505 [D loss: 0.523913, acc.: 71.88%] [G loss: 1.186568]\n",
      "1506 [D loss: 0.345830, acc.: 87.50%] [G loss: 0.749048]\n",
      "1507 [D loss: 1.132041, acc.: 45.31%] [G loss: 1.300749]\n",
      "1508 [D loss: 0.555629, acc.: 70.31%] [G loss: 1.616833]\n",
      "1509 [D loss: 0.961389, acc.: 46.88%] [G loss: 1.443379]\n",
      "1510 [D loss: 1.107445, acc.: 40.62%] [G loss: 1.071043]\n",
      "1511 [D loss: 0.609340, acc.: 64.06%] [G loss: 1.244823]\n",
      "1512 [D loss: 0.681928, acc.: 59.38%] [G loss: 1.155953]\n",
      "1513 [D loss: 1.226774, acc.: 34.38%] [G loss: 0.998849]\n",
      "1514 [D loss: 0.935149, acc.: 50.00%] [G loss: 1.325960]\n",
      "1515 [D loss: 0.933093, acc.: 48.44%] [G loss: 1.117696]\n",
      "1516 [D loss: 0.608843, acc.: 67.19%] [G loss: 1.021010]\n",
      "1517 [D loss: 0.410258, acc.: 84.38%] [G loss: 0.916922]\n",
      "1518 [D loss: 0.855517, acc.: 51.56%] [G loss: 1.108939]\n",
      "1519 [D loss: 0.665254, acc.: 75.00%] [G loss: 0.678024]\n",
      "1520 [D loss: 1.088362, acc.: 42.19%] [G loss: 0.830744]\n",
      "1521 [D loss: 1.192811, acc.: 32.81%] [G loss: 1.109942]\n",
      "1522 [D loss: 0.881670, acc.: 39.06%] [G loss: 1.604099]\n",
      "1523 [D loss: 0.803979, acc.: 48.44%] [G loss: 2.130067]\n",
      "1524 [D loss: 0.843708, acc.: 51.56%] [G loss: 2.060926]\n",
      "1525 [D loss: 0.679908, acc.: 64.06%] [G loss: 1.454712]\n",
      "1526 [D loss: 0.713439, acc.: 60.94%] [G loss: 1.333303]\n",
      "1527 [D loss: 0.514198, acc.: 70.31%] [G loss: 1.528533]\n",
      "1528 [D loss: 0.892167, acc.: 51.56%] [G loss: 1.414786]\n",
      "1529 [D loss: 0.919470, acc.: 43.75%] [G loss: 1.493882]\n",
      "1530 [D loss: 0.821141, acc.: 53.12%] [G loss: 1.173836]\n",
      "1531 [D loss: 0.780215, acc.: 53.12%] [G loss: 0.806530]\n",
      "1532 [D loss: 1.026286, acc.: 42.19%] [G loss: 1.034839]\n",
      "1533 [D loss: 0.701612, acc.: 57.81%] [G loss: 1.398018]\n",
      "1534 [D loss: 0.522072, acc.: 71.88%] [G loss: 1.007516]\n",
      "1535 [D loss: 0.698797, acc.: 59.38%] [G loss: 0.629670]\n",
      "1536 [D loss: 0.617250, acc.: 70.31%] [G loss: 0.905107]\n",
      "1537 [D loss: 0.343338, acc.: 89.06%] [G loss: 1.215885]\n",
      "1538 [D loss: 0.473368, acc.: 75.00%] [G loss: 0.790708]\n",
      "1539 [D loss: 0.417375, acc.: 75.00%] [G loss: 0.573684]\n",
      "1540 [D loss: 0.687678, acc.: 57.81%] [G loss: 1.012466]\n",
      "1541 [D loss: 0.302900, acc.: 87.50%] [G loss: 0.629844]\n",
      "1542 [D loss: 0.487335, acc.: 78.12%] [G loss: 0.722888]\n",
      "1543 [D loss: 0.613704, acc.: 64.06%] [G loss: 0.983458]\n",
      "1544 [D loss: 0.575196, acc.: 76.56%] [G loss: 0.839414]\n",
      "1545 [D loss: 0.715768, acc.: 62.50%] [G loss: 1.165548]\n",
      "1546 [D loss: 0.721195, acc.: 59.38%] [G loss: 0.972072]\n",
      "1547 [D loss: 0.567915, acc.: 67.19%] [G loss: 0.789562]\n",
      "1548 [D loss: 0.629272, acc.: 68.75%] [G loss: 1.233223]\n",
      "1549 [D loss: 0.790225, acc.: 62.50%] [G loss: 0.859005]\n",
      "1550 [D loss: 0.539849, acc.: 71.88%] [G loss: 0.733835]\n",
      "1551 [D loss: 0.811639, acc.: 59.38%] [G loss: 1.607302]\n",
      "1552 [D loss: 1.577459, acc.: 28.12%] [G loss: 0.887911]\n",
      "1553 [D loss: 0.831393, acc.: 51.56%] [G loss: 0.885255]\n",
      "1554 [D loss: 0.814180, acc.: 48.44%] [G loss: 1.575884]\n",
      "1555 [D loss: 0.523128, acc.: 75.00%] [G loss: 1.430669]\n",
      "1556 [D loss: 1.115874, acc.: 40.62%] [G loss: 0.865617]\n",
      "1557 [D loss: 0.585089, acc.: 68.75%] [G loss: 1.098032]\n",
      "1558 [D loss: 0.453693, acc.: 79.69%] [G loss: 1.537844]\n",
      "1559 [D loss: 0.934905, acc.: 45.31%] [G loss: 1.391469]\n",
      "1560 [D loss: 0.491365, acc.: 79.69%] [G loss: 1.544552]\n",
      "1561 [D loss: 0.520560, acc.: 73.44%] [G loss: 0.637275]\n",
      "1562 [D loss: 0.693443, acc.: 68.75%] [G loss: 0.770709]\n",
      "1563 [D loss: 0.748785, acc.: 54.69%] [G loss: 1.141779]\n",
      "1564 [D loss: 0.465772, acc.: 75.00%] [G loss: 0.917000]\n",
      "1565 [D loss: 0.757560, acc.: 56.25%] [G loss: 1.142139]\n",
      "1566 [D loss: 0.668165, acc.: 62.50%] [G loss: 0.898126]\n",
      "1567 [D loss: 0.747451, acc.: 59.38%] [G loss: 0.803279]\n",
      "1568 [D loss: 0.600879, acc.: 65.62%] [G loss: 0.808928]\n",
      "1569 [D loss: 0.444151, acc.: 76.56%] [G loss: 1.114748]\n",
      "1570 [D loss: 0.618744, acc.: 73.44%] [G loss: 0.736725]\n",
      "1571 [D loss: 1.135554, acc.: 43.75%] [G loss: 1.157035]\n",
      "1572 [D loss: 1.304961, acc.: 29.69%] [G loss: 0.933384]\n",
      "1573 [D loss: 0.757909, acc.: 57.81%] [G loss: 1.660000]\n",
      "1574 [D loss: 0.802728, acc.: 50.00%] [G loss: 1.431206]\n",
      "1575 [D loss: 1.005462, acc.: 53.12%] [G loss: 1.421401]\n",
      "1576 [D loss: 1.171969, acc.: 43.75%] [G loss: 1.366093]\n",
      "1577 [D loss: 0.902680, acc.: 50.00%] [G loss: 1.117158]\n",
      "1578 [D loss: 0.716629, acc.: 68.75%] [G loss: 1.389337]\n",
      "1579 [D loss: 0.701399, acc.: 59.38%] [G loss: 1.471248]\n",
      "1580 [D loss: 0.687091, acc.: 60.94%] [G loss: 1.509406]\n",
      "1581 [D loss: 1.008693, acc.: 37.50%] [G loss: 1.340083]\n",
      "1582 [D loss: 0.768750, acc.: 53.12%] [G loss: 1.830647]\n",
      "1583 [D loss: 0.616946, acc.: 70.31%] [G loss: 2.001977]\n",
      "1584 [D loss: 0.677443, acc.: 60.94%] [G loss: 1.594818]\n",
      "1585 [D loss: 0.767266, acc.: 53.12%] [G loss: 0.987550]\n",
      "1586 [D loss: 0.829234, acc.: 56.25%] [G loss: 1.054529]\n",
      "1587 [D loss: 0.843991, acc.: 53.12%] [G loss: 0.925382]\n",
      "1588 [D loss: 1.084559, acc.: 42.19%] [G loss: 1.189602]\n",
      "1589 [D loss: 0.424509, acc.: 82.81%] [G loss: 1.368489]\n",
      "1590 [D loss: 0.732709, acc.: 64.06%] [G loss: 1.370620]\n",
      "1591 [D loss: 0.631050, acc.: 67.19%] [G loss: 1.468968]\n",
      "1592 [D loss: 0.397560, acc.: 84.38%] [G loss: 1.717974]\n",
      "1593 [D loss: 0.688965, acc.: 65.62%] [G loss: 1.739683]\n",
      "1594 [D loss: 0.531335, acc.: 73.44%] [G loss: 1.653161]\n",
      "1595 [D loss: 1.083297, acc.: 40.62%] [G loss: 0.887566]\n",
      "1596 [D loss: 0.631288, acc.: 62.50%] [G loss: 1.283904]\n",
      "1597 [D loss: 0.637148, acc.: 67.19%] [G loss: 1.287450]\n",
      "1598 [D loss: 0.665483, acc.: 65.62%] [G loss: 0.913572]\n",
      "1599 [D loss: 0.540915, acc.: 75.00%] [G loss: 1.214462]\n",
      "1600 [D loss: 0.465514, acc.: 79.69%] [G loss: 0.673008]\n",
      "1601 [D loss: 0.943631, acc.: 50.00%] [G loss: 1.112383]\n",
      "1602 [D loss: 0.774607, acc.: 53.12%] [G loss: 0.979986]\n",
      "1603 [D loss: 0.712629, acc.: 70.31%] [G loss: 1.533760]\n",
      "1604 [D loss: 0.637492, acc.: 64.06%] [G loss: 1.529364]\n",
      "1605 [D loss: 0.577064, acc.: 71.88%] [G loss: 1.034330]\n",
      "1606 [D loss: 0.513611, acc.: 73.44%] [G loss: 0.877035]\n",
      "1607 [D loss: 0.841002, acc.: 51.56%] [G loss: 0.893898]\n",
      "1608 [D loss: 0.632627, acc.: 70.31%] [G loss: 1.593314]\n",
      "1609 [D loss: 0.676143, acc.: 64.06%] [G loss: 1.866055]\n",
      "1610 [D loss: 0.558267, acc.: 67.19%] [G loss: 1.701818]\n",
      "1611 [D loss: 0.759094, acc.: 57.81%] [G loss: 0.921373]\n",
      "1612 [D loss: 0.717753, acc.: 62.50%] [G loss: 1.145339]\n",
      "1613 [D loss: 0.658826, acc.: 67.19%] [G loss: 1.600335]\n",
      "1614 [D loss: 1.170861, acc.: 31.25%] [G loss: 0.940422]\n",
      "1615 [D loss: 0.651880, acc.: 59.38%] [G loss: 1.347079]\n",
      "1616 [D loss: 0.660429, acc.: 73.44%] [G loss: 1.606935]\n",
      "1617 [D loss: 0.787732, acc.: 57.81%] [G loss: 1.472350]\n",
      "1618 [D loss: 0.842815, acc.: 59.38%] [G loss: 1.224819]\n",
      "1619 [D loss: 0.507453, acc.: 73.44%] [G loss: 1.485774]\n",
      "1620 [D loss: 0.761980, acc.: 64.06%] [G loss: 0.911011]\n",
      "1621 [D loss: 0.628007, acc.: 67.19%] [G loss: 1.515697]\n",
      "1622 [D loss: 0.416436, acc.: 81.25%] [G loss: 1.376573]\n",
      "1623 [D loss: 0.871258, acc.: 56.25%] [G loss: 1.276339]\n",
      "1624 [D loss: 0.546594, acc.: 71.88%] [G loss: 1.585317]\n",
      "1625 [D loss: 0.783890, acc.: 59.38%] [G loss: 1.351773]\n",
      "1626 [D loss: 1.357331, acc.: 21.88%] [G loss: 1.459914]\n",
      "1627 [D loss: 0.862055, acc.: 48.44%] [G loss: 1.530391]\n",
      "1628 [D loss: 0.772824, acc.: 57.81%] [G loss: 1.168448]\n",
      "1629 [D loss: 0.693798, acc.: 65.62%] [G loss: 1.037250]\n",
      "1630 [D loss: 0.399280, acc.: 82.81%] [G loss: 1.305391]\n",
      "1631 [D loss: 0.799960, acc.: 60.94%] [G loss: 1.451972]\n",
      "1632 [D loss: 0.447510, acc.: 79.69%] [G loss: 1.090977]\n",
      "1633 [D loss: 0.795662, acc.: 51.56%] [G loss: 1.584136]\n",
      "1634 [D loss: 1.011820, acc.: 50.00%] [G loss: 0.951401]\n",
      "1635 [D loss: 0.615193, acc.: 68.75%] [G loss: 1.424710]\n",
      "1636 [D loss: 0.550280, acc.: 70.31%] [G loss: 1.879997]\n",
      "1637 [D loss: 0.752918, acc.: 67.19%] [G loss: 1.044129]\n",
      "1638 [D loss: 0.854415, acc.: 56.25%] [G loss: 1.111201]\n",
      "1639 [D loss: 0.642583, acc.: 67.19%] [G loss: 1.014722]\n",
      "1640 [D loss: 0.790805, acc.: 51.56%] [G loss: 1.485366]\n",
      "1641 [D loss: 0.629117, acc.: 70.31%] [G loss: 1.486277]\n",
      "1642 [D loss: 0.713738, acc.: 57.81%] [G loss: 1.426069]\n",
      "1643 [D loss: 0.469573, acc.: 76.56%] [G loss: 1.624541]\n",
      "1644 [D loss: 0.570773, acc.: 73.44%] [G loss: 0.754938]\n",
      "1645 [D loss: 0.904417, acc.: 45.31%] [G loss: 0.810315]\n",
      "1646 [D loss: 0.683779, acc.: 67.19%] [G loss: 1.215420]\n",
      "1647 [D loss: 0.731570, acc.: 59.38%] [G loss: 1.726290]\n",
      "1648 [D loss: 0.688646, acc.: 62.50%] [G loss: 0.946237]\n",
      "1649 [D loss: 0.938080, acc.: 48.44%] [G loss: 1.486693]\n",
      "1650 [D loss: 0.571940, acc.: 81.25%] [G loss: 1.128946]\n",
      "1651 [D loss: 0.797801, acc.: 59.38%] [G loss: 0.691212]\n",
      "1652 [D loss: 0.615211, acc.: 65.62%] [G loss: 0.875754]\n",
      "1653 [D loss: 0.667834, acc.: 65.62%] [G loss: 1.411267]\n",
      "1654 [D loss: 0.717369, acc.: 59.38%] [G loss: 2.089497]\n",
      "1655 [D loss: 1.082240, acc.: 48.44%] [G loss: 1.730808]\n",
      "1656 [D loss: 0.517058, acc.: 79.69%] [G loss: 1.202231]\n",
      "1657 [D loss: 0.857296, acc.: 48.44%] [G loss: 1.339513]\n",
      "1658 [D loss: 0.545545, acc.: 73.44%] [G loss: 1.397166]\n",
      "1659 [D loss: 1.063884, acc.: 40.62%] [G loss: 1.148797]\n",
      "1660 [D loss: 0.689701, acc.: 64.06%] [G loss: 0.880168]\n",
      "1661 [D loss: 0.700006, acc.: 68.75%] [G loss: 1.012308]\n",
      "1662 [D loss: 0.652237, acc.: 64.06%] [G loss: 0.973968]\n",
      "1663 [D loss: 0.347119, acc.: 85.94%] [G loss: 0.900625]\n",
      "1664 [D loss: 0.722536, acc.: 56.25%] [G loss: 0.780888]\n",
      "1665 [D loss: 1.289819, acc.: 28.12%] [G loss: 1.086440]\n",
      "1666 [D loss: 1.214930, acc.: 31.25%] [G loss: 1.317470]\n",
      "1667 [D loss: 0.431477, acc.: 82.81%] [G loss: 1.519949]\n",
      "1668 [D loss: 0.704362, acc.: 57.81%] [G loss: 0.871109]\n",
      "1669 [D loss: 0.696032, acc.: 57.81%] [G loss: 1.280703]\n",
      "1670 [D loss: 0.628366, acc.: 68.75%] [G loss: 1.336873]\n",
      "1671 [D loss: 0.650811, acc.: 65.62%] [G loss: 1.253306]\n",
      "1672 [D loss: 0.697083, acc.: 65.62%] [G loss: 1.342557]\n",
      "1673 [D loss: 0.571616, acc.: 68.75%] [G loss: 1.795020]\n",
      "1674 [D loss: 0.876396, acc.: 50.00%] [G loss: 0.950203]\n",
      "1675 [D loss: 0.444307, acc.: 79.69%] [G loss: 1.013597]\n",
      "1676 [D loss: 0.486434, acc.: 75.00%] [G loss: 0.842626]\n",
      "1677 [D loss: 0.258631, acc.: 92.19%] [G loss: 0.416966]\n",
      "1678 [D loss: 0.691260, acc.: 64.06%] [G loss: 1.042096]\n",
      "1679 [D loss: 0.490331, acc.: 75.00%] [G loss: 1.175932]\n",
      "1680 [D loss: 0.448995, acc.: 79.69%] [G loss: 0.868028]\n",
      "1681 [D loss: 0.479399, acc.: 75.00%] [G loss: 0.740254]\n",
      "1682 [D loss: 0.503553, acc.: 76.56%] [G loss: 0.869817]\n",
      "1683 [D loss: 0.455192, acc.: 75.00%] [G loss: 1.455412]\n",
      "1684 [D loss: 0.732304, acc.: 56.25%] [G loss: 1.383737]\n",
      "1685 [D loss: 0.675099, acc.: 67.19%] [G loss: 1.531446]\n",
      "1686 [D loss: 0.530111, acc.: 75.00%] [G loss: 1.688319]\n",
      "1687 [D loss: 1.006088, acc.: 39.06%] [G loss: 1.158172]\n",
      "1688 [D loss: 0.668348, acc.: 65.62%] [G loss: 1.504816]\n",
      "1689 [D loss: 0.800023, acc.: 53.12%] [G loss: 0.796910]\n",
      "1690 [D loss: 0.640824, acc.: 68.75%] [G loss: 1.380157]\n",
      "1691 [D loss: 0.385572, acc.: 81.25%] [G loss: 1.426827]\n",
      "1692 [D loss: 0.445979, acc.: 78.12%] [G loss: 0.740415]\n",
      "1693 [D loss: 0.281147, acc.: 90.62%] [G loss: 0.595106]\n",
      "1694 [D loss: 0.403825, acc.: 76.56%] [G loss: 0.798089]\n",
      "1695 [D loss: 0.326485, acc.: 84.38%] [G loss: 0.740567]\n",
      "1696 [D loss: 0.373495, acc.: 87.50%] [G loss: 0.833352]\n",
      "1697 [D loss: 0.841350, acc.: 51.56%] [G loss: 1.663380]\n",
      "1698 [D loss: 0.547240, acc.: 68.75%] [G loss: 2.198718]\n",
      "1699 [D loss: 1.154621, acc.: 39.06%] [G loss: 1.269037]\n",
      "1700 [D loss: 0.999550, acc.: 45.31%] [G loss: 1.629905]\n",
      "1701 [D loss: 0.683933, acc.: 70.31%] [G loss: 1.398214]\n",
      "1702 [D loss: 0.880284, acc.: 51.56%] [G loss: 1.250626]\n",
      "1703 [D loss: 0.740739, acc.: 56.25%] [G loss: 0.938449]\n",
      "1704 [D loss: 0.716663, acc.: 64.06%] [G loss: 1.261578]\n",
      "1705 [D loss: 0.436353, acc.: 78.12%] [G loss: 0.941753]\n",
      "1706 [D loss: 0.475772, acc.: 76.56%] [G loss: 0.928171]\n",
      "1707 [D loss: 0.319673, acc.: 85.94%] [G loss: 0.940782]\n",
      "1708 [D loss: 1.085710, acc.: 34.38%] [G loss: 0.683310]\n",
      "1709 [D loss: 0.625929, acc.: 68.75%] [G loss: 0.886633]\n",
      "1710 [D loss: 0.541424, acc.: 75.00%] [G loss: 1.278452]\n",
      "1711 [D loss: 0.510025, acc.: 75.00%] [G loss: 1.283578]\n",
      "1712 [D loss: 0.791468, acc.: 54.69%] [G loss: 0.932694]\n",
      "1713 [D loss: 0.746726, acc.: 59.38%] [G loss: 1.196723]\n",
      "1714 [D loss: 0.617977, acc.: 67.19%] [G loss: 1.202613]\n",
      "1715 [D loss: 0.627653, acc.: 67.19%] [G loss: 0.659639]\n",
      "1716 [D loss: 0.872627, acc.: 53.12%] [G loss: 0.702057]\n",
      "1717 [D loss: 0.586706, acc.: 75.00%] [G loss: 1.052145]\n",
      "1718 [D loss: 0.586658, acc.: 70.31%] [G loss: 1.721841]\n",
      "1719 [D loss: 0.571953, acc.: 71.88%] [G loss: 1.215344]\n",
      "1720 [D loss: 0.617880, acc.: 64.06%] [G loss: 1.447855]\n",
      "1721 [D loss: 0.484581, acc.: 75.00%] [G loss: 1.640686]\n",
      "1722 [D loss: 0.803142, acc.: 56.25%] [G loss: 1.519937]\n",
      "1723 [D loss: 0.686617, acc.: 67.19%] [G loss: 0.760738]\n",
      "1724 [D loss: 0.969358, acc.: 48.44%] [G loss: 1.320919]\n",
      "1725 [D loss: 0.998619, acc.: 40.62%] [G loss: 1.423776]\n",
      "1726 [D loss: 1.142272, acc.: 37.50%] [G loss: 1.715579]\n",
      "1727 [D loss: 0.679065, acc.: 64.06%] [G loss: 1.680357]\n",
      "1728 [D loss: 0.708704, acc.: 57.81%] [G loss: 1.442333]\n",
      "1729 [D loss: 0.779225, acc.: 64.06%] [G loss: 1.478230]\n",
      "1730 [D loss: 0.541752, acc.: 70.31%] [G loss: 1.287690]\n",
      "1731 [D loss: 0.961075, acc.: 42.19%] [G loss: 1.011015]\n",
      "1732 [D loss: 0.886924, acc.: 46.88%] [G loss: 1.449123]\n",
      "1733 [D loss: 0.484181, acc.: 73.44%] [G loss: 1.544639]\n",
      "1734 [D loss: 0.811262, acc.: 59.38%] [G loss: 1.808907]\n",
      "1735 [D loss: 0.814742, acc.: 54.69%] [G loss: 2.055648]\n",
      "1736 [D loss: 1.004180, acc.: 42.19%] [G loss: 1.488317]\n",
      "1737 [D loss: 0.799771, acc.: 53.12%] [G loss: 1.026115]\n",
      "1738 [D loss: 0.614876, acc.: 65.62%] [G loss: 1.122747]\n",
      "1739 [D loss: 0.841609, acc.: 53.12%] [G loss: 1.343050]\n",
      "1740 [D loss: 0.683371, acc.: 62.50%] [G loss: 0.937901]\n",
      "1741 [D loss: 0.755149, acc.: 59.38%] [G loss: 0.775902]\n",
      "1742 [D loss: 0.869682, acc.: 53.12%] [G loss: 1.168023]\n",
      "1743 [D loss: 0.609578, acc.: 70.31%] [G loss: 1.894953]\n",
      "1744 [D loss: 0.618264, acc.: 68.75%] [G loss: 1.266999]\n",
      "1745 [D loss: 0.874709, acc.: 45.31%] [G loss: 0.724781]\n",
      "1746 [D loss: 0.441000, acc.: 79.69%] [G loss: 0.505286]\n",
      "1747 [D loss: 0.413368, acc.: 81.25%] [G loss: 0.805240]\n",
      "1748 [D loss: 1.370324, acc.: 25.00%] [G loss: 1.040182]\n",
      "1749 [D loss: 0.719921, acc.: 62.50%] [G loss: 1.444131]\n",
      "1750 [D loss: 0.517522, acc.: 75.00%] [G loss: 1.847449]\n",
      "1751 [D loss: 0.684627, acc.: 65.62%] [G loss: 1.365975]\n",
      "1752 [D loss: 0.642787, acc.: 67.19%] [G loss: 1.889692]\n",
      "1753 [D loss: 0.515552, acc.: 76.56%] [G loss: 1.362669]\n",
      "1754 [D loss: 1.271871, acc.: 34.38%] [G loss: 1.116024]\n",
      "1755 [D loss: 0.568625, acc.: 71.88%] [G loss: 1.218072]\n",
      "1756 [D loss: 0.824283, acc.: 53.12%] [G loss: 1.430413]\n",
      "1757 [D loss: 0.725696, acc.: 64.06%] [G loss: 1.189993]\n",
      "1758 [D loss: 0.672594, acc.: 62.50%] [G loss: 1.316729]\n",
      "1759 [D loss: 0.852531, acc.: 48.44%] [G loss: 1.894619]\n",
      "1760 [D loss: 0.859065, acc.: 54.69%] [G loss: 1.290271]\n",
      "1761 [D loss: 0.767627, acc.: 59.38%] [G loss: 1.497420]\n",
      "1762 [D loss: 0.663657, acc.: 62.50%] [G loss: 1.451042]\n",
      "1763 [D loss: 0.608958, acc.: 67.19%] [G loss: 1.320940]\n",
      "1764 [D loss: 1.448556, acc.: 28.12%] [G loss: 1.232564]\n",
      "1765 [D loss: 0.775591, acc.: 59.38%] [G loss: 0.956533]\n",
      "1766 [D loss: 0.446598, acc.: 78.12%] [G loss: 1.244365]\n",
      "1767 [D loss: 0.740097, acc.: 59.38%] [G loss: 1.153665]\n",
      "1768 [D loss: 0.607039, acc.: 73.44%] [G loss: 1.351310]\n",
      "1769 [D loss: 0.675556, acc.: 57.81%] [G loss: 1.485275]\n",
      "1770 [D loss: 0.747213, acc.: 56.25%] [G loss: 1.140384]\n",
      "1771 [D loss: 1.196660, acc.: 26.56%] [G loss: 1.580555]\n",
      "1772 [D loss: 0.731122, acc.: 56.25%] [G loss: 1.987935]\n",
      "1773 [D loss: 0.448128, acc.: 76.56%] [G loss: 0.945068]\n",
      "1774 [D loss: 0.976826, acc.: 42.19%] [G loss: 0.955648]\n",
      "1775 [D loss: 0.756722, acc.: 53.12%] [G loss: 1.611681]\n",
      "1776 [D loss: 0.772390, acc.: 51.56%] [G loss: 1.468963]\n",
      "1777 [D loss: 0.649348, acc.: 60.94%] [G loss: 1.372823]\n",
      "1778 [D loss: 0.975027, acc.: 45.31%] [G loss: 1.340636]\n",
      "1779 [D loss: 0.552137, acc.: 67.19%] [G loss: 1.043459]\n",
      "1780 [D loss: 0.602273, acc.: 64.06%] [G loss: 0.579288]\n",
      "1781 [D loss: 0.281810, acc.: 92.19%] [G loss: 0.594777]\n",
      "1782 [D loss: 0.264714, acc.: 90.62%] [G loss: 0.410980]\n",
      "1783 [D loss: 0.555133, acc.: 70.31%] [G loss: 0.920538]\n",
      "1784 [D loss: 0.496566, acc.: 75.00%] [G loss: 1.300922]\n",
      "1785 [D loss: 0.972393, acc.: 43.75%] [G loss: 0.908717]\n",
      "1786 [D loss: 0.666800, acc.: 57.81%] [G loss: 1.164208]\n",
      "1787 [D loss: 1.005152, acc.: 42.19%] [G loss: 1.692093]\n",
      "1788 [D loss: 1.040581, acc.: 37.50%] [G loss: 1.292298]\n",
      "1789 [D loss: 0.997574, acc.: 51.56%] [G loss: 1.355647]\n",
      "1790 [D loss: 0.694717, acc.: 56.25%] [G loss: 1.596568]\n",
      "1791 [D loss: 0.661141, acc.: 62.50%] [G loss: 1.045955]\n",
      "1792 [D loss: 0.697593, acc.: 67.19%] [G loss: 0.797164]\n",
      "1793 [D loss: 0.470904, acc.: 78.12%] [G loss: 0.944487]\n",
      "1794 [D loss: 0.681122, acc.: 60.94%] [G loss: 0.806473]\n",
      "1795 [D loss: 0.802348, acc.: 54.69%] [G loss: 1.240090]\n",
      "1796 [D loss: 0.444350, acc.: 81.25%] [G loss: 1.018332]\n",
      "1797 [D loss: 0.918837, acc.: 42.19%] [G loss: 1.260506]\n",
      "1798 [D loss: 0.542222, acc.: 73.44%] [G loss: 1.311088]\n",
      "1799 [D loss: 1.069284, acc.: 37.50%] [G loss: 1.549020]\n",
      "1800 [D loss: 0.912781, acc.: 50.00%] [G loss: 1.072276]\n",
      "1801 [D loss: 0.956734, acc.: 51.56%] [G loss: 1.028242]\n",
      "1802 [D loss: 1.185934, acc.: 42.19%] [G loss: 1.048814]\n",
      "1803 [D loss: 0.738078, acc.: 59.38%] [G loss: 1.009098]\n",
      "1804 [D loss: 1.067351, acc.: 39.06%] [G loss: 1.551238]\n",
      "1805 [D loss: 0.845644, acc.: 54.69%] [G loss: 0.892495]\n",
      "1806 [D loss: 0.686483, acc.: 67.19%] [G loss: 0.912531]\n",
      "1807 [D loss: 0.549985, acc.: 73.44%] [G loss: 0.865510]\n",
      "1808 [D loss: 0.517116, acc.: 78.12%] [G loss: 1.017475]\n",
      "1809 [D loss: 0.574485, acc.: 70.31%] [G loss: 1.222367]\n",
      "1810 [D loss: 0.798855, acc.: 53.12%] [G loss: 1.670883]\n",
      "1811 [D loss: 0.715479, acc.: 57.81%] [G loss: 0.993733]\n",
      "1812 [D loss: 0.841774, acc.: 53.12%] [G loss: 1.202385]\n",
      "1813 [D loss: 0.682495, acc.: 65.62%] [G loss: 1.275513]\n",
      "1814 [D loss: 0.667759, acc.: 65.62%] [G loss: 1.395221]\n",
      "1815 [D loss: 1.167435, acc.: 34.38%] [G loss: 1.324832]\n",
      "1816 [D loss: 0.675932, acc.: 62.50%] [G loss: 1.525573]\n",
      "1817 [D loss: 0.857370, acc.: 48.44%] [G loss: 1.447895]\n",
      "1818 [D loss: 0.777258, acc.: 62.50%] [G loss: 2.077232]\n",
      "1819 [D loss: 0.724057, acc.: 62.50%] [G loss: 1.209045]\n",
      "1820 [D loss: 0.963275, acc.: 39.06%] [G loss: 0.941517]\n",
      "1821 [D loss: 0.791407, acc.: 56.25%] [G loss: 1.097749]\n",
      "1822 [D loss: 0.812885, acc.: 51.56%] [G loss: 1.052640]\n",
      "1823 [D loss: 0.460760, acc.: 78.12%] [G loss: 1.315038]\n",
      "1824 [D loss: 0.685187, acc.: 67.19%] [G loss: 1.515086]\n",
      "1825 [D loss: 0.645028, acc.: 67.19%] [G loss: 1.557839]\n",
      "1826 [D loss: 1.462192, acc.: 18.75%] [G loss: 1.133532]\n",
      "1827 [D loss: 0.565149, acc.: 71.88%] [G loss: 1.263955]\n",
      "1828 [D loss: 1.049076, acc.: 43.75%] [G loss: 1.285626]\n",
      "1829 [D loss: 0.969168, acc.: 46.88%] [G loss: 1.324500]\n",
      "1830 [D loss: 0.736689, acc.: 56.25%] [G loss: 1.219595]\n",
      "1831 [D loss: 0.641701, acc.: 62.50%] [G loss: 1.236316]\n",
      "1832 [D loss: 0.907034, acc.: 50.00%] [G loss: 1.045444]\n",
      "1833 [D loss: 1.075690, acc.: 34.38%] [G loss: 1.027828]\n",
      "1834 [D loss: 0.750909, acc.: 53.12%] [G loss: 1.456997]\n",
      "1835 [D loss: 0.622353, acc.: 59.38%] [G loss: 1.292035]\n",
      "1836 [D loss: 0.777745, acc.: 57.81%] [G loss: 0.942758]\n",
      "1837 [D loss: 0.782122, acc.: 54.69%] [G loss: 0.807840]\n",
      "1838 [D loss: 0.606844, acc.: 65.62%] [G loss: 0.957731]\n",
      "1839 [D loss: 0.949043, acc.: 39.06%] [G loss: 0.947996]\n",
      "1840 [D loss: 0.793287, acc.: 50.00%] [G loss: 1.831406]\n",
      "1841 [D loss: 0.906664, acc.: 46.88%] [G loss: 1.250190]\n",
      "1842 [D loss: 0.904145, acc.: 46.88%] [G loss: 1.093771]\n",
      "1843 [D loss: 0.794277, acc.: 56.25%] [G loss: 0.950374]\n",
      "1844 [D loss: 0.714635, acc.: 59.38%] [G loss: 1.627547]\n",
      "1845 [D loss: 0.793546, acc.: 50.00%] [G loss: 1.569612]\n",
      "1846 [D loss: 0.821568, acc.: 56.25%] [G loss: 1.824670]\n",
      "1847 [D loss: 0.693791, acc.: 62.50%] [G loss: 0.937197]\n",
      "1848 [D loss: 0.743438, acc.: 50.00%] [G loss: 1.454148]\n",
      "1849 [D loss: 0.793496, acc.: 51.56%] [G loss: 1.174331]\n",
      "1850 [D loss: 0.630354, acc.: 59.38%] [G loss: 1.398916]\n",
      "1851 [D loss: 0.906117, acc.: 45.31%] [G loss: 1.273886]\n",
      "1852 [D loss: 0.949723, acc.: 45.31%] [G loss: 1.468521]\n",
      "1853 [D loss: 0.753410, acc.: 60.94%] [G loss: 1.304500]\n",
      "1854 [D loss: 0.746079, acc.: 59.38%] [G loss: 1.263370]\n",
      "1855 [D loss: 0.623829, acc.: 70.31%] [G loss: 1.323311]\n",
      "1856 [D loss: 1.089653, acc.: 45.31%] [G loss: 1.263224]\n",
      "1857 [D loss: 0.751543, acc.: 56.25%] [G loss: 0.863927]\n",
      "1858 [D loss: 0.884816, acc.: 50.00%] [G loss: 1.238629]\n",
      "1859 [D loss: 0.720881, acc.: 56.25%] [G loss: 1.153316]\n",
      "1860 [D loss: 0.830171, acc.: 54.69%] [G loss: 0.886487]\n",
      "1861 [D loss: 0.822599, acc.: 56.25%] [G loss: 1.285045]\n",
      "1862 [D loss: 0.469322, acc.: 73.44%] [G loss: 1.609908]\n",
      "1863 [D loss: 0.704651, acc.: 62.50%] [G loss: 0.923597]\n",
      "1864 [D loss: 0.888993, acc.: 45.31%] [G loss: 1.189954]\n",
      "1865 [D loss: 0.562535, acc.: 73.44%] [G loss: 1.290524]\n",
      "1866 [D loss: 0.682808, acc.: 62.50%] [G loss: 1.293646]\n",
      "1867 [D loss: 0.633926, acc.: 60.94%] [G loss: 1.566186]\n",
      "1868 [D loss: 0.797545, acc.: 56.25%] [G loss: 1.362555]\n",
      "1869 [D loss: 0.897652, acc.: 53.12%] [G loss: 1.372603]\n",
      "1870 [D loss: 0.780784, acc.: 54.69%] [G loss: 1.123521]\n",
      "1871 [D loss: 0.836388, acc.: 53.12%] [G loss: 0.947813]\n",
      "1872 [D loss: 0.547362, acc.: 73.44%] [G loss: 1.208780]\n",
      "1873 [D loss: 0.599001, acc.: 65.62%] [G loss: 0.820637]\n",
      "1874 [D loss: 0.428683, acc.: 79.69%] [G loss: 0.905674]\n",
      "1875 [D loss: 0.827713, acc.: 53.12%] [G loss: 1.045223]\n",
      "1876 [D loss: 0.633755, acc.: 64.06%] [G loss: 0.760640]\n",
      "1877 [D loss: 0.376927, acc.: 82.81%] [G loss: 0.954189]\n",
      "1878 [D loss: 0.521395, acc.: 73.44%] [G loss: 1.123337]\n",
      "1879 [D loss: 0.813052, acc.: 48.44%] [G loss: 1.325724]\n",
      "1880 [D loss: 0.612128, acc.: 67.19%] [G loss: 1.450255]\n",
      "1881 [D loss: 0.513235, acc.: 73.44%] [G loss: 0.713770]\n",
      "1882 [D loss: 0.903666, acc.: 50.00%] [G loss: 1.048438]\n",
      "1883 [D loss: 0.754666, acc.: 54.69%] [G loss: 1.548839]\n",
      "1884 [D loss: 0.727172, acc.: 57.81%] [G loss: 1.705492]\n",
      "1885 [D loss: 1.476044, acc.: 23.44%] [G loss: 0.874053]\n",
      "1886 [D loss: 0.708698, acc.: 60.94%] [G loss: 1.510417]\n",
      "1887 [D loss: 1.089010, acc.: 39.06%] [G loss: 1.100603]\n",
      "1888 [D loss: 1.117163, acc.: 43.75%] [G loss: 1.195573]\n",
      "1889 [D loss: 1.030519, acc.: 40.62%] [G loss: 2.001744]\n",
      "1890 [D loss: 0.591496, acc.: 67.19%] [G loss: 1.348558]\n",
      "1891 [D loss: 0.871890, acc.: 53.12%] [G loss: 1.160613]\n",
      "1892 [D loss: 0.902496, acc.: 45.31%] [G loss: 1.447061]\n",
      "1893 [D loss: 0.774898, acc.: 54.69%] [G loss: 1.153563]\n",
      "1894 [D loss: 0.684854, acc.: 53.12%] [G loss: 1.043247]\n",
      "1895 [D loss: 1.054319, acc.: 39.06%] [G loss: 1.314366]\n",
      "1896 [D loss: 0.973916, acc.: 43.75%] [G loss: 1.036236]\n",
      "1897 [D loss: 0.952116, acc.: 43.75%] [G loss: 1.106532]\n",
      "1898 [D loss: 0.935188, acc.: 39.06%] [G loss: 1.198691]\n",
      "1899 [D loss: 0.733327, acc.: 54.69%] [G loss: 1.160339]\n",
      "1900 [D loss: 0.844313, acc.: 51.56%] [G loss: 1.218043]\n",
      "1901 [D loss: 0.723340, acc.: 54.69%] [G loss: 1.195850]\n",
      "1902 [D loss: 0.692690, acc.: 54.69%] [G loss: 1.646391]\n",
      "1903 [D loss: 0.772148, acc.: 53.12%] [G loss: 1.441164]\n",
      "1904 [D loss: 0.550391, acc.: 68.75%] [G loss: 1.078987]\n",
      "1905 [D loss: 0.547605, acc.: 75.00%] [G loss: 1.386249]\n",
      "1906 [D loss: 0.623759, acc.: 68.75%] [G loss: 1.233371]\n",
      "1907 [D loss: 0.727842, acc.: 62.50%] [G loss: 1.312659]\n",
      "1908 [D loss: 0.831417, acc.: 57.81%] [G loss: 0.752554]\n",
      "1909 [D loss: 0.790176, acc.: 60.94%] [G loss: 1.327023]\n",
      "1910 [D loss: 0.854703, acc.: 54.69%] [G loss: 1.236061]\n",
      "1911 [D loss: 0.681052, acc.: 60.94%] [G loss: 1.147080]\n",
      "1912 [D loss: 0.826516, acc.: 53.12%] [G loss: 1.643405]\n",
      "1913 [D loss: 0.547223, acc.: 73.44%] [G loss: 1.071234]\n",
      "1914 [D loss: 0.819894, acc.: 53.12%] [G loss: 0.901995]\n",
      "1915 [D loss: 0.704002, acc.: 56.25%] [G loss: 1.200158]\n",
      "1916 [D loss: 0.566656, acc.: 73.44%] [G loss: 0.948235]\n",
      "1917 [D loss: 0.909241, acc.: 48.44%] [G loss: 1.068051]\n",
      "1918 [D loss: 0.922124, acc.: 51.56%] [G loss: 1.078799]\n",
      "1919 [D loss: 0.688449, acc.: 60.94%] [G loss: 1.128787]\n",
      "1920 [D loss: 0.770335, acc.: 53.12%] [G loss: 1.120517]\n",
      "1921 [D loss: 0.568718, acc.: 70.31%] [G loss: 0.847605]\n",
      "1922 [D loss: 0.641213, acc.: 71.88%] [G loss: 0.745514]\n",
      "1923 [D loss: 0.761719, acc.: 53.12%] [G loss: 0.990478]\n",
      "1924 [D loss: 0.901048, acc.: 42.19%] [G loss: 1.310781]\n",
      "1925 [D loss: 0.674130, acc.: 62.50%] [G loss: 1.211056]\n",
      "1926 [D loss: 0.680287, acc.: 59.38%] [G loss: 1.592852]\n",
      "1927 [D loss: 1.018906, acc.: 39.06%] [G loss: 1.128415]\n",
      "1928 [D loss: 0.705700, acc.: 64.06%] [G loss: 1.558710]\n",
      "1929 [D loss: 0.772839, acc.: 57.81%] [G loss: 1.697596]\n",
      "1930 [D loss: 0.728617, acc.: 57.81%] [G loss: 0.928322]\n",
      "1931 [D loss: 0.775463, acc.: 53.12%] [G loss: 0.943167]\n",
      "1932 [D loss: 0.399184, acc.: 78.12%] [G loss: 1.531390]\n",
      "1933 [D loss: 0.644063, acc.: 68.75%] [G loss: 0.874229]\n",
      "1934 [D loss: 0.525044, acc.: 71.88%] [G loss: 0.654593]\n",
      "1935 [D loss: 0.748312, acc.: 64.06%] [G loss: 0.549438]\n",
      "1936 [D loss: 0.433833, acc.: 75.00%] [G loss: 0.889628]\n",
      "1937 [D loss: 0.594544, acc.: 62.50%] [G loss: 1.643878]\n",
      "1938 [D loss: 0.510985, acc.: 75.00%] [G loss: 1.546287]\n",
      "1939 [D loss: 0.824775, acc.: 46.88%] [G loss: 1.176721]\n",
      "1940 [D loss: 0.806920, acc.: 59.38%] [G loss: 1.575043]\n",
      "1941 [D loss: 0.714682, acc.: 59.38%] [G loss: 1.208106]\n",
      "1942 [D loss: 0.968198, acc.: 45.31%] [G loss: 1.151373]\n",
      "1943 [D loss: 0.544680, acc.: 71.88%] [G loss: 1.124779]\n",
      "1944 [D loss: 0.834385, acc.: 57.81%] [G loss: 1.024130]\n",
      "1945 [D loss: 0.911283, acc.: 45.31%] [G loss: 1.229906]\n",
      "1946 [D loss: 0.676992, acc.: 59.38%] [G loss: 1.365191]\n",
      "1947 [D loss: 0.817709, acc.: 46.88%] [G loss: 1.511177]\n",
      "1948 [D loss: 0.466012, acc.: 76.56%] [G loss: 1.059475]\n",
      "1949 [D loss: 0.496214, acc.: 78.12%] [G loss: 0.772955]\n",
      "1950 [D loss: 0.614853, acc.: 65.62%] [G loss: 0.710508]\n",
      "1951 [D loss: 0.689553, acc.: 64.06%] [G loss: 1.512513]\n",
      "1952 [D loss: 0.661663, acc.: 65.62%] [G loss: 1.449630]\n",
      "1953 [D loss: 0.745602, acc.: 62.50%] [G loss: 1.455971]\n",
      "1954 [D loss: 0.560467, acc.: 70.31%] [G loss: 1.912147]\n",
      "1955 [D loss: 0.647550, acc.: 60.94%] [G loss: 1.135107]\n",
      "1956 [D loss: 0.610206, acc.: 65.62%] [G loss: 0.891229]\n",
      "1957 [D loss: 0.597390, acc.: 68.75%] [G loss: 0.370568]\n",
      "1958 [D loss: 0.657952, acc.: 65.62%] [G loss: 0.747457]\n",
      "1959 [D loss: 0.704950, acc.: 62.50%] [G loss: 1.813766]\n",
      "1960 [D loss: 0.685022, acc.: 60.94%] [G loss: 1.658262]\n",
      "1961 [D loss: 0.835109, acc.: 50.00%] [G loss: 1.361400]\n",
      "1962 [D loss: 0.591710, acc.: 75.00%] [G loss: 0.848957]\n",
      "1963 [D loss: 0.894645, acc.: 53.12%] [G loss: 1.293681]\n",
      "1964 [D loss: 0.582109, acc.: 71.88%] [G loss: 0.765102]\n",
      "1965 [D loss: 0.552609, acc.: 71.88%] [G loss: 1.087669]\n",
      "1966 [D loss: 0.996750, acc.: 43.75%] [G loss: 1.149537]\n",
      "1967 [D loss: 0.653981, acc.: 68.75%] [G loss: 1.245041]\n",
      "1968 [D loss: 0.910417, acc.: 42.19%] [G loss: 1.106197]\n",
      "1969 [D loss: 0.676997, acc.: 57.81%] [G loss: 1.506301]\n",
      "1970 [D loss: 0.525460, acc.: 73.44%] [G loss: 1.636588]\n",
      "1971 [D loss: 0.659380, acc.: 67.19%] [G loss: 0.769596]\n",
      "1972 [D loss: 0.821165, acc.: 60.94%] [G loss: 1.061139]\n",
      "1973 [D loss: 0.988011, acc.: 43.75%] [G loss: 1.266278]\n",
      "1974 [D loss: 0.529686, acc.: 70.31%] [G loss: 1.415016]\n",
      "1975 [D loss: 0.723307, acc.: 62.50%] [G loss: 2.068328]\n",
      "1976 [D loss: 1.080222, acc.: 43.75%] [G loss: 1.077105]\n",
      "1977 [D loss: 0.579182, acc.: 70.31%] [G loss: 1.331601]\n",
      "1978 [D loss: 0.937675, acc.: 48.44%] [G loss: 1.313693]\n",
      "1979 [D loss: 0.816836, acc.: 56.25%] [G loss: 1.112451]\n",
      "1980 [D loss: 0.581103, acc.: 73.44%] [G loss: 0.770962]\n",
      "1981 [D loss: 0.741873, acc.: 54.69%] [G loss: 0.962877]\n",
      "1982 [D loss: 0.374884, acc.: 85.94%] [G loss: 1.044870]\n",
      "1983 [D loss: 0.827449, acc.: 57.81%] [G loss: 1.103292]\n",
      "1984 [D loss: 1.471027, acc.: 20.31%] [G loss: 0.984141]\n",
      "1985 [D loss: 0.334257, acc.: 87.50%] [G loss: 0.993291]\n",
      "1986 [D loss: 0.509863, acc.: 75.00%] [G loss: 1.048964]\n",
      "1987 [D loss: 0.646713, acc.: 62.50%] [G loss: 0.812306]\n",
      "1988 [D loss: 0.575464, acc.: 75.00%] [G loss: 0.936549]\n",
      "1989 [D loss: 0.614318, acc.: 65.62%] [G loss: 0.674595]\n",
      "1990 [D loss: 1.445428, acc.: 21.88%] [G loss: 1.045144]\n",
      "1991 [D loss: 1.061476, acc.: 40.62%] [G loss: 1.590846]\n",
      "1992 [D loss: 0.881262, acc.: 46.88%] [G loss: 1.443893]\n",
      "1993 [D loss: 0.499688, acc.: 70.31%] [G loss: 1.727827]\n",
      "1994 [D loss: 0.697655, acc.: 62.50%] [G loss: 1.788388]\n",
      "1995 [D loss: 0.984536, acc.: 43.75%] [G loss: 0.942897]\n",
      "1996 [D loss: 0.462458, acc.: 78.12%] [G loss: 0.471148]\n",
      "1997 [D loss: 0.451785, acc.: 79.69%] [G loss: 0.520004]\n",
      "1998 [D loss: 0.594099, acc.: 71.88%] [G loss: 0.698041]\n",
      "1999 [D loss: 0.604961, acc.: 71.88%] [G loss: 1.038391]\n",
      "2000 [D loss: 1.347601, acc.: 26.56%] [G loss: 0.786402]\n",
      "2001 [D loss: 0.591866, acc.: 67.19%] [G loss: 0.745875]\n",
      "2002 [D loss: 0.466402, acc.: 81.25%] [G loss: 1.312865]\n",
      "2003 [D loss: 0.591137, acc.: 70.31%] [G loss: 1.079454]\n",
      "2004 [D loss: 0.596968, acc.: 64.06%] [G loss: 0.875022]\n",
      "2005 [D loss: 0.815195, acc.: 56.25%] [G loss: 1.460791]\n",
      "2006 [D loss: 0.687979, acc.: 62.50%] [G loss: 1.271107]\n",
      "2007 [D loss: 0.581094, acc.: 70.31%] [G loss: 1.463328]\n",
      "2008 [D loss: 0.591375, acc.: 65.62%] [G loss: 1.104456]\n",
      "2009 [D loss: 0.851766, acc.: 51.56%] [G loss: 0.946508]\n",
      "2010 [D loss: 0.450544, acc.: 75.00%] [G loss: 1.215819]\n",
      "2011 [D loss: 0.652510, acc.: 65.62%] [G loss: 0.957633]\n",
      "2012 [D loss: 0.527485, acc.: 73.44%] [G loss: 0.927906]\n",
      "2013 [D loss: 0.592074, acc.: 71.88%] [G loss: 0.533619]\n",
      "2014 [D loss: 0.431068, acc.: 81.25%] [G loss: 0.432624]\n",
      "2015 [D loss: 0.338871, acc.: 85.94%] [G loss: 0.620463]\n",
      "2016 [D loss: 0.585680, acc.: 70.31%] [G loss: 0.742515]\n",
      "2017 [D loss: 0.527159, acc.: 71.88%] [G loss: 0.704852]\n",
      "2018 [D loss: 0.544256, acc.: 68.75%] [G loss: 1.024853]\n",
      "2019 [D loss: 0.549707, acc.: 76.56%] [G loss: 0.923787]\n",
      "2020 [D loss: 0.794836, acc.: 59.38%] [G loss: 1.346440]\n",
      "2021 [D loss: 1.551502, acc.: 28.12%] [G loss: 0.742523]\n",
      "2022 [D loss: 0.502668, acc.: 78.12%] [G loss: 1.280409]\n",
      "2023 [D loss: 0.609704, acc.: 64.06%] [G loss: 1.720953]\n",
      "2024 [D loss: 0.392690, acc.: 81.25%] [G loss: 1.072199]\n",
      "2025 [D loss: 0.815710, acc.: 54.69%] [G loss: 1.134552]\n",
      "2026 [D loss: 0.742459, acc.: 57.81%] [G loss: 0.752826]\n",
      "2027 [D loss: 0.839148, acc.: 53.12%] [G loss: 0.897897]\n",
      "2028 [D loss: 0.389865, acc.: 81.25%] [G loss: 0.662866]\n",
      "2029 [D loss: 0.530195, acc.: 73.44%] [G loss: 0.916237]\n",
      "2030 [D loss: 0.822950, acc.: 60.94%] [G loss: 1.143922]\n",
      "2031 [D loss: 0.592610, acc.: 70.31%] [G loss: 1.656708]\n",
      "2032 [D loss: 1.043688, acc.: 50.00%] [G loss: 0.801309]\n",
      "2033 [D loss: 0.916095, acc.: 46.88%] [G loss: 1.215838]\n",
      "2034 [D loss: 0.597807, acc.: 67.19%] [G loss: 1.277346]\n",
      "2035 [D loss: 0.889760, acc.: 53.12%] [G loss: 1.147601]\n",
      "2036 [D loss: 0.718185, acc.: 65.62%] [G loss: 1.385190]\n",
      "2037 [D loss: 0.557720, acc.: 75.00%] [G loss: 1.157600]\n",
      "2038 [D loss: 0.588758, acc.: 67.19%] [G loss: 0.542309]\n",
      "2039 [D loss: 0.595539, acc.: 68.75%] [G loss: 0.588784]\n",
      "2040 [D loss: 0.977279, acc.: 54.69%] [G loss: 0.814359]\n",
      "2041 [D loss: 0.671061, acc.: 64.06%] [G loss: 1.017134]\n",
      "2042 [D loss: 0.447253, acc.: 84.38%] [G loss: 0.432234]\n",
      "2043 [D loss: 0.407610, acc.: 76.56%] [G loss: 0.661678]\n",
      "2044 [D loss: 0.729575, acc.: 62.50%] [G loss: 0.757637]\n",
      "2045 [D loss: 0.703382, acc.: 57.81%] [G loss: 0.776827]\n",
      "2046 [D loss: 0.564052, acc.: 75.00%] [G loss: 0.549871]\n",
      "2047 [D loss: 0.652522, acc.: 68.75%] [G loss: 0.721835]\n",
      "2048 [D loss: 0.704844, acc.: 56.25%] [G loss: 1.351350]\n",
      "2049 [D loss: 0.720744, acc.: 59.38%] [G loss: 1.817931]\n",
      "2050 [D loss: 1.099158, acc.: 50.00%] [G loss: 0.717917]\n",
      "2051 [D loss: 0.961223, acc.: 51.56%] [G loss: 1.074732]\n",
      "2052 [D loss: 0.353392, acc.: 90.62%] [G loss: 1.122611]\n",
      "2053 [D loss: 0.906488, acc.: 45.31%] [G loss: 0.613985]\n",
      "2054 [D loss: 0.382206, acc.: 85.94%] [G loss: 0.696675]\n",
      "2055 [D loss: 1.318015, acc.: 23.44%] [G loss: 0.980182]\n",
      "2056 [D loss: 0.526547, acc.: 75.00%] [G loss: 1.044768]\n",
      "2057 [D loss: 0.582587, acc.: 60.94%] [G loss: 1.050938]\n",
      "2058 [D loss: 0.562689, acc.: 73.44%] [G loss: 1.160469]\n",
      "2059 [D loss: 0.517655, acc.: 75.00%] [G loss: 1.445015]\n",
      "2060 [D loss: 0.551451, acc.: 68.75%] [G loss: 1.574243]\n",
      "2061 [D loss: 1.035744, acc.: 48.44%] [G loss: 1.156876]\n",
      "2062 [D loss: 1.223842, acc.: 29.69%] [G loss: 1.703360]\n",
      "2063 [D loss: 0.588462, acc.: 73.44%] [G loss: 1.112816]\n",
      "2064 [D loss: 0.557221, acc.: 71.88%] [G loss: 1.182559]\n",
      "2065 [D loss: 0.761804, acc.: 59.38%] [G loss: 1.625828]\n",
      "2066 [D loss: 0.823521, acc.: 51.56%] [G loss: 1.389566]\n",
      "2067 [D loss: 0.631920, acc.: 67.19%] [G loss: 1.406134]\n",
      "2068 [D loss: 0.790068, acc.: 53.12%] [G loss: 1.126654]\n",
      "2069 [D loss: 0.907883, acc.: 42.19%] [G loss: 1.182252]\n",
      "2070 [D loss: 0.842825, acc.: 43.75%] [G loss: 1.490745]\n",
      "2071 [D loss: 0.740547, acc.: 59.38%] [G loss: 1.441098]\n",
      "2072 [D loss: 0.860024, acc.: 51.56%] [G loss: 1.021894]\n",
      "2073 [D loss: 0.663822, acc.: 59.38%] [G loss: 1.562316]\n",
      "2074 [D loss: 0.626179, acc.: 64.06%] [G loss: 1.385761]\n",
      "2075 [D loss: 1.125313, acc.: 31.25%] [G loss: 0.950648]\n",
      "2076 [D loss: 0.441950, acc.: 81.25%] [G loss: 1.600502]\n",
      "2077 [D loss: 0.519501, acc.: 75.00%] [G loss: 1.749194]\n",
      "2078 [D loss: 0.542371, acc.: 70.31%] [G loss: 1.634249]\n",
      "2079 [D loss: 0.875496, acc.: 50.00%] [G loss: 0.676628]\n",
      "2080 [D loss: 0.625471, acc.: 62.50%] [G loss: 0.956466]\n",
      "2081 [D loss: 0.477260, acc.: 78.12%] [G loss: 0.869994]\n",
      "2082 [D loss: 0.660724, acc.: 64.06%] [G loss: 1.108730]\n",
      "2083 [D loss: 0.784522, acc.: 57.81%] [G loss: 0.836783]\n",
      "2084 [D loss: 0.519463, acc.: 71.88%] [G loss: 1.042444]\n",
      "2085 [D loss: 0.901177, acc.: 46.88%] [G loss: 1.094174]\n",
      "2086 [D loss: 0.817109, acc.: 45.31%] [G loss: 1.668150]\n",
      "2087 [D loss: 0.683944, acc.: 67.19%] [G loss: 1.221277]\n",
      "2088 [D loss: 0.679601, acc.: 68.75%] [G loss: 0.649490]\n",
      "2089 [D loss: 0.483536, acc.: 78.12%] [G loss: 0.572576]\n",
      "2090 [D loss: 1.021558, acc.: 37.50%] [G loss: 0.854937]\n",
      "2091 [D loss: 0.651448, acc.: 64.06%] [G loss: 0.973402]\n",
      "2092 [D loss: 0.689061, acc.: 59.38%] [G loss: 1.881748]\n",
      "2093 [D loss: 0.487614, acc.: 73.44%] [G loss: 1.647464]\n",
      "2094 [D loss: 0.856710, acc.: 48.44%] [G loss: 0.875141]\n",
      "2095 [D loss: 1.096027, acc.: 32.81%] [G loss: 0.950683]\n",
      "2096 [D loss: 1.702715, acc.: 18.75%] [G loss: 1.218320]\n",
      "2097 [D loss: 0.750260, acc.: 53.12%] [G loss: 1.428752]\n",
      "2098 [D loss: 0.709835, acc.: 57.81%] [G loss: 1.340394]\n",
      "2099 [D loss: 0.865308, acc.: 48.44%] [G loss: 0.892350]\n",
      "2100 [D loss: 1.226559, acc.: 29.69%] [G loss: 0.569861]\n",
      "2101 [D loss: 0.514135, acc.: 67.19%] [G loss: 1.358869]\n",
      "2102 [D loss: 0.440625, acc.: 81.25%] [G loss: 1.209130]\n",
      "2103 [D loss: 0.477469, acc.: 81.25%] [G loss: 1.271536]\n",
      "2104 [D loss: 1.088865, acc.: 39.06%] [G loss: 0.973706]\n",
      "2105 [D loss: 0.371036, acc.: 85.94%] [G loss: 1.441932]\n",
      "2106 [D loss: 0.733094, acc.: 59.38%] [G loss: 1.430664]\n",
      "2107 [D loss: 0.945460, acc.: 48.44%] [G loss: 1.236967]\n",
      "2108 [D loss: 0.578185, acc.: 64.06%] [G loss: 1.126149]\n",
      "2109 [D loss: 0.654119, acc.: 64.06%] [G loss: 1.731123]\n",
      "2110 [D loss: 0.901640, acc.: 54.69%] [G loss: 1.353396]\n",
      "2111 [D loss: 0.786510, acc.: 53.12%] [G loss: 1.105311]\n",
      "2112 [D loss: 0.771226, acc.: 54.69%] [G loss: 1.365797]\n",
      "2113 [D loss: 0.909433, acc.: 48.44%] [G loss: 1.442412]\n",
      "2114 [D loss: 0.717061, acc.: 60.94%] [G loss: 0.789281]\n",
      "2115 [D loss: 0.598754, acc.: 68.75%] [G loss: 1.410583]\n",
      "2116 [D loss: 0.175174, acc.: 96.88%] [G loss: 1.270175]\n",
      "2117 [D loss: 0.442804, acc.: 81.25%] [G loss: 0.712134]\n",
      "2118 [D loss: 0.533514, acc.: 65.62%] [G loss: 1.229553]\n",
      "2119 [D loss: 0.365437, acc.: 87.50%] [G loss: 0.646591]\n",
      "2120 [D loss: 0.369364, acc.: 81.25%] [G loss: 0.764146]\n",
      "2121 [D loss: 0.513394, acc.: 73.44%] [G loss: 0.358804]\n",
      "2122 [D loss: 0.722842, acc.: 62.50%] [G loss: 0.597789]\n",
      "2123 [D loss: 0.375217, acc.: 81.25%] [G loss: 0.698309]\n",
      "2124 [D loss: 0.432339, acc.: 85.94%] [G loss: 0.589352]\n",
      "2125 [D loss: 0.487475, acc.: 76.56%] [G loss: 1.669719]\n",
      "2126 [D loss: 0.628029, acc.: 60.94%] [G loss: 1.753973]\n",
      "2127 [D loss: 0.873206, acc.: 51.56%] [G loss: 1.499770]\n",
      "2128 [D loss: 0.587460, acc.: 68.75%] [G loss: 1.934664]\n",
      "2129 [D loss: 0.976792, acc.: 43.75%] [G loss: 1.516576]\n",
      "2130 [D loss: 0.488868, acc.: 73.44%] [G loss: 1.250394]\n",
      "2131 [D loss: 0.490889, acc.: 76.56%] [G loss: 0.911115]\n",
      "2132 [D loss: 0.686671, acc.: 64.06%] [G loss: 0.997355]\n",
      "2133 [D loss: 0.424451, acc.: 85.94%] [G loss: 0.940823]\n",
      "2134 [D loss: 0.750772, acc.: 51.56%] [G loss: 0.832002]\n",
      "2135 [D loss: 0.864669, acc.: 51.56%] [G loss: 1.521801]\n",
      "2136 [D loss: 0.617994, acc.: 71.88%] [G loss: 1.667911]\n",
      "2137 [D loss: 0.811803, acc.: 59.38%] [G loss: 1.658657]\n",
      "2138 [D loss: 0.727174, acc.: 51.56%] [G loss: 1.232809]\n",
      "2139 [D loss: 0.723404, acc.: 60.94%] [G loss: 1.445297]\n",
      "2140 [D loss: 0.629335, acc.: 62.50%] [G loss: 1.092071]\n",
      "2141 [D loss: 0.778355, acc.: 59.38%] [G loss: 1.289783]\n",
      "2142 [D loss: 0.597523, acc.: 68.75%] [G loss: 1.618972]\n",
      "2143 [D loss: 0.888356, acc.: 56.25%] [G loss: 1.176440]\n",
      "2144 [D loss: 0.669675, acc.: 60.94%] [G loss: 1.630082]\n",
      "2145 [D loss: 0.537362, acc.: 75.00%] [G loss: 1.175790]\n",
      "2146 [D loss: 0.733097, acc.: 60.94%] [G loss: 1.089540]\n",
      "2147 [D loss: 0.607476, acc.: 67.19%] [G loss: 1.454012]\n",
      "2148 [D loss: 0.506830, acc.: 78.12%] [G loss: 1.246692]\n",
      "2149 [D loss: 0.441893, acc.: 75.00%] [G loss: 1.230886]\n",
      "2150 [D loss: 0.709367, acc.: 62.50%] [G loss: 0.903286]\n",
      "2151 [D loss: 0.838919, acc.: 57.81%] [G loss: 1.165576]\n",
      "2152 [D loss: 0.538542, acc.: 67.19%] [G loss: 1.960456]\n",
      "2153 [D loss: 0.557254, acc.: 70.31%] [G loss: 1.648627]\n",
      "2154 [D loss: 0.345115, acc.: 89.06%] [G loss: 1.231488]\n",
      "2155 [D loss: 0.625537, acc.: 64.06%] [G loss: 1.318067]\n"
     ]
    }
   ],
   "source": [
    "train(epochs=40000, batch_size=32, save_interval=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save(\"generator_latest.h5\")\n",
    "discriminator.save(\"discriminator_latest.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8CZGezKgISP8"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This was a very simple implementation of the DCGAN. Implemented to understand the basics of the working of the model. With the basics done, I can now move on to more applications of this. ALthough I heavily doubt about the computation power required and if Google Colabs will be sufficient enough. Let's just hope for the best.\n",
    "\n",
    "### References:\n",
    "\n",
    "1. https://github.com/pavitrakumar78/Anime-Face-GAN-Keras : One of the most detailed implementation to start with. The model has been trained for 10000 steps. Sadly I could not do that because my Colabs session somehow seemed to crash.\n",
    "\n",
    "2. https://towardsdatascience.com/generate-anime-style-face-using-dcgan-and-explore-its-latent-feature-representation-ae0e905f3974\n",
    "\n",
    "3. https://heartbeat.fritz.ai/my-mangagan-building-my-first-generative-adversarial-network-2ec1920257e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GAN_To_Generate_Anime_Faces.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
